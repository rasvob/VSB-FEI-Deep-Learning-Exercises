{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm",
    "tags": []
   },
   "source": [
    "# Deep Learning - Exercise 7\n",
    "\n",
    "The aim of this exercise is to learn how to build unsupervised word embeddings using the Word2Vec Skip-Gram method and implement recurrent neural networks (RNNs) for text generation using Harry Potter books as our dataset.\n",
    "\n",
    "**Core Concepts**\n",
    "* ğŸ§  Word2Vec Skip-Gram model for creating word embeddings\n",
    "* ğŸ“š Harry Potter corpus for training word embeddings\n",
    "* ğŸ”¤ Analyzing word relationships in embedding space\n",
    "* âš¡ Text generation using character-based RNNs\n",
    "* ğŸ“ Creating Harry Potter style stories with generative models\n",
    "\n",
    "The Word2Vec approach is based on [official Keras tutorial](https://www.tensorflow.org/tutorials/text/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_07.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_07.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 09:53:44.934617: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 09:53:44.944543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740736424.956149  931301 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740736424.959642  931301 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 09:53:44.972352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import string as tf_string\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "\n",
    "from sklearn.model_selection import train_test_split # \n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import scipy\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "import tqdm\n",
    "import io\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” What is word embedding?\n",
    "* Why do we use it? \n",
    "* Do we need to train our own embedding?\n",
    "* Do the embedding have any other usage beside ANN applications?\n",
    "\n",
    "# ğŸ“’ Word2Vec\n",
    "\n",
    "## ğŸ’¡ There are two approaches for a Word2Vec embedding training\n",
    "\n",
    "* **Continuous bag-of-words model**: \n",
    "    * predicts the middle word based on surrounding context words. \n",
    "    * the context consists of a few words before and after the current (middle) word. \n",
    "    * this architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "\n",
    "* **Continuous skip-gram model**: \n",
    "    * predicts words within a certain range before and after the current word in the same sentence. \n",
    "    * **we will use this as it is easier concept to grasp**\n",
    "\n",
    "![w2v](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_skip.png?raw=true)\n",
    "  \n",
    "* ğŸ“Œ Bag-of-words model predicts a word given the neighboring context\n",
    "* ğŸ“Œ Skip-gram model predicts the context (or neighbors) of a word, given the word itself\n",
    "\n",
    "* The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). \n",
    "* ğŸ’¡ The context of a word can be represented through a set of skip-gram pairs of `(target_word, context_word)` where `context_word` appears in the neighboring context of target_word.\n",
    "\n",
    "## ğŸ” What is the difference between Word2Vec or other embeddings and GPT-like models?\n",
    "* The core idea is the same: Both models need a way to represent words as dense vectors in a continuous vector space\n",
    "* The models serve different purposes and operate in different ways\n",
    "    * GPT-like models are focused more on vector-to-vector tasks - generating of text answers\n",
    "        * e.g. question answering, translation, text completion, ....\n",
    "    * Traditional models focus rather on vector-to-scalar tasks - classification\n",
    "        * e.g. sentiment analysis\n",
    "* Word2Vec embeddings generate fixed-size vector representations for words based on their co-occurrence statistics within a context window\n",
    "    * These representations do not capture the context in which the word appears in a specific sentence or document\n",
    "    * ğŸ’¡ Word2Vec embeddings can be trained on relatively smaller corpora and still capture meaningful semantic relationships between words\n",
    "* GPT models capture contextual understanding by considering the entire preceding context when generating each token in a sequence\n",
    "    * This allows them to generate more contextually relevant responses and understand nuances in language better\n",
    "    * ğŸ’¡ The model to learn rich representations of language and capture long-range dependencies\n",
    "    * ğŸ’¡ GPT models need exposure to a diverse range of linguistic patterns and contexts, which typically requires a large dataset\n",
    "* ğŸ“Œ A \"crossover\" between both approaches are transformer models models like BERT \n",
    "    * It uses a masked language model (MLM) objective, where random words in a sentence are masked, and the model is trained to predict these masked words based on the surrounding context \n",
    "    * Additionally, BERT also uses a next sentence prediction (NSP) objective during pre-training to learn sentence-level relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will demonstrate the Word2Vec working using single sentence\n",
    "\n",
    "* The context words for each of the 8 words of this sentence are defined by a window size. \n",
    "    * ğŸ’¡ The window size determines the span of words on either side of a target_word that can be considered a context word.\n",
    "\n",
    "![w2v_tab](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_tab.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the first step we will tokenize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can build the vocabulary and mapping WORD -> ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## It is common to also build also the inverse mapping ID -> WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So int-encoded sentences will look like this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Œ We can use the [tf.keras.preprocessing.sequence.skipgrams](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence) to generate skip-gram pairs\n",
    "\n",
    "*  We will generate skip-grams from the example_sequence with a given window_size from tokens in the range [0, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0,\n",
    "      seed=int(SEED))\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Let's take a look at some skip-gram examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4): (the, shimmered)\n",
      "(4, 1): (shimmered, the)\n",
      "(5, 4): (in, shimmered)\n",
      "(7, 1): (sun, the)\n",
      "(4, 3): (shimmered, road)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "    print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ğŸ“’ Negative sampling for one skip-gram\n",
    "\n",
    "* The skip-gram function returns all positive skip-gram pairs by sliding over a given window span\n",
    "\n",
    "### ğŸ’¡ But we also need some negative examples to train the model as well\n",
    "\n",
    "## ğŸ” How can we generate such samples?\n",
    "* To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary\n",
    "* Use the `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` number of negative samples for a given target word in a window\n",
    "    * ğŸ’¡ You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shimmered\n",
      "tf.Tensor([3 0 4 2], shape=(4,), dtype=int64)\n",
      "['road', '<pad>', 'shimmered', 'wide']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740736583.683355  931301 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 909 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:02:00.0, compute capability: 8.9\n",
      "I0000 00:00:1740736583.684787  931301 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21792 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:41:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(inverse_vocab[target_word], inverse_vocab[context_word])\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce a dimension so you can use concatenation (in the next step).\n",
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 1\n",
      "target_word     : the\n",
      "context_indices : [4 3 0 4 2]\n",
      "context_words   : ['shimmered', 'road', '<pad>', 'shimmered', 'wide']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The whole process can be illustrated with this example\n",
    "\n",
    "![w2v_example](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_example.png?raw=true)\n",
    "\n",
    "## Skip-gram sampling table\n",
    "* A large dataset means larger vocabulary with higher number of more frequent words such as stopwords\n",
    "* ğŸ’¡ Training examples obtained from sampling commonly occurring words (such as the, is, on) don't add much useful information for the model\n",
    "    * Subsampling of frequent words as a helpful practice to improve embedding quality\n",
    "\n",
    "### `sampling_table[i]` denotes the probability of sampling the `i-th` most common word in a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        \n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "              sequence,\n",
    "              vocabulary_size=vocab_size,\n",
    "              sampling_table=sampling_table,\n",
    "              window_size=window_size,\n",
    "              seed=int(SEED),\n",
    "              negative_samples=0)\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with a positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "              true_classes=context_class,\n",
    "              num_true=1,\n",
    "              num_sampled=num_ns,\n",
    "              unique=True,\n",
    "              range_max=vocab_size,\n",
    "              seed=int(SEED),\n",
    "              name=\"negative_sampling\")\n",
    "\n",
    "          # Build context and label vectors (for one target word)\n",
    "            context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "          # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“’ Now we can download the Harry Potter and the Sorcerer's Stone book and train our own ombedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('hp1.txt', 'https://raw.githubusercontent.com/rasvob/VSB-FEI-Deep-Learning-Exercises/main/datasets/hp1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 50 lines of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone\n",
      "\n",
      "\n",
      "CHAPTER ONE\n",
      "\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the last\n",
      "people you'd expect to be involved in anything strange or mysterious,\n",
      "because they just didn't hold with such nonsense.\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made\n",
      "drills. He was a big, beefy man with hardly any neck, although he did\n",
      "have a very large mustache. Mrs. Dursley was thin and blonde and had\n",
      "nearly twice the usual amount of neck, which came in very useful as she\n",
      "spent so much of her time craning over garden fences, spying on the\n",
      "neighbors. The Dursleys had a small son called Dudley and in their\n",
      "opinion there was no finer boy anywhere.\n",
      "\n",
      "The Dursleys had everything they wanted, but they also had a secret, and\n",
      "their greatest fear was that somebody would discover it. They didn't\n",
      "think they could bear it if anyone found out about the Potters. Mrs.\n",
      "Potter was Mrs. Dursley's sister, but they hadn't met for several years;\n",
      "in fact, Mrs. Dursley pretended she didn't have a sister, because her\n",
      "sister and her good-for-nothing husband were as unDursleyish as it was\n",
      "possible to be. The Dursleys shuddered to think what the neighbors would\n",
      "say if the Potters arrived in the street. The Dursleys knew that the\n",
      "Potters had a small son, too, but they had never even seen him. This boy\n",
      "was another good reason for keeping the Potters away; they didn't want\n",
      "Dudley mixing with a child like that.\n",
      "\n",
      "When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story\n",
      "starts, there was nothing about the cloudy sky outside to suggest that\n",
      "strange and mysterious things would soon be happening all over the\n",
      "country. Mr. Dursley hummed as he picked out his most boring tie for\n",
      "work, and Mrs. Dursley gossiped away happily as she wrestled a screaming\n",
      "Dudley into his high chair.\n",
      "\n",
      "None of them noticed a large, tawny owl flutter past the window.\n",
      "\n",
      "At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs.\n",
      "Dursley on the cheek, and tried to kiss Dudley good-bye but missed,\n",
      "because Dudley was now having a tantrum and throwing his cereal at the\n",
      "walls. \"Little tyke,\" chortled Mr. Dursley as he left the house. He got\n",
      "into his car and backed out of number four's drive.\n",
      "\n",
      "It was on the corner of the street that he noticed the first sign of\n",
      "something peculiar -- a cat reading a map. For a second, Mr. Dursley\n",
      "didn't realize what he had seen -- then he jerked his head around to\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f:\n",
    "    lines = f.read().splitlines()\n",
    "for line in lines[:50]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ We will employ the *TextLineDataset* from the TF data API\n",
    "* It allows us to easily load text file line by line and preprocess it\n",
    "* We will skip the book title and blank lines, then we will remove the CHAPTER XYZ lines as the information is not useful\n",
    "    * Then we can transform the text into lowercase and remove the punctuation\n",
    "    * We will use the punctuation from the `re` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.escape(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).skip(1).filter(lambda x: tf.cast(tf.strings.length(x), bool)).filter(lambda y: not tf.strings.regex_full_match(y, 'CHAPTER.*')).map(lambda z: tf.strings.lower(z)).map(lambda a: tf.strings.regex_replace(a, f'[{re.escape(string.punctuation)}]', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is our pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'the boy who lived'\n",
      "b'mr and mrs dursley of number four privet drive were proud to say'\n",
      "b'that they were perfectly normal thank you very much they were the last'\n",
      "b'people youd expect to be involved in anything strange or mysterious'\n",
      "b'because they just didnt hold with such nonsense'\n",
      "b'mr dursley was the director of a firm called grunnings which made'\n",
      "b'drills he was a big beefy man with hardly any neck although he did'\n",
      "b'have a very large mustache mrs dursley was thin and blonde and had'\n",
      "b'nearly twice the usual amount of neck which came in very useful as she'\n",
      "b'spent so much of her time craning over garden fences spying on the'\n"
     ]
    }
   ],
   "source": [
    "for element in text_ds.take(10).as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TF dataset works as a data stream\n",
    "* ğŸ’¡ The TF dataset uses standard map/reduce API\n",
    "    * ğŸ” How do we iterate over data stream?\n",
    "    * ğŸ” How to count elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of lines in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int32(7628)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds.map(lambda x: tf.cast(tf.strings.length(x), tf.int32)).reduce(0, lambda x, y: x + 1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Total length of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int32(405596)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds.map(lambda x: tf.cast(tf.strings.length(x), tf.int32)).reduce(0, lambda x, y: x + y).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average length of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x795d97b4a340> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x795d97b4a340>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x: tf.cast(tf.strings.length(x), tf.int32)\n",
      "\n",
      "Match 1:\n",
      "lambda x: tf.cast(tf.strings.length(x), tf.int32)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x795d97b4a340> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x795d97b4a340>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x: tf.cast(tf.strings.length(x), tf.int32)\n",
      "\n",
      "Match 1:\n",
      "lambda x: tf.cast(tf.strings.length(x), tf.int32)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x795d953002c0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x795d953002c0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x, y: x + y\n",
      "\n",
      "Match 1:\n",
      "lambda x, y: x + 1\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x795d953002c0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x795d953002c0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x, y: x + y\n",
      "\n",
      "Match 1:\n",
      "lambda x, y: x + 1\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int32(53)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds.map(lambda x: tf.cast(tf.strings.length(x), tf.int32)).reduce(0, lambda x, y: x + y).numpy() // text_ds.map(lambda x: tf.cast(tf.strings.length(x), tf.int32)).reduce(0, lambda x, y: x + 1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can setup the `TextVectorization` layer for integer encoding of the tokens\n",
    "* ğŸ’¡ It is the same layer as in the sentiment analysis excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_length = 15\n",
    "vectorize_layer = keras.layers.TextVectorization(max_tokens=None, output_mode='int', output_sequence_length=sequence_length)\n",
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('the'), np.str_('and'), np.str_('to'), np.str_('a'), np.str_('he'), np.str_('of'), np.str_('harry'), np.str_('was'), np.str_('it'), np.str_('in'), np.str_('his'), np.str_('you'), np.str_('said'), np.str_('had'), np.str_('i'), np.str_('on'), np.str_('at'), np.str_('that')]\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Number of tokens in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ The `unbatch` works in a similar way as the `ravel` in numpy\n",
    "* i.e. flattening `(n, 1)` shaped array into `(n,)` shaped one\n",
    "    * e.g. `[[5], [0], [2]]` -> `[5, 0, 2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  141   74 1071    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "[ 148    3  258  223    7  616  366  646  534   32 1272    4  157    0\n",
      "    0]\n",
      "[  19   20   32 1574  974 1128   13   68  155   20   32    2  143    0\n",
      "    0]\n",
      "[ 131  469  789    4   30 1608   11  165  471  102 1421    0    0    0\n",
      "    0]\n",
      "[ 140   20   63   56  835   24  497 1844    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "for x in text_vector_ds.take(5).as_numpy_iterator():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ We can take a look at number of sequences generated and some examples of the data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 09:58:54.835134: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7628"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Why are there the `0` ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  141   74 1071    0    0    0    0    0    0    0    0    0    0\n",
      "    0] => [np.str_('the'), np.str_('boy'), np.str_('who'), np.str_('lived'), '', '', '', '', '', '', '', '', '', '', '']\n",
      "[ 148    3  258  223    7  616  366  646  534   32 1272    4  157    0\n",
      "    0] => [np.str_('mr'), np.str_('and'), np.str_('mrs'), np.str_('dursley'), np.str_('of'), np.str_('number'), np.str_('four'), np.str_('privet'), np.str_('drive'), np.str_('were'), np.str_('proud'), np.str_('to'), np.str_('say'), '', '']\n",
      "[  19   20   32 1574  974 1128   13   68  155   20   32    2  143    0\n",
      "    0] => [np.str_('that'), np.str_('they'), np.str_('were'), np.str_('perfectly'), np.str_('normal'), np.str_('thank'), np.str_('you'), np.str_('very'), np.str_('much'), np.str_('they'), np.str_('were'), np.str_('the'), np.str_('last'), '', '']\n",
      "[ 131  469  789    4   30 1608   11  165  471  102 1421    0    0    0\n",
      "    0] => [np.str_('people'), np.str_('youd'), np.str_('expect'), np.str_('to'), np.str_('be'), np.str_('involved'), np.str_('in'), np.str_('anything'), np.str_('strange'), np.str_('or'), np.str_('mysterious'), '', '', '', '']\n",
      "[ 140   20   63   56  835   24  497 1844    0    0    0    0    0    0\n",
      "    0] => [np.str_('because'), np.str_('they'), np.str_('just'), np.str_('didnt'), np.str_('hold'), np.str_('with'), np.str_('such'), np.str_('nonsense'), '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Finally we can create the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7628/7628 [00:13<00:00, 554.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (38611,)\n",
      "contexts.shape: (38611, 5)\n",
      "labels.shape: (38611, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=int(SEED))\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([366, 366, 366, ..., 606,  45, 606])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 534,  373,   10,  631,  168],\n",
       "       [   7,  612,    0,    1, 1509],\n",
       "       [ 616,   72, 1017,  234,    1],\n",
       "       ...,\n",
       "       [  45,    9,   39,   90,   47],\n",
       "       [ 606,    4,   17, 3876,    0],\n",
       "       [  32, 2654,    1,   51, 1825]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will form the dataset using TF data API as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(64, 1), dtype=tf.int64, name=None), TensorSpec(shape=(64, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(64, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets.reshape(-1, 1), contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance tweaks\n",
    "* When the GPU is working on forward / backward propagation on the current batch, we want the CPU to process the next batch of data so that it is immediately ready \n",
    "* ğŸ’¡ As the most expensive part of the computer, we want the GPU to be fully used all the time during training\n",
    "    * We call this consumer / producer overlap, where the consumer is the GPU and the producer is the CPU\n",
    "\n",
    "* With `tf.data`, you can do this with a simple call to `dataset.prefetch(N)` at the end of the pipeline (after batching). \n",
    "    * ğŸ’¡ This will always prefetch `N` batches of data and make sure that there is always `N` batches ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ The final step is to define and train the model\n",
    "* We will use 2 `Embedding` layers\n",
    "    * One for the **target word** and one for the **context words**\n",
    "* Finally the dot product of the Embedding outputs will be computed to combine the vectors and the result will be taken as an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fei/svo0175/Documents/VSB-FEI-Deep-Learning-Exercises/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ input_layer_3       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ w2v_embedding       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">301,800</span> â”‚ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ctx_embedding       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">301,800</span> â”‚ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ w2v_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ ctx_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ input_layer_3       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ w2v_embedding       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)     â”‚    \u001b[38;5;34m301,800\u001b[0m â”‚ input_layer_2[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ctx_embedding       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)     â”‚    \u001b[38;5;34m301,800\u001b[0m â”‚ input_layer_3[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dot (\u001b[38;5;33mDot\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m5\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ w2v_embedding[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ ctx_embedding[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dot[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">603,600</span> (2.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m603,600\u001b[0m (2.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">603,600</span> (2.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m603,600\u001b[0m (2.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "target_input = keras.layers.Input((1,))\n",
    "context_input = keras.layers.Input((num_ns+1,))\n",
    "\n",
    "emb_w2v = keras.layers.Embedding(vocab_size, embedding_dim, name=\"w2v_embedding\", embeddings_initializer='glorot_uniform', input_length=1)(target_input)\n",
    "emb_ctx = keras.layers.Embedding(vocab_size, embedding_dim, name=\"ctx_embedding\", embeddings_initializer='glorot_uniform', input_length=num_ns+1)(context_input)\n",
    "\n",
    "dots = keras.layers.dot([emb_w2v, emb_ctx], axes=2)\n",
    "\n",
    "fl = keras.layers.Flatten()(dots)\n",
    "\n",
    "# out = keras.layers.Dense(num_ns+1, activation='linear')(fl)\n",
    "\n",
    "model = keras.Model(inputs=[target_input, context_input], outputs=fl)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740736770.382245  931669 service.cc:148] XLA service 0x7957e80032e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1740736770.382282  931669 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1740736770.382287  931669 service.cc:156]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-02-28 09:59:30.412370: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1740736770.455442  931669 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 86/603\u001b[0m \u001b[32mâ”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2188 - loss: 1.6094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740736770.678852  931669 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2437 - loss: 1.6031\n",
      "Epoch 2/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896us/step - accuracy: 0.3884 - loss: 1.4880\n",
      "Epoch 3/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944us/step - accuracy: 0.4914 - loss: 1.3534\n",
      "Epoch 4/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - accuracy: 0.6071 - loss: 1.2029\n",
      "Epoch 5/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 954us/step - accuracy: 0.6946 - loss: 1.0499\n",
      "Epoch 6/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - accuracy: 0.7585 - loss: 0.9060\n",
      "Epoch 7/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - accuracy: 0.8087 - loss: 0.7763\n",
      "Epoch 8/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - accuracy: 0.8457 - loss: 0.6624\n",
      "Epoch 9/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step - accuracy: 0.8737 - loss: 0.5642\n",
      "Epoch 10/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 898us/step - accuracy: 0.8952 - loss: 0.4808\n",
      "Epoch 11/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 924us/step - accuracy: 0.9140 - loss: 0.4107\n",
      "Epoch 12/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896us/step - accuracy: 0.9280 - loss: 0.3520\n",
      "Epoch 13/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 935us/step - accuracy: 0.9404 - loss: 0.3030\n",
      "Epoch 14/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 912us/step - accuracy: 0.9508 - loss: 0.2621\n",
      "Epoch 15/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 910us/step - accuracy: 0.9598 - loss: 0.2277\n",
      "Epoch 16/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 869us/step - accuracy: 0.9669 - loss: 0.1989\n",
      "Epoch 17/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - accuracy: 0.9722 - loss: 0.1745\n",
      "Epoch 18/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 835us/step - accuracy: 0.9760 - loss: 0.1539\n",
      "Epoch 19/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 901us/step - accuracy: 0.9795 - loss: 0.1363\n",
      "Epoch 20/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 908us/step - accuracy: 0.9822 - loss: 0.1214\n",
      "Epoch 21/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step - accuracy: 0.9837 - loss: 0.1087\n",
      "Epoch 22/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 903us/step - accuracy: 0.9854 - loss: 0.0978\n",
      "Epoch 23/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 923us/step - accuracy: 0.9865 - loss: 0.0885\n",
      "Epoch 24/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919us/step - accuracy: 0.9874 - loss: 0.0805\n",
      "Epoch 25/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 907us/step - accuracy: 0.9878 - loss: 0.0737\n",
      "Epoch 26/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step - accuracy: 0.9885 - loss: 0.0678\n",
      "Epoch 27/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 931us/step - accuracy: 0.9892 - loss: 0.0627\n",
      "Epoch 28/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - accuracy: 0.9893 - loss: 0.0584\n",
      "Epoch 29/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 915us/step - accuracy: 0.9894 - loss: 0.0546\n",
      "Epoch 30/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890us/step - accuracy: 0.9896 - loss: 0.0514\n",
      "Epoch 31/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917us/step - accuracy: 0.9897 - loss: 0.0486\n",
      "Epoch 32/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 918us/step - accuracy: 0.9899 - loss: 0.0462\n",
      "Epoch 33/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 922us/step - accuracy: 0.9898 - loss: 0.0441\n",
      "Epoch 34/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - accuracy: 0.9898 - loss: 0.0422\n",
      "Epoch 35/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - accuracy: 0.9899 - loss: 0.0406\n",
      "Epoch 36/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step - accuracy: 0.9899 - loss: 0.0393\n",
      "Epoch 37/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - accuracy: 0.9900 - loss: 0.0380\n",
      "Epoch 38/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888us/step - accuracy: 0.9899 - loss: 0.0370\n",
      "Epoch 39/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 883us/step - accuracy: 0.9899 - loss: 0.0361\n",
      "Epoch 40/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - accuracy: 0.9900 - loss: 0.0353\n",
      "Epoch 41/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842us/step - accuracy: 0.9901 - loss: 0.0345\n",
      "Epoch 42/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854us/step - accuracy: 0.9901 - loss: 0.0339\n",
      "Epoch 43/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881us/step - accuracy: 0.9901 - loss: 0.0334\n",
      "Epoch 44/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 932us/step - accuracy: 0.9902 - loss: 0.0329\n",
      "Epoch 45/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 914us/step - accuracy: 0.9901 - loss: 0.0324\n",
      "Epoch 46/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 939us/step - accuracy: 0.9901 - loss: 0.0321\n",
      "Epoch 47/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - accuracy: 0.9901 - loss: 0.0317\n",
      "Epoch 48/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 933us/step - accuracy: 0.9901 - loss: 0.0314\n",
      "Epoch 49/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - accuracy: 0.9901 - loss: 0.0311\n",
      "Epoch 50/50\n",
      "\u001b[1m603/603\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 924us/step - accuracy: 0.9901 - loss: 0.0309\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## âš¡ Now we can save the vectors and visualize it using [TF projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Word2Vec tutorial will be ever complete without the similar word search ğŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2word = {k: v for k, v in enumerate(vocab)}\n",
    "word2id = {v: k for k, v in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So here we go, we will search the closest vectors for selected words\n",
    "* ğŸ” How is it done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6036, 6036)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'harry': [np.str_('pearlywhite'),\n",
       "  np.str_('doom'),\n",
       "  np.str_('lamplike'),\n",
       "  np.str_('patted'),\n",
       "  np.str_('crutches')],\n",
       " 'hagrid': [np.str_('touching'),\n",
       "  np.str_('fastened'),\n",
       "  np.str_('sunburn'),\n",
       "  np.str_('poke'),\n",
       "  np.str_('sandy')],\n",
       " 'potter': [np.str_('five'),\n",
       "  np.str_('blinked'),\n",
       "  np.str_('statues'),\n",
       "  np.str_('important'),\n",
       "  np.str_('marge')],\n",
       " 'go': [np.str_('toe'),\n",
       "  np.str_('yet'),\n",
       "  np.str_('make'),\n",
       "  np.str_('dish'),\n",
       "  np.str_('chickened')],\n",
       " 'he': [np.str_('bravo'),\n",
       "  np.str_('glinting'),\n",
       "  np.str_('creeps'),\n",
       "  np.str_('health'),\n",
       "  np.str_('sir')],\n",
       " 'the': [np.str_('such'),\n",
       "  np.str_('arrivals'),\n",
       "  np.str_('presents'),\n",
       "  np.str_('believe'),\n",
       "  np.str_('whippy')],\n",
       " 'one': [np.str_('fleet'),\n",
       "  np.str_('twisting'),\n",
       "  np.str_('traffic'),\n",
       "  np.str_('kind'),\n",
       "  np.str_('subjects')],\n",
       " 'hermione': [np.str_('bending'),\n",
       "  np.str_('boring'),\n",
       "  np.str_('admiring'),\n",
       "  np.str_('west'),\n",
       "  np.str_('kitchens')]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "distance_matrix = cosine_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]].argsort()[1:6]] \n",
    "                   for search_term in ['harry', 'hagrid', 'potter', 'go', 'he', 'the', 'one','hermione']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The results are clearly far from ideal ğŸ˜ª\n",
    "![w2v_meme_01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_meme_01.png?raw=true)\n",
    "\n",
    "## ğŸ” What happend? Did we do anything wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ok, let's try again with pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = 'glove.6B.50d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ğŸ’¡ This is how the embedding latent vector looks like for the word 'audi' and 'bmw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.051355 ,  0.11694  ,  1.0251   ,  0.12414  , -0.83236  ,\n",
       "        1.0288   , -0.64566  , -1.4468   , -0.89265  , -0.32658  ,\n",
       "        0.66507  , -0.65524  , -1.8323   , -1.0347   ,  0.13486  ,\n",
       "       -0.033565 , -0.2208   ,  1.855    , -0.2495   , -0.84343  ,\n",
       "        0.14318  , -0.81258  , -0.84232  ,  1.1247   , -0.075604 ,\n",
       "       -0.30852  , -0.79071  ,  0.80721  , -0.24747  , -0.029263 ,\n",
       "        0.2684   ,  0.6531   ,  0.48872  ,  1.1838   ,  0.5606   ,\n",
       "       -0.68087  ,  0.25192  ,  0.98091  , -1.0433   , -0.27203  ,\n",
       "        1.1912   , -0.88594  ,  0.022038 , -0.82012  , -0.0022396,\n",
       "       -0.68251  ,  0.12713  ,  0.85041  ,  1.002    ,  0.33904  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['audi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70038 , -0.16073 ,  1.3423  ,  0.63331 , -0.21958 ,  0.31944 ,\n",
       "       -0.67042 , -0.94041 , -0.56935 , -0.67842 ,  0.39705 , -0.18964 ,\n",
       "       -2.2101  , -0.90947 ,  0.95511 , -0.01321 , -0.32738 ,  1.1554  ,\n",
       "       -0.48464 , -1.7606  , -0.051495, -1.0745  , -1.183   ,  0.68672 ,\n",
       "       -0.107   , -0.42152 , -0.15516 ,  0.12724 , -0.42114 ,  0.30905 ,\n",
       "        0.59784 ,  0.050149,  0.24022 ,  0.86494 ,  0.63488 , -0.75644 ,\n",
       "       -0.09189 ,  1.0218  , -0.96638 , -0.90508 ,  0.80575 , -0.75225 ,\n",
       "        0.7642  , -0.94425 ,  0.4609  ,  0.11877 ,  0.24907 ,  0.066667,\n",
       "        0.59622 ,  0.1275  ], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['bmw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Hypothesis: The cosine distance of the car brands should be smaller than with some random word\n",
    "* ğŸ” Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.16636306)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(embeddings_index['audi'], embeddings_index['bmw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.0900573)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(embeddings_index['audi'], embeddings_index['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ For trying the famous `queen -> king` example we need to build the embedding matrix\n",
    "\n",
    "![w2v_meme_03](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_meme_03.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = len(embeddings_index.keys())\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "word2id = {k:i for i, (k,v) in enumerate(embeddings_index.items())}\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word2id.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the closest words is pretty easy now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_w = cosine_distances(embedding_matrix[word2id['man']].reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n",
      "boy\n",
      "another\n",
      "old\n",
      "one\n"
     ]
    }
   ],
   "source": [
    "for x in c_w.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_w = cosine_distances(embedding_matrix[word2id['woman']].reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girl\n",
      "man\n",
      "mother\n",
      "her\n",
      "boy\n"
     ]
    }
   ],
   "source": [
    "for x in c_w.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea is that using the difference between `man` and `woman` should be simillar as `king` and `queen`\n",
    "* ğŸ’¡ Thus it should be possible to use the difference for searching for analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = embeddings_index['man'] - embeddings_index['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.087144  , -0.2182    ,  0.40985996,  0.03922001,  0.10320008,\n",
       "       -0.94165003,  0.06042001, -0.32988   , -0.46144   ,  0.35962   ,\n",
       "       -0.31102   ,  0.86824   , -0.96006   , -0.01073003, -0.24337   ,\n",
       "       -0.08193001,  1.02722   ,  0.21122   , -0.695044  ,  0.00222   ,\n",
       "       -0.29106003, -0.50530005,  0.099454  , -0.40445   , -0.30181003,\n",
       "       -0.1355002 ,  0.06060004,  0.07131001,  0.19245   ,  0.06115001,\n",
       "        0.3204    , -0.07165   ,  0.13337001,  0.25068715,  0.14292999,\n",
       "        0.224957  ,  0.14899999, -0.048882  , -0.12191002,  0.27362   ,\n",
       "        0.16547601,  0.20426002, -0.54376   ,  0.27142498,  0.10244995,\n",
       "        0.32108003, -0.2516    ,  0.33454996,  0.04371002, -0.01258   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summed = embeddings_index['queen'] + dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.465684  ,  1.6051    , -0.85494   , -0.06507999,  0.46149006,\n",
       "       -0.34136003, -0.11496   ,  0.50779   , -0.518238  , -0.39833   ,\n",
       "       -0.08420999,  1.85411   , -0.35419   , -0.32492003,  0.04539999,\n",
       "        0.4782    ,  0.25266004,  0.282641  , -1.269144  ,  0.21564001,\n",
       "        0.28568   , -0.11850005, -0.02628601, -0.12433001, -0.02046004,\n",
       "       -1.9408002 , -0.9814999 , -0.12123999, -0.3613    ,  0.00662401,\n",
       "        1.8778    ,  0.32131   , -0.11412999,  0.59319717,  0.59657997,\n",
       "        0.38732702,  0.67364   , -0.119154  , -0.95935005, -0.75898004,\n",
       "        0.624936  ,  0.45728   , -0.72213   , -0.46255502, -0.09780005,\n",
       "        0.55578005, -0.81254995, -1.9493501 ,  0.05298533, -0.61542   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = cosine_distances(summed.reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And here we go ğŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "prince\n",
      "crown\n",
      "coronation\n",
      "royal\n"
     ]
    }
   ],
   "source": [
    "for x in res.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![w2v_meme_02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_meme_02.png?raw=true)\n",
    "\n",
    "# We will also learn how to use RNN as a text generator! ğŸ™‚\n",
    "* There are two main ways for solving the task\n",
    "    * ğŸ’¡ Word-based model\n",
    "    * ğŸ’¡ Character-based model\n",
    "    \n",
    "* We have relatively small dataset thus we will use the **Character-based model** as it works better with smaller datasets\n",
    "    * ğŸ’¡ We will also simplify the task for using only lower case letters\n",
    "\n",
    "* ğŸ” If we would have very large text corpus available, which way would be better and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_hp = tf.data.TextLineDataset(path_to_file).skip(1).filter(lambda x: tf.cast(tf.strings.length(x), bool)).filter(lambda y: not tf.strings.regex_full_match(y, 'CHAPTER.*')).map(lambda z: tf.strings.lower(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt_one_line = ''\n",
    "for x in text_hp.as_numpy_iterator():\n",
    "    txt_one_line += str(x)[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the boy who livedmr. and mrs. dursley, of number four, privet drive, were proud to saythat they were perfectly normal, thank you very much. they were '"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_one_line[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an array of letters from the whole text and filter out everything which is not lower-case letters and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "letters = []\n",
    "for x in txt_one_line:\n",
    "    if x >= 'a' and x <= 'z' or x == ' ':\n",
    "        letters.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', ' ', 'b', 'o', 'y', ' ', 'w', 'h']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have corpus of 405551 characters available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405551"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ But only 27 unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(letters)))\n",
    "print(\"Total chars:\", len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will build `ID -> CHAR` and `CHAR -> ID` lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ We need to create fixed length sequences for the model\n",
    "* We will shift the sliding window of `SEQ_LEN` by `step` and for `X,y` pair\n",
    "    * ğŸ’¡ Input is array of `SEQ_LEN` letters output is just **1** letter which comes after the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 40\n",
    "step = 1\n",
    "X, y = [], []\n",
    "for i in range(0, len(letters) - SEQ_LEN, step):\n",
    "    seq, ch = letters[i:i+SEQ_LEN], letters[i + SEQ_LEN]\n",
    "    X.append(seq)\n",
    "    y.append(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at the example\n",
    "* ğŸ’¡ Focus on the last letter of the second sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', ' ', 'b', 'o', 'y', ' ', 'w', 'h', 'o', ' ', 'l', 'i', 'v', 'e', 'd', 'm', 'r', ' ', 'a', 'n', 'd', ' ', 'm', 'r', 's', ' ', 'd', 'u', 'r', 's', 'l', 'e', 'y', ' ', 'o', 'f', ' ', 'n']\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', ' ', 'b', 'o', 'y', ' ', 'w', 'h', 'o', ' ', 'l', 'i', 'v', 'e', 'd', 'm', 'r', ' ', 'a', 'n', 'd', ' ', 'm', 'r', 's', ' ', 'd', 'u', 'r', 's', 'l', 'e', 'y', ' ', 'o', 'f', ' ', 'n', 'u']\n"
     ]
    }
   ],
   "source": [
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Characted level RNN uses usually one-hot encoding as we work just with a few unique tokens \n",
    "* So no complex embedding is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_ohe = np.zeros((len(X), SEQ_LEN, len(chars)), dtype=bool)\n",
    "y_ohe = np.zeros((len(X), len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(X):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X_ohe[i, t, char_indices[char]] = 1\n",
    "    y_ohe[i, char_indices[y[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405511, 40, 27)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405511, 27)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ohe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ The final step is the model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">79,872</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,483</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m27\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m79,872\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m131,584\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚        \u001b[38;5;34m33,024\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)             â”‚         \u001b[38;5;34m3,483\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">280,859</span> (1.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m280,859\u001b[0m (1.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">280,859</span> (1.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m280,859\u001b[0m (1.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_layer = keras.layers.Input(shape=(SEQ_LEN, len(chars)))\n",
    "x = LSTM(128, return_sequences=True)(input_layer)\n",
    "x = LSTM(128, return_sequences=False)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, 'relu')(x)\n",
    "x = keras.layers.Dense(128, 'relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "output_layer = keras.layers.Dense(len(chars), activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=keras.losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.2488 - loss: 2.5921 - val_accuracy: 0.3968 - val_loss: 2.0201\n",
      "Epoch 2/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.4205 - loss: 1.9438 - val_accuracy: 0.4693 - val_loss: 1.7706\n",
      "Epoch 3/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.4811 - loss: 1.7324 - val_accuracy: 0.5061 - val_loss: 1.6364\n",
      "Epoch 4/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.5104 - loss: 1.6256 - val_accuracy: 0.5225 - val_loss: 1.5803\n",
      "Epoch 5/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.5293 - loss: 1.5598 - val_accuracy: 0.5355 - val_loss: 1.5496\n",
      "Epoch 6/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.5418 - loss: 1.5172 - val_accuracy: 0.5444 - val_loss: 1.5278\n",
      "Epoch 7/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.5509 - loss: 1.4873 - val_accuracy: 0.5403 - val_loss: 1.5520\n",
      "Epoch 8/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.5567 - loss: 1.4684 - val_accuracy: 0.5511 - val_loss: 1.5197\n",
      "Epoch 9/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.5620 - loss: 1.4496 - val_accuracy: 0.5487 - val_loss: 1.5270\n",
      "Epoch 10/10\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.5634 - loss: 1.4432 - val_accuracy: 0.5508 - val_loss: 1.5348\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_ohe, y_ohe, validation_split=0.2, callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [ True, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ohe[0].reshape((1, 40, 27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_ohe[0].reshape((1, 40, 27)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2556537e-06, 1.2102733e-01, 1.1308529e-06, 1.4314616e-05,\n",
       "       1.6170390e-05, 2.0247650e-01, 6.6902512e-06, 1.7490454e-06,\n",
       "       1.3461773e-03, 1.2410305e-01, 3.2819289e-06, 1.3510156e-07,\n",
       "       2.4752755e-04, 4.1886477e-05, 4.0077633e-05, 4.2724881e-01,\n",
       "       2.2451946e-05, 1.0808331e-05, 7.8866342e-03, 6.7308447e-06,\n",
       "       6.4243686e-06, 1.1320993e-01, 2.3141618e-05, 2.8381607e-04,\n",
       "       9.4277520e-06, 1.9599844e-03, 3.6056110e-06], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Œ We won't use output probabilities directly \n",
    "* We will sample from the predicted outputs using Temperature Softmax [see this](https://medium.com/@majid.ghafouri/why-should-we-use-temperature-in-softmax-3709f4e0161)\n",
    "\n",
    "* Basically, the ideas is that it would re-weight the probability distribution so that you can control how much surprising (i.e. higher temperature/entropy) or predictable (i.e. lower temperature/entropy) the next selected character would be\n",
    "    * ğŸ’¡ The concept of *Temperature* is used among many generative models\n",
    "        * e.g. LLMs like [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = sample(y_pred)\n",
    "indices_char[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ To use the model as a next characted generator for given seed text we need to create a feedback loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_931301/913138547.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n"
     ]
    }
   ],
   "source": [
    "whole_text = X[10].copy()\n",
    "seq = X[10].copy()\n",
    "for i in range(500):\n",
    "    paragraph_ohe = np.zeros((1, SEQ_LEN, len(chars)))\n",
    "    for t, char in enumerate(seq):\n",
    "        paragraph_ohe[0, t, char_indices[char]] = 1\n",
    "    y_pred = model.predict(paragraph_ohe)\n",
    "    c = sample(y_pred[0], 0.5)\n",
    "    next_char = indices_char[c]\n",
    "    whole_text.append(next_char)\n",
    "    seq = whole_text[-SEQ_LEN:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see that the model has only seen character-level data however it has learnt to generate existing words/phrases \n",
    "* And yes, the output is still far from ideal ğŸ™‚\n",
    "* ğŸ” How would you make it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o livedmr and mrs dursley of number four other and harrys and harry had a ron he was around the haw suddenly a his the windows and people that he was a rose the sign in his arrif it had been hagrids the said hagrid harry had to see harry snape seen and harry were a lot of the dursleys said hagrid and said harry the back to the back to a lot the great and hagrid as his he but a show they said harry of the than he was the mead him and hagrid didnt have been his and the the gryffindor so threwhere was before that was and had one a gaspes'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(whole_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…  Tasks for the lecture\n",
    "* Homework's on vacation mode thanks to the amount of new topics in this lecture, enjoy the break ğŸ™‚"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
