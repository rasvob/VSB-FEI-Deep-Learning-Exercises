{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm"
   },
   "source": [
    "# Deep Learning - Exercise 5\n",
    "\n",
    "This lecture is about advanced topics of the CNN such as Autoencoder for image reconstruction and Variatioanl autoencoder for generating images.\n",
    "\n",
    "We use MNIST dataset for the tasks as it is quite easy to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_05.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_05.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # images\n",
    "import numpy as np #numpy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import requests\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "from scipy.signal import convolve2d # convolutionof the 2D signals\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import scipy\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "def show_example(train_x, train_y, class_names):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(train_x[i].reshape(28,28), cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[train_y[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé What is an autoencoder? üîé\n",
    "* The autoencoder is a special type of neural network that is able to learn without the classes just from the input data \n",
    "    * It is equivalent to the feature extraction from the data\n",
    "    * üîé Can you name any use-case for such model?\n",
    "* What is the latent vector?\n",
    "* How can we compare two images?\n",
    "\n",
    "![Arch01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_05_enc_arch.png?raw=true)\n",
    "\n",
    "## The autoencoder uses binary-crossentropy as loss function\n",
    "* How is the function used?\n",
    "\n",
    "\n",
    "$BCE = -\\frac{1}{N}\\sum_{i=1}^{N}y_i*log(p(y_i))+(1-y_i*log(1-p(y_i)))$\n",
    "\n",
    "* üí° Beware the asymetry of the BCE function\n",
    "\n",
    "* **During the backpropagation process BCE cares more about very bright (1) or very dark pixels (0), but puts less effort on optimizing middle tones (0.5).**\n",
    "\n",
    "* Give the true value is 0.2, and autoencoder A predicts 0.1 while autoencoder B predicts 0.3. \n",
    "    * The loss for A would be: $‚àí(0.2 * log_{10}(0.1) + (1‚àí0.2) * log_{10}(1‚àí0.2)) = 0.278$    \n",
    "    * The loss for B would be: $‚àí(0.2 * log_{10}(0.3) + (1‚àí0.2) * log_{10}(1‚àí0.3)) = 0.228$\n",
    "\n",
    "* üìå Check [this](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) or [this](https://stats.stackexchange.com/questions/394582/why-is-binary-cross-entropy-or-log-loss-used-in-autoencoders-for-non-binary-da) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Let's start!\n",
    "* We will use *MNIST* dataset again for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mnist is the basic dataset for image classifaction\n",
    "dataset = tf.keras.datasets.mnist\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "train_x = train_x.reshape(*train_x.shape, 1)\n",
    "test_x = test_x.reshape(*test_x.shape, 1)\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 28x28 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "\n",
    "class_names = [str(x) for x in range(10)]\n",
    "class_count = len(class_names)\n",
    "print('Class count:', class_count, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I bet that you already remember the data üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_example(train_x, train_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract single image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = train_x.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The autoencoder definition is really straightforward\n",
    "* I hope that you would be able to define the architecture using the knowledge from previous lectures yourselves\n",
    "    * üîé Why do we use `padding='same'`?\n",
    "        * üí° The convolutional layer will add padding to the input such that the output has the same width and height as the input\n",
    "    * üîé Why do we have 128 numbers in the latent vector?\n",
    "    * What does the ``UpSampling2D`` layer?\n",
    "        * What interpolation methods do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder = keras.Sequential([\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=input_shape),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(8, (3,3), padding='same', activation='relu'),    \n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    # a 128 values of the minimized knowledge / features\n",
    "    keras.layers.Conv2D(8, (3,3), padding='same', activation='relu'),\n",
    "    keras.layers.UpSampling2D((2,2)),\n",
    "    keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "    keras.layers.UpSampling2D((2,2)),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    keras.layers.UpSampling2D((2,2)),\n",
    "    \n",
    "    keras.layers.Conv2D(1, (3,3), activation='sigmoid', padding='same')\n",
    "])\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = autoencoder.fit(train_x, train_x, validation_data=(valid_x, valid_x), epochs=10, batch_size=128)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now you can generate original and reconstructed images\n",
    "* ‚ö° You feed-forward the test image set and the model will provide the reconstructed image on the output\n",
    "* üîé How good is the reconstruction (just visually üôÇ)?\n",
    "    * On what parameter does the quality depend the most? \n",
    "    * üîé Do you think the output is blurry?\n",
    "        * Why?\n",
    "        * Do you remember the BCE loss function caveat? \n",
    "            * üí° Take a look at the white pixels brightness üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted = autoencoder.predict(test_x)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(test_x[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1+n)\n",
    "    plt.imshow(predicted[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We can also vizualize the encoded vectors\n",
    "* We will just re-construct the encoder architecture and feed it with our pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = keras.Sequential([\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(28,28,1)),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(8, (3,3), padding='same', activation='relu'),    \n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "])\n",
    "\n",
    "# encoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "encoder.set_weights(autoencoder.get_weights()[:6])\n",
    "\n",
    "for layer in encoder.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Take a look at the latent vectors\n",
    "* Do you see any differences among the vectors?\n",
    "    * üí° Compare vectors of *4* to other *4*s and then compare vectors of *4*s to vectors of *1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded = encoder.predict(test_x)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(encoded[i].reshape(8, 16).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    ax = plt.subplot(2, n, n+i+1)\n",
    "    plt.imshow(test_x[i].reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hey, but the classifier also has the latent vector after the encoder (CNN layers) part! ü§î\n",
    "* The latent vector generated using the autoencoder principle generate a compressed representation of the input.\n",
    "* Both models follow different set of goals and the goal of the autoencoder is different than from the classifier\n",
    "    * üí° Autoencoder want to compress the image into the latent vector in a way that it can be reconstructed with the best quality\n",
    "    * üí° Classifier want to embed the image into a latent vector as well, but the main goal is to create such representation that the vectors are different from each other so the model can distinguish among the images easily\n",
    "    * üîé Which setting of the model has the biggest impact on the criterion?\n",
    "\n",
    "* üí° Some variants of encoder are able to generate better representation with some features - a sparse autoencoders that generate sparse representation for example.\n",
    "    * üîé What is sparcity? Why would anyone want such property?\n",
    "\n",
    "### Can we use latent vectors from just the **encoder** part of the autoencoder as a *feature engineering* step for classifier? ü§î\n",
    "* üí° Yes, it is possible but given the different properties of the latent vectors, there would be a negative accuracy impact\n",
    "* **TL;DR: Use classifiers for classification tasks and autoencoder for autoencoder tasks**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Outlier Detection With Autoencoders\n",
    "\n",
    "> ### üìå If you cannot compress it, there is probably something wrong with it\n",
    "\n",
    "* A classical obstacle you stumble upon in data science and machine learning are the outliers\n",
    "* The concept of an outlier is intuitively clear to a human, yet there is no generally meaningful mathematical definition apart from simple hacks that involve the standard deviation or the interquartile range\n",
    "\n",
    "* üí° **By definition, outliers are not like other data points.**\n",
    "\n",
    "![Meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_05_meme_01.png?raw=true)\n",
    "\n",
    "* üîé Can you give some examples of an outlier in the data?\n",
    "\n",
    "## How can we use autoencoders for the outlier detection?\n",
    "* üí° A good autoencoder should be able to compress (encode) data into fewer dimensions and then decompress (decode) it again without introducing many errors\n",
    "    * An autoencoder tries to learn good encodings for a given dataset\n",
    "    * üí° Since most data points in the dataset are not outliers, the autoencoder will be influenced the most by \"normal\" data\n",
    "* **üìå If the autoencoder introduces a large error for some data point, this data point might be an outlier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° We can test it out\n",
    "* First, create sample dataset of 25 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_x = test_x[:25]\n",
    "sample_y = test_y[:25]\n",
    "\n",
    "show_example(sample_x, sample_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will add a noise in the data to create the **outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modified_x = np.clip(sample_x + 0.1 * np.random.normal(loc=0.0, scale=1.0, size=sample_x.shape), 0., 1.) \n",
    "modified_y = sample_y\n",
    "show_example(modified_x, modified_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first step is to create the reconstructed representation for the **original** samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_predicted = autoencoder.predict(sample_x)\n",
    "show_example(sample_predicted, sample_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Then we will create the reconstructed representation for the **noisy** samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modified_pred = autoencoder.predict(modified_x)\n",
    "show_example(modified_pred, modified_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Now we can compute the reconstruction errors for both of the image sets using Frobenius norm\n",
    "* https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
    "* Frobenius norm (Euclidean norm) ~ Root of squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_norms = [np.linalg.norm(sample_x[i].reshape(28,28) - sample_predicted[i].reshape(28,28)) for i in range(len(sample_x))]\n",
    "modified_norms = [np.linalg.norm(modified_x[i].reshape(28,28) - modified_pred[i].reshape(28,28)) for i in range(len(sample_x))]\n",
    "\n",
    "sample_mean, sample_std = np.mean(sample_norms), np.std(sample_norms)\n",
    "modified_mean, modified_std = np.mean(modified_norms), np.std(modified_norms)\n",
    "\n",
    "sample_mean, modified_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final step is the visualization of the errors distribution\n",
    "* üîé Can you see an outlier data in the plot?\n",
    "    * How would you detect them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "xmin = np.min([sample_mean-4*sample_std, modified_mean-4*modified_std])\n",
    "xmax = np.max([sample_mean+4*sample_std, modified_mean+4*modified_std])\n",
    "\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "y = scipy.stats.norm.pdf(x,sample_mean,sample_std)\n",
    "\n",
    "plt.plot(x,y)\n",
    "\n",
    "y = scipy.stats.norm.pdf(x,modified_mean,modified_std)\n",
    "\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.bar(x=sample_norms, height=[0.6 for x in sample_norms], width=0.01)\n",
    "plt.bar(x=modified_norms, height=[0.6 for x in sample_norms], width=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìí Variational Autoencoder (VAE)\n",
    "* A variational autoencoder (VAE) provides a probabilistic manner for describing an observation in latent space\n",
    "\n",
    "* Rather than building an encoder which outputs a single value to describe each latent state attribute, we'll formulate our encoder to describe a probability distribution for each latent attribute\n",
    "\n",
    "* üí° Let's suppose we've trained an autoencoder model on a large dataset of faces with a encoding dimension of 6\n",
    "    * An ideal autoencoder will learn descriptive attributes of faces such as skin color, whether or not the person is wearing glasses, etc. in an attempt to describe an observation in some compressed representation\n",
    "\n",
    "![VAE01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_05_vae_01.png?raw=true)\n",
    "\n",
    "* üìå **Using a variational autoencoder, we can describe latent attributes in probabilistic terms**\n",
    "\n",
    "![VAE02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_05_vae_02.png?raw=true)\n",
    "\n",
    "![VAE03](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_05_vae_03.png?raw=true)\n",
    "\n",
    "* By constructing our encoder model to output a range of possible values (a statistical distribution) from which we'll randomly sample to feed into our decoder model, we're essentially enforcing a continuous, smooth latent space representation \n",
    "    * üí° For any sampling of the latent distributions, we're expecting our decoder model to be able to accurately reconstruct the input\n",
    "    * üìå Thus, values which are nearby to one another in latent space should correspond with very similar reconstructions\n",
    "\n",
    "![VAE04](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_05_vae_04.png?raw=true)\n",
    "\n",
    "\n",
    "* **You can see [this](https://www.jeremyjordan.me/variational-autoencoders/) for very good explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mnist is the basic dataset for image classifaction\n",
    "dataset = tf.keras.datasets.mnist\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "train_x = train_x.reshape(*train_x.shape, 1)\n",
    "test_x = test_x.reshape(*test_x.shape, 1)\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 28x28 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "\n",
    "class_names = [str(x) for x in range(10)]\n",
    "class_count = len(class_names)\n",
    "print('Class count:', class_count, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö† There are some differences to the traditional classification task approach\n",
    "\n",
    "* We will reshape data from 28x28 to 32x32x1 shape\n",
    "    * We will use subsampling/pooling in the model so we expect division of width and height by 2 with each of the subsampling step\n",
    "    * ‚ö† Dividing 28 by 2 multiple times will lead to sequence -> `28 >> 14 >> 7 >> ...` \n",
    "        * Number 7 could be an issue for reconstruction because the subsampling uses integer division (7 / 2 = 3) but 2*3 = 6\n",
    "\n",
    "* üí° Resizing to 32x32 will save us from this issue because we will get sequence `32 >> 16 >> 8 >> 4 >> 2 >> 1` which is no problem to reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = 32\n",
    "\n",
    "train_x_res = np.array([cv2.resize(x, (size, size)) for x in train_x])\n",
    "test_x_res = np.array([cv2.resize(x, (size, size)) for x in test_x])\n",
    "valid_x_res = np.array([cv2.resize(x, (size, size)) for x in valid_x])\n",
    "\n",
    "train_x_r = train_x_res.reshape(-1, size, size, 1).astype('float32')\n",
    "test_x_r = test_x_res.reshape(-1, size, size, 1).astype('float32')\n",
    "valid_x_r = valid_x_res.reshape(-1, size, size, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x_r.shape, test_x_res.shape, valid_x_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = train_x_r.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just for the compatibility of TF versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the latent vector dimension first\n",
    "* Latent dimension define frow how many distributions we want to sample\n",
    "* üí° Each distribution a is Gaussian one, so we are creating basically multivariate Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The last two Dense layers represent vectors of mean and logarithm of variance of the individual distributions\n",
    "* Sampling function will combine these vectors with samples from Normal (Gaussian) distribution ~ N(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    mean_mu, log_var = args\n",
    "    epsilon = keras.backend.random_normal(shape=keras.backend.shape(mean_mu), mean=0., stddev=1.)\n",
    "    return mean_mu + keras.backend.exp(log_var/2)*epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° It's worth to mention that we use strides=2 in the Conv2D layers instead of strides=1 so we are doing subsampling, beside the filtering, in this layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_input = keras.layers.Input(shape=input_shape)\n",
    "x = keras.layers.Conv2D(28, (3,3), strides=2, padding='same', activation='relu')(encoder_input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(64, (3,3), strides=2, padding='same', activation='relu')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "before_flatten_shape = keras.backend.int_shape(x)[1:]\n",
    "print(before_flatten_shape)\n",
    "\n",
    "x = keras.layers.Flatten()(x)\n",
    "mean_mu = keras.layers.Dense(latent_dim, name='mean_mu')(x)\n",
    "log_var = keras.layers.Dense(latent_dim, name='log_var')(x)\n",
    "encoder_output = keras.layers.Lambda(sampling)([mean_mu, log_var])\n",
    "\n",
    "enc_model = keras.Model(encoder_input, encoder_output)\n",
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can take a look at the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(enc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Now we need to define the decoder part\n",
    "* Input shape is equal to the latent dimensions count\n",
    "* We use Conv2DTranspose, sometimes called de-conv layer\n",
    "* A simple way of how to think about it is that it both performs the upsample operation and interprets the coarse input data to fill in the detail while it is upsampling\n",
    "    * üí° It is like a layer that combines the UpSampling2D and Conv2D layers into one layer\n",
    "    * Models that use these layers can be referred to as deconvolutional networks, or deconvnets\n",
    "* üìå Unlike upsampling, it involves trainable parameters and **can learn to add new information** while increasing the size of the input\n",
    "\n",
    "* A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so **instead of mapping pixels to features it does the opposite**\n",
    "\n",
    "### strides=2 in the Conv2DTranspose is responsible for the upsampling operation\n",
    "* The Reshape layers creates from the vector of 64 elements matrix 8x8 which can be upsampled to the original size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_input = keras.layers.Input(shape = (latent_dim,) , name = 'decoder_input')\n",
    "x = keras.layers.Dense(np.prod(before_flatten_shape))(decoder_input)\n",
    "x = keras.layers.Reshape(before_flatten_shape)(x)\n",
    "x = keras.layers.Conv2DTranspose(128, (3,3), strides=2, padding='same', activation='relu')(x)\n",
    "x = keras.layers.Conv2DTranspose(1, (3,3), strides=2, padding='same', activation='sigmoid')(x)\n",
    "decoder_output = x\n",
    "\n",
    "dec_model = keras.Model(decoder_input, decoder_output)\n",
    "dec_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Now we will just connect the two parts together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae_input = encoder_input\n",
    "\n",
    "vae_output = dec_model(encoder_output)\n",
    "\n",
    "vae_model = keras.Model(vae_input, vae_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° VAE uses specific loss function which we have to define ourselves\n",
    "* The loss function combines RMSE with KL-divergence\n",
    "    * KL divergence score, quantifies how much one probability distribution differs from another probability distribution.\n",
    "        * https://machinelearningmastery.com/divergence-between-probability-distributions/\n",
    "        * https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8\n",
    "\n",
    "* We use the KL-divergence score to penalize the activation values of the mean and log_variance layers if they differ from N(0, 1) distribution - \n",
    "    * We want to be as close as possible to this distribution\n",
    "    * üîé Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def total_loss_closure(mean_mu, log_var):\n",
    "    def r_loss(y_true, y_pred):\n",
    "        return keras.backend.mean(keras.backend.square(y_true - y_pred), axis=[0, 1])\n",
    "\n",
    "    def kl_loss(y_true, y_pred):\n",
    "        kl_loss =  -0.5 * keras.backend.sum(1 + log_var - keras.backend.square(mean_mu) - keras.backend.exp(log_var), axis = 1)\n",
    "        return kl_loss\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        LOSS_FACTOR = 10000\n",
    "        return LOSS_FACTOR*r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae_model.compile(optimizer='adam', loss = total_loss_closure(mean_mu, log_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "history = vae_model.fit(train_x_r, train_x_r, validation_data=(valid_x_r, valid_x_r), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will the reconstructed images look like?\n",
    "* üöÄ Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted = vae_model.predict(test_x_r)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(test_x_r[i].reshape(32, 32))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1+n)\n",
    "    plt.imshow(predicted[i].reshape(32, 32))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use VAE for creating new images? ü§î\n",
    "* If we use just the decoder part of the model and feed it samples from distribution N(0, 1) we can generate new images with the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "reconst_images = dec_model.predict(np.random.normal(0,1,size=(n,latent_dim)))\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(reconst_images[i].reshape(32, 32))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà We can display even the whole manifold\n",
    "* üîé How can we select scale?\n",
    "\n",
    "* üí° 3-sigma rule is helpful!\n",
    "   * ùëÉ(ùúá ‚àí ùëòùúé < ùëã < ùúá + ùëòùúé) = 0.954 for k=2\n",
    "   * k = 2, ùúá = 0, ùúé = 1 in our case so 95,4% of numbers will be in interval <-2, 2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_latent(decoder):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 32\n",
    "    scale = 2.0\n",
    "    figsize = 15\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "#     plt.xticks(pixel_range, sample_range_x)\n",
    "#     plt.yticks(pixel_range, sample_range_y)\n",
    "#     plt.xlabel(\"z[0]\")\n",
    "#     plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "plot_latent(dec_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ  Tasks for the lecture (2p)\n",
    "\n",
    "* Implement an autoencoder model that is able to denoise an image\n",
    "    * üí° Implement it on the data defined below\n",
    "\n",
    "* The denoising autoencoder is an autoencoder that will learn how to remove random noise from the images\n",
    "    * First, noisy images have to be generated\n",
    "    * Then the autoencoder need to be defined and trained\n",
    "    * In the end, denoised images may be reconstructed\n",
    "    \n",
    "* Experiment with **noise_factor** values of `0.1`, `0.5` and `0.9` \n",
    "    * üìíWrite down a short summary about the reconstruction capabilities of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_factor = 0.5\n",
    "noisy_train_x = np.clip(train_x + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_x.shape), 0., 1.)\n",
    "noisy_valid_x = np.clip(valid_x + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=valid_x.shape), 0., 1.)\n",
    "noisy_test_x = np.clip(test_x + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=test_x.shape), 0., 1.) \n",
    "\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(2*n, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(train_x[i].reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    ax = plt.subplot(2, n, n+i+1)\n",
    "    plt.imshow(noisy_train_x[i].reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
