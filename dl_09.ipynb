{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm",
    "tags": []
   },
   "source": [
    "# Deep Learning - Exercise 8\n",
    "\n",
<<<<<<< HEAD
    "This lecture is focused on the transformer models using Huggingface wrapper for Tensorflow 2\n",
    "\n",
    "The lecture is based on [official Huggingface tutorials](https://huggingface.co/transformers/v4.2.2/notebooks.html)"
=======
    "This lecture is focused on using the attention mechanism in deep learning models.\n",
    "\n",
    "We recomment reading [this](https://analyticsindiamag.com/a-beginners-guide-to-using-attention-layer-in-neural-networks/) for more detailed information."
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_08.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_08.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "! pip install transformers datasets huggingface_hub evaluate"
=======
    "!pip install keract\n",
    "!pip install attention"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import string as tf_string\n",
    "from tensorflow.keras.layers import TextVectorization\n",
<<<<<<< HEAD
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
=======
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Layer\n",
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
    "\n",
    "from sklearn.model_selection import train_test_split # \n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import scipy\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "import tqdm\n",
    "import io\n",
<<<<<<< HEAD
=======
    "import os\n",
    "\n",
    "import unicodedata, re, string\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "from keract import get_activations\n",
    "from keras import Input, Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "\n",
    "os.environ['KERAS_ATTENTION_DEBUG'] = '1'\n",
    "from attention import Attention\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, create_optimizer, AutoModelForSequenceClassification\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "print(transformers.__version__)"
=======
    "# tf.config.set_visible_devices([], 'GPU')"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "SEED = 13"
=======
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# What is the idea behind transformer models? ðŸ”Ž\n",
    "\n",
    "## The good news is that you already know most of the things from the Attention-focused lecture ðŸ™‚\n",
    "\n",
    "* The main idea behind the transformer architecture is to use **self-attention mechanisms** to capture the relationships between different words in a sentence. \n",
    "* Self-attention allows the model to focus on different parts of the input sequence when processing each word in the sequence. \n",
    "    * This allows the model to take into account the context and dependencies between different words in the sequence, which is important for many NLP tasks.\n",
    "\n",
    "![att](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_008_meme_02.png?raw=true)\n",
    "\n",
    "\n",
    "## Is there any difference when you compare it to the RNN model? ðŸ”Ž\n",
    "* The main difference between the transformer architecture and recurrent neural networks (RNNs) is the way they handle sequential data. \n",
    "* RNNs process sequential data one element at a time, using hidden states to capture information about the previous elements in the sequence. \n",
    "    * In contrast, the transformer architecture processes the entire sequence at once, using self-attention mechanisms to capture dependencies between different elements in the sequence.\n",
    "* The transformer architecture is **more parallelizable**. Because the transformer architecture processes the entire sequence at once, it can be trained more efficiently on parallel hardware like GPUs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# We will use the ðŸ’¡ BERT model for sample classification task from the GLUE Benchmark\n",
    "\n",
    "## We will test the model on ðŸ’¡ CoLA dataset which is meant for classification as we need to label every sencente if it is grammatically correct or not\n",
    "\n",
    "## You can use any of these datasets in this notebook for your later experiments\n",
    "\n",
    "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a  dataset containing sentences labeled grammatically correct or not.\n",
    "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
    "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
    "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
    "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
    "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
    "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
    "- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
    "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can select any task from the list below\n",
    "* The **batch_size** should be set according to your GPU memory ðŸ’¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\n",
    "    \"cola\",\n",
    "    \"mnli\",\n",
    "    \"mrpc\",\n",
    "    \"qnli\",\n",
    "    \"qqp\",\n",
    "    \"rte\",\n",
    "    \"sst2\",\n",
    "    \"stsb\",\n",
    "    \"wnli\",\n",
    "]\n",
    "\n",
    "task = \"cola\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use the **datasets** library to download the data and the **evaluate** library to get the metric we need to use for evaluation (to compare our model to the benchmark). \n",
    "    * This can be easily done with the load_dataset function from datasets and and the load function from evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", task)\n",
    "metric = load(\"glue\", task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `dataset` object itself is [DatasetDict](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We should always take a look at the example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **metric** is an instance of [datasets.Metric](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric)\n",
    "    * It simplify the process of model evaluation so we don't have to use raw scikit-learn functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can call its compute method with your predictions and labels directly and it will return a dictionary with the metric(s) value\n",
    "* The metric is chosen by the task name we specified so we use the right metric for the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "* Before we can feed those texts to our model, we need to preprocess them. This is done by a Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "* To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "* That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nothing new here - just a regular word2id mapping ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, this is a sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names\n",
    "* Remember that sentence, label, idx dict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can them write the function that will preprocess our samples. \n",
    "* We just feed them to the tokenizer with the arguments truncation=True and padding='longest. \n",
    "    * This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model, and all inputs will be padded to the maximum input length to give us a single input array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using this code we can tokenize the sentences in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_function(dataset[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the map method of our dataset object we created earlier. \n",
    "* This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_tokenizer_columns = set(dataset[\"train\"].features)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenizer_columns = list(set(encoded_dataset[\"train\"].features) - pre_tokenizer_columns)\n",
    "print(\"Columns added by tokenizer:\", tokenizer_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-tuning the model\n",
    "* Now that our data is ready, we can download the pretrained model and fine-tune it. \n",
    "\n",
    "* Since all our tasks are about sentence classification, we use the `TFAutoModelForSequenceClassification` class.\n",
    "* The only thing we have to specify is the number of labels for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task == \"stsb\" else 2\n",
    "if task == \"stsb\":\n",
    "    num_labels = 1\n",
    "elif task.startswith(\"mnli\"):\n",
    "    num_labels = 3\n",
    "else:\n",
    "    num_labels = 2\n",
    "    \n",
    "# This next little bit is optional, but will give us cleaner label outputs later\n",
    "# If you're using a task other than CoLA, you will probably need to change these\n",
    "# to match the label names for your task!\n",
    "id2label = {0: \"Invalid\", 1: \"Valid\"}\n",
    "label2id = {val: key for key, val in id2label.items()}\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")"
=======
    "# ðŸ”Ž What is Attention mechanism?\n",
    "\n",
    "- When we think about the English word â€œAttentionâ€, we know that it means directing your focus at something and taking greater notice. \n",
    "- The Attention mechanism in Deep Learning is based off this concept of directing your focus, and it pays greater attention to certain factors when processing the data.\n",
    "- Paying attention to important information is necessary and it can improve the performance of the model. \n",
    "- **Attention mechanism can help a neural network to memorize long sequences of the information**\n",
    "    - Remember the RNN and even LSTM long-context issues?\n",
    "\n",
    "## ðŸ”Ž Can you imagine some use-cases where it can help us? \n",
    "\n",
    "\n",
    "\n",
    "### The process is usually computed in these few steps\n",
    "\n",
    "![Img00](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_04.png?raw=true)\n",
    "\n",
    "- Letâ€™s say that we have an input with n sequences and output y with m sequence in a network.\n",
    "    - $x=[x_1, x_2, ..., x_n]$\n",
    "    - $y = [y_1, y_2, ..., y_n]$\n",
    "    \n",
    "- The encoder which we are using in the network is a bidirectional LSTM network where it has a forward hidden state and a backward hidden state.\n",
    "    - Representation of the encoder state can be done by concatenation of these forward and backward states. \n",
    "    - $h_i = [h_i^{L2R}, h_i^{R2L}]$\n",
    "\n",
    "- The hidden state is:\n",
    "    - $s_t=f(s_{t-1}, y_{t-1}, c_t)$\n",
    "    \n",
    "- For the output word at position t, the context vector $C_t$ can be the sum of the hidden states of the input sequence.\n",
    "- Thus we have:\n",
    "\n",
    "![Img02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_02.png?raw=true)\n",
    "\n",
    "- Here we can see that the sum of the hidden state is weighted by the alignment scores. \n",
    "- We can say that ${\\alpha_{t,i}}$  are the weights that are responsible for defining how much of each sourceâ€™s hidden state should be taken into consideration for each output.\n",
    "\n",
    "- There can be various types of alignment scores according to their geometry. \n",
    "    - It can be either linear or in the curve geometry. \n",
    "\n",
    "- Below are some of the popular attention mechanisms:\n",
    "\n",
    "![Img03](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_03.png?raw=true)\n",
    "\n",
    "## There are many variants of the mechanism in the wild but the basic computation process is the same\n",
    "\n",
    "## The very common and easy to understand example is **Self-Attention Mechanism**\n",
    "- When an attention mechanism is applied to the network so that it can relate to different positions of a single sequence and can compute the representation of the same sequence, it can be considered as self-attention\n",
    "\n",
    "![Img01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_01.png?raw=true)\n",
    "\n",
    "- Here in the image, the red color represents the word which is currently learning and the blue color is of the memory, and the intensity of the color represents the degree of memory activation. "
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## One of the last steps is to create a TF datasets which will feed the data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_key = (\n",
    "    \"validation_mismatched\"\n",
    "    if task == \"mnli-mm\"\n",
    "    else \"validation_matched\"\n",
    "    if task == \"mnli\"\n",
    "    else \"validation\"\n",
    ")\n",
    "\n",
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    encoded_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_validation_dataset = model.prepare_tf_dataset(\n",
    "    encoded_dataset[validation_key],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batches_per_epoch = len(encoded_dataset[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps\n",
    ")\n",
    "model.compile(optimizer=optimizer)"
=======
    "# We will use Attention layer from the library first and demonstrate the usage for Find Max Task"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## The last thing to define is how to compute the metrics from the predictions. \n",
    "* We need to define a function for this, which will just use the metric we loaded earlier. \n",
    "    * The only preprocessing we have to do is to take the argmax of our predicted logits\n",
    "\n",
    "* In addition, let's wrap this metric computation function in a KerasMetricCallback. \n",
    "    * This callback will compute the metric on the validation set each epoch, including printing it and logging it for other callbacks like TensorBoard and EarlyStopping."
=======
    "## We need to define callback for vizualizing the attentions maps"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def compute_metrics(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_dataset)"
=======
    "class VisualizeAttentionMap(Callback):\n",
    "\n",
    "    def __init__(self, model, x):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.x = x\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        attention_map = get_activations(self.model, self.x, layer_names='attention_weight')['attention_weight']\n",
    "        x = self.x[..., 0]\n",
    "        plt.close()\n",
    "        fig, axes = plt.subplots(nrows=3, figsize=(10, 8))\n",
    "        maps = [attention_map, create_argmax_mask(attention_map), create_argmax_mask(x)]\n",
    "        maps_names = ['attention layer (continuous)', 'attention layer - argmax (discrete)', 'ground truth (discrete)']\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            im = ax.imshow(maps[i], interpolation='none', cmap='jet')\n",
    "            ax.set_ylabel(maps_names[i] + '\\n#sample axis')\n",
    "            ax.set_xlabel('sequence axis')\n",
    "            ax.xaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticks([])\n",
    "        cbar_ax = fig.add_axes([0.75, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(im, cax=cbar_ax)\n",
    "        fig.suptitle(f'Epoch {epoch} - training\\nEach plot shows a 2-D matrix x-axis: sequence length * y-axis: '\n",
    "                     f'batch/sample axis. \\nThe first matrix contains the attention weights (softmax).'\n",
    "                     f'\\nWe manually apply argmax on the attention weights to see which time step ID has '\n",
    "                     f'the strongest weight. \\nFinally, the last matrix displays the ground truth. The task '\n",
    "                     f'is solved when the second and third matrix match.')\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "\n",
    "def create_argmax_mask(x):\n",
    "    mask = np.zeros_like(x)\n",
    "    for i, m in enumerate(x.argmax(axis=1)):\n",
    "        mask[i, m] = 1\n",
    "    return mask"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## We can now finetune our model by just calling the fit method. \n",
    "* Be sure to pass the TF datasets, and not the original datasets! "
=======
    "# We will create training examples first\n",
    "- Goal of the task is to predict the maximum value of the input array"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "callbacks = [metric_callback]\n",
    "\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
=======
    "seq_length = 10\n",
    "num_samples = 100000\n",
    "# https://stats.stackexchange.com/questions/485784/which-distribution-has-its-maximum-uniformly-distributed\n",
    "# Choose beta(1/N,1) to have max(X_1,...,X_n) ~ U(0, 1) => minimizes amount of knowledge.\n",
    "# If all the max(s) are concentrated around 1, then it makes the task easy for the model.\n",
    "x_data = np.random.beta(a=1 / seq_length, b=1, size=(num_samples, seq_length, 1))\n",
    "y_data = np.max(x_data, axis=1)"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Now we can do inference using our own inputs\n",
    "* Now, let's make up some sentences and see if the model can classify them properly!\n",
    "* The first sentence is valid English, but the second one makes a grammatical mistake."
=======
    "## The data looks like this"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "sentences = [\n",
    "    \"The judge told the jurors to think carefully.\",\n",
    "    \"The judge told that the jurors to think carefully.\"\n",
    "]"
=======
    "x_data[0], y_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_data[1], y_data[2]"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## To feed them into our model, we'll need to tokenize them and then get our model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized = tokenizer(sentences, return_tensors=\"np\", padding=\"longest\")\n",
    "\n",
    "outputs = model(tokenized).logits\n",
    "\n",
    "classifications = np.argmax(outputs, axis=1)\n",
    "print(classifications)"
=======
    "# We use just very simple LSTM-based model with attention applied to it\n",
    "\n",
    "## ðŸ’¡ What is the intuition behind using attention? ðŸ’¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(seq_length, 1))\n",
    "x = LSTM(128, return_sequences=True)(model_input)\n",
    "x = Attention()(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "model = Model(model_input, x)\n",
    "\n",
    "model.compile(loss='mae')"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "classifications = [model.config.id2label[output] for output in classifications]\n",
    "print(classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ’¡ But how can we utilize such models in more std. task setup - I have data in Pandas DF and what is next?\n",
    "* Let's do such use-case together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'text': dataset['train']['text'], 'labels': dataset['train']['label']})\n",
    "df_test = pd.DataFrame({'text': dataset['test']['text'], 'labels': dataset['test']['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.labels.value_counts()"
=======
    "model.summary()"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Ok, Pandas seems ready ðŸ™‚\n",
    "* The easies way is to wrap the Pandas dataset in HF Dataset object and proceed with their API"
=======
    "# Let's train the model\n",
    "## ðŸ”Ž Take a look at the attention output - what is the ideal state?"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "hf_df_train = Dataset.from_pandas(df_train)\n",
    "hf_df_test = Dataset.from_pandas(df_test)"
=======
    "max_epoch = 100\n",
    "# visualize the attention on the first 12 samples.\n",
    "visualize = VisualizeAttentionMap(model, x_data[0:12])\n",
    "model.fit(x_data, y_data, epochs=max_epoch, validation_split=0.2, callbacks=[visualize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now as we know how the mechanism works we can employ it for the sentiment analysis task\n",
    "- We will use Yelp dataset which contains reviews of restaurants with either positive (1) or negative (0) labels assigned"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## We can split the data into train and valid subsets"
=======
    "## Download and load the dataset"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "ds = hf_df_train.train_test_split(test_size=0.2, shuffle=True)"
=======
    "path_to_file = tf.keras.utils.get_file('yelp_labelled.txt', 'https://raw.githubusercontent.com/rasvob/VSB-FEI-Deep-Learning-Exercises/main/datasets/yelp_labelled.txt')"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "ds"
=======
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(path_to_file) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [x.rstrip() for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines_dict = [{'Text': x[:-1].rstrip(), 'Label': int(x[-1])} for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(lines_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## And add validation set"
=======
    "## We will use TextVectorization layer as usuall and create baseline model without attention"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "ds_tst = ds['test']\n",
    "ds['valid'] = ds_tst\n",
    "ds['test'] = hf_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['valid'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)"
=======
    "embedding_dim = 64 # Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN\n",
    "max_tokens = 3000\n",
    "sequence_length = 32 # Output dimension after vectorizing - words in vectorited representation are independent\n",
    "\n",
    "vect_layer = TextVectorization(max_tokens=max_tokens, output_mode='int', output_sequence_length=sequence_length)\n",
    "vect_layer.adapt(df.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = vect_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Vocabulary example: ', vocab[:10])\n",
    "print('Vocabulary shape: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Text, df.Label, test_size=0.20, random_state=13, stratify=df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train')\n",
    "print(y_train.value_counts())\n",
    "print('Test')\n",
    "print(y_test.value_counts())"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# âš  BEWARE: The label columns must be named as **labels** because the model expects this name!"
=======
    "## Let's define very simple model first"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_test_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets['test'],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_valid_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets['valid'],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer\n",
    ")"
=======
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(len(vocab), output_dim=embedding_dim, embeddings_regularizer=keras.regularizers.l2(.001))(x_v)\n",
    "x = LSTM(50, dropout=0.3,recurrent_dropout=0.4)(emb)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.tf',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batches_per_epoch = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps\n",
    ")\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_valid_dataset,\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test = np.array(ds['test']['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_f = np.argmax(y_pred.logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_f"
=======
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train.values, y_train.values, validation_data=(X_test.values, y_test.values), callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Now we can compute an accuracy_score like usually"
=======
    "# Now we will create our own Attention layer and add it to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_meme_01.jpg?raw=true)\n",
    "\n"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "accuracy_score(y_true=y_test, y_pred=y_pred_f)"
=======
    "class MyAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyAttention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(MyAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        dot = K.dot(x,self.W)+self.b\n",
    "        print('dot', dot.shape)\n",
    "        th = K.tanh(dot)\n",
    "        print('th', th.shape)\n",
    "        et=K.squeeze(th,axis=-1)\n",
    "        print('squeeze', et.shape)\n",
    "        at=K.softmax(et)\n",
    "        print('softmax', et.shape)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        print('expand_dims', et.shape)\n",
    "        output=x*at\n",
    "        print('output', output.shape)\n",
    "        res = K.sum(output,axis=1)\n",
    "        print('res', res.shape)\n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(MyAttention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(len(vocab), output_dim=embedding_dim, embeddings_regularizer=keras.regularizers.l2(.001))(x_v)\n",
    "x = LSTM(128, dropout=0.3,recurrent_dropout=0.2, return_sequences=True)(emb)\n",
    "x = MyAttention()(x)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.tf',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train.values, y_train.values, validation_data=(X_test.values, y_test.values),callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "![dude](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_008_meme_01.png?raw=true)"
=======
    "# ðŸ”Ž Can you notice any difference in the model accuracy or training? ðŸ”Ž "
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.6"
=======
   "version": "3.10.9"
>>>>>>> 9e627307931ad41b060c151af7df5649b9d912d1
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
