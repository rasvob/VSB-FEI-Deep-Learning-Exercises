{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm"
   },
   "source": [
    "# Deep Learning - Exercise 6\n",
    "\n",
    "The aim of the lecture is to learn how to use recurrent neural networks (RNN) for text data analysis, specifically focusing on sentiment analysis tasks using Twitter data.\n",
    "\n",
    "**Core Concepts**\n",
    "* üß† Recurrent neural networks for sequence processing\n",
    "* üìù Sentiment analysis of textual data\n",
    "* üê¶ Twitter dataset utilization\n",
    "* üî§ GloVe embeddings for word representation\n",
    "* üìä Text classification by sentiment\n",
    "\n",
    "You can download the dataset from [this link](https://github.com/MohamedAfham/Twitter-Sentiment-Analysis-Supervised-Learning/tree/master/Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_06.ipynb)\n",
    "[Download from Github](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_06.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import string as tf_string\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "\n",
    "from sklearn.model_selection import train_test_split # \n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy\n",
    "import itertools\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìí We will work with LSTM/GRU layers\n",
    "* üîéDo you know anything abour RNN in general?\n",
    "    * How are they different from FCNN?\n",
    "\n",
    "![Meme02](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_06_meme_02.png?raw=true)\n",
    "\n",
    "* How is pure RNN and LSTM/GRU layer different?\n",
    "    * üîé What issue of RNN do they address?\n",
    "* Can you imagine some use-cases for RNN?\n",
    "    * Can you imagine some limits of ML/DL solutions in the usecases as well?\n",
    "\n",
    "## üîé We have some new packages today üîé\n",
    "* Below is a short description of them, check out the URLs for more details and API \n",
    "\n",
    "### üìå NLTK\n",
    "* NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "    * https://www.nltk.org/\n",
    "\n",
    "### üìå TextBlob\n",
    "* TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "    * https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unicodedata, re, string\n",
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punkt Sentence Tokenizer\n",
    "* üîé Why do we use tokenizers?\n",
    "* This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. \n",
    "    * It must be trained on a large collection of plaintext in the target language before it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/raw/main/datasets/train_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the classes are highly imbalanced, because we have only 2242 negative tweets compared to positive ones number\n",
    "* üîé What will be impacted by class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° We can see that sentences are of similar length regardless the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df.tweet.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='label', y='length', data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the text data are full of noise\n",
    "\n",
    "* üí° Social posts suffer the most from this effect\n",
    "    * The text is full of hashtags, emojis, @mentions and so on\n",
    "    * These parts usually don't influence the sentiment score much\n",
    "* üí° Although most advanced models usually extract even this features because e.g. emojis can help you with the sarcasm understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at few examples, it will share many of these caveates which we've just discussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in df.loc[:10, 'tweet']:\n",
    "    print(x, '\\n', '-'*len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìí We have a few specific pre-processing techniques for the text data\n",
    "* üí° Benefits of using these techniques varies from approach to approach\n",
    "    * However it is good to have at least some knowledge about them\n",
    "\n",
    "## Stemming\n",
    "* Stemming is the process of producing morphological variants of a root/base word\n",
    "    * Stemming programs are commonly referred to as stemming algorithms or stemmers\n",
    "* üí° A stemming algorithm reduces the words ‚Äúchocolates‚Äù, ‚Äúchocolatey‚Äù, ‚Äúchoco‚Äù to the root word, ‚Äúchocolate‚Äù and ‚Äúretrieval‚Äù, ‚Äúretrieved‚Äù, ‚Äúretrieves‚Äù reduce to the stem ‚Äúretrieve‚Äù\n",
    "\n",
    "#### ‚ö° Examples of stemming:\n",
    "* chocolates, chocolatey, choco : **chocolate**\n",
    "* retrieval, retrieved, retrieves : **retrieve**\n",
    "\n",
    "\n",
    "## Lemmatization \n",
    "* Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item\n",
    "* üí° Lemmatization is similar to stemming but it brings context to the words\n",
    "    * üí° It links words with similar meaning to one word\n",
    "\n",
    "#### ‚ö° Examples of lemmatization:\n",
    "* rocks : **rock**\n",
    "* corpora : **corpus**\n",
    "* better : **good**\n",
    "\n",
    "### Both techiques can be used in the preprocessing pipeline\n",
    "* You have to decide if it is beneficial to you, because this steps leads to generalization of the data by itself\n",
    "    * üí° You will definitely lose some pieces of the information!\n",
    "\n",
    "# üìå Embedding note\n",
    "* **If you use some form of embedding like Word2Vec or Glove, it is better to skip this steps because during the embedding vocabulary building process it was skipped as well** üôÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You don't have to code the pre-process steps yourself üôÇ\n",
    "* We have already prepared the most common functions used\n",
    "    * üí° Modify function `normalize(...)` for different step combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(\"\\d+\", \"\", word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "# words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "#    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return tweet_blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we must tokenize sentences and remove puncuation \n",
    "* We will use the `TextBlob` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Words'] = df['tweet'].apply(form_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize sentences \n",
    "* We want only ascii and lowercase characters and we also want to get rid of numbers in the strings\n",
    "\n",
    "### You can always experiments with different preprocessing steps! üôÇ\n",
    "* üí° The steps choice usually depends on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Words_normalized'] = df['Words'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the 'user' word from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Words_normalized_no_user'] = df['Words_normalized'].apply(lambda x: [y for y in x if 'user' not in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° We can see that no pre-processing is ideal and we have to fix some issues by ourselves\n",
    "* e.g. n't splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.tweet.iloc[1])\n",
    "print(df.Words_normalized_no_user.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_nt(words):\n",
    "    st_res = []\n",
    "    for i in range(0, len(words) - 1):\n",
    "        if words[i+1] == \"n't\" or words[i+1] == \"nt\":\n",
    "            st_res.append(words[i]+(\"n't\"))\n",
    "        else:\n",
    "            if words[i] != \"n't\" and words[i] != \"nt\":\n",
    "                st_res.append(words[i])\n",
    "    return st_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Words_normalized_no_user_fixed'] = df['Words_normalized_no_user'].apply(fix_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° The issue is now fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.tweet.iloc[1])\n",
    "print(df.Words_normalized_no_user.iloc[1])\n",
    "print(df.Words_normalized_no_user_fixed.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can join the text into single string again for each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Clean_text'] = df['Words_normalized_no_user_fixed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Clean_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Let's take a look at the most common words in corpus\n",
    "* It is one of the usual EDA step for text data\n",
    "    * ‚ö† Without the preprocessing there will be a lot of so-called *stopwords*\n",
    "\n",
    "## üîé Do you know what the term **stopword** mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Tokenize each string and merge the token array into one big array using `itertools.chain()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_words = list(itertools.chain(*df.Words_normalized_no_user_fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute frequency of every token using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° The most common tokens are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° We have 34289 unique words (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° The longest tweet has 42 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(df.Words_normalized_no_user_fixed.apply(len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Our dataset is ready, we can start our Deep learning experiments \n",
    "* üîé Can you use regular FCANN for the sentiment analysis?\n",
    "\n",
    "![Meme01](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_06_meme_01.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will use `TextVectorization` layer for creating vector model from our text data\n",
    "* For those of you who are interested in the topic there is very good [article on Medium](https://towardsdatascience.com/you-should-try-the-new-tensorflows-textvectorization-layer-a80b3c6b00ee) about the layer and its parameters\n",
    "    * There is of course a [documentation page](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) about the layer as well\n",
    "\n",
    "## üîé What does *text vectorization* mean in this context?\n",
    "* Is it a different term from the one used in information retrieval?\n",
    "\n",
    "## üìå There are few important parameters:\n",
    "* `emedding_dim` \n",
    "    * Dimension of embedded representation\n",
    "    * This is already part of latent space, there is captured dependecy among words in these vectors, we are learning this vectors using the ANN\n",
    "* `vocab_size`\n",
    "    * Number of unique tokens in vocabulary\n",
    "* `sequence_length`\n",
    "    * Output dimension after vectorizing - words in vectorized representation are treated as independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128 # Dimension of embedded representation\n",
    "vocab_size = 10000 # Number of unique tokens in vocabulary\n",
    "sequence_length = 30 # Output dimension after vectorizing\n",
    "\n",
    "vect_layer = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\n",
    "vect_layer.adapt(df.Clean_text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will split our dataset to train and test parts with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ COMPETITION TEST SET HERE üéØ\n",
    "\n",
    "### COMPETITION ?!?! ü§î\n",
    "* I will provide the details in the end of the lecture üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Clean_text, df.label, test_size=0.20, random_state=13, stratify=df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=13, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change X_train to numpy strings (new version of pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(str).to_numpy()\n",
    "X_test = X_test.astype(str).to_numpy()\n",
    "X_valid = X_valid.astype(str).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train')\n",
    "print(y_train.value_counts())\n",
    "print('Test')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Vocabulary example: ', vect_layer.get_vocabulary()[:10])\n",
    "print('Vocabulary shape: ', len(vect_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Let's finally try the RNN-based model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(vocab_size, embedding_dim)(x_v)\n",
    "x = LSTM(64, activation='relu', return_sequences=True)(emb)\n",
    "x = GRU(64, activation='relu', return_sequences=False)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "# keras.metrics.F1Score(average='weighted',threshold=0.5, name='f1') - currently bugged\n",
    "model.compile(optimizer=keras.optimizers.AdamW(), loss=keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.BinaryAccuracy(name='accuracy')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are using `AdamW` optimizer, which is a variant of Adam optimizer ‚öôÔ∏è\n",
    "### Adam Optimizer üß†\n",
    "\n",
    "Adam (Adaptive Moment Estimation) combines two key techniques to improve neural network training:\n",
    "\n",
    "- **Momentum** üîÑ: Keeps track of previous directions, helping navigate tricky optimization landscapes like a ball rolling downhill\n",
    "- **Adaptive learning rates** üìä: Adjusts the step size individually for each parameter based on its history\n",
    "\n",
    "Adam is memory-efficient and works well for smaller networks, simple regression problems, and when working with clean datasets\n",
    "\n",
    "**Adaptive Learning Rates in Adam: A Simple Explanation** üß≠\n",
    "\n",
    "* Imagine you're hiking through mountains with varying terrain - steep cliffs, gentle slopes, and flat meadows üèîÔ∏è\n",
    "* The \"adaptive learning rates\" in Adam work like an intelligent hiking assistant that helps you navigate this landscape efficiently\n",
    "\n",
    "**How It Works** ‚öôÔ∏è\n",
    "\n",
    "1. **Different Parameters Need Different Step Sizes** üìè\n",
    "   - Some parameters might need big updates (steep terrain) üìà\n",
    "   - Others might need tiny, careful updates (slippery slopes) üìâ\n",
    "\n",
    "2. **Adam Keeps Track of History** üìú\n",
    "   - For each parameter, Adam remembers how much that parameter has changed recently\n",
    "   - If a parameter has been changing a lot, Adam gives it smaller updates ‚¨áÔ∏è\n",
    "   - If a parameter has barely moved, Adam gives it larger updates ‚¨ÜÔ∏è\n",
    "\n",
    "3. **Real-World Example** üåç\n",
    "   - Parameter A has seen huge gradients (values like 10.0, 8.5, 9.2)\n",
    "   - Parameter B has seen tiny gradients (values like 0.01, 0.02, 0.01)\n",
    "   - With a fixed learning rate, A would move too quickly and B too slowly ‚ö†Ô∏è\n",
    "   - Adam automatically gives A smaller steps and B larger steps ‚úÖ\n",
    "\n",
    "**Why This Matters** üí°\n",
    "\n",
    "- **Prevents Oscillation** üîÑ: Parameters that tend to bounce back and forth get smaller updates\n",
    "- **Escapes Flat Regions** üèùÔ∏è: Parameters stuck in flat areas get larger updates\n",
    "- **Training Stability** üõ°Ô∏è: Each parameter moves at its appropriate pace\n",
    "- **Faster Convergence** üèÅ: The model learns more efficiently overall\n",
    "\n",
    "This adaptive behavior is why Adam often converges faster than optimizers with fixed learning rates and requires less manual tuning.\n",
    "\n",
    "### AdamW Optimizer üöÄ\n",
    "\n",
    "AdamW is an improved version of Adam with one crucial difference:\n",
    "\n",
    "- **Decoupled weight decay** ‚öñÔ∏è: AdamW separates the weight decay (regularization) from the gradient updates, while Adam applies it as part of the gradient update process\n",
    "\n",
    "This seemingly small change makes AdamW more effective for:\n",
    "- Training large models üèóÔ∏è\n",
    "- Improving generalization üåê\n",
    "- Handling complex datasets üìö\n",
    "\n",
    "AdamW has become the default optimizer for transformer pretraining and many deep learning applications due to its consistent performance across various tasks\n",
    "\n",
    "**Understanding Decoupled Weight Decay in AdamW** üß©\n",
    "\n",
    "* In traditional Adam optimization, L2 regularization (weight decay) is applied through the loss function, which means it affects the gradients before they're processed by Adam's adaptive learning rate mechanism\n",
    "* This creates a problem because the adaptive nature of Adam distorts the regularization effect, especially for parameters with large historical gradients ‚ö†Ô∏è\n",
    "\n",
    "* AdamW solves this problem by **decoupling** weight decay from the gradient computation üîì\n",
    "* Instead of including weight decay in the loss function (which affects gradients), AdamW applies weight decay directly to the weights as a separate step in the update process\n",
    "\n",
    "**How It Works** ‚öôÔ∏è\n",
    "\n",
    "1. **Adam (with L2 regularization)** üìä: \n",
    "   - Adds the L2 penalty to the loss function: `L_new(w) = L_original(w) + Œªw^Tw`\n",
    "   - Gradients are computed from this combined loss\n",
    "   - Adam's adaptive learning rates are applied to these modified gradients\n",
    "   - Problem: Parameters with large historical gradients receive less regularization ‚ùå\n",
    "\n",
    "2. **AdamW (with decoupled weight decay)** üîÑ:\n",
    "   - Computes gradients from only the original loss function\n",
    "   - Applies Adam's adaptive learning rate mechanism to these gradients\n",
    "   - Separately applies weight decay directly to the weights: `w = w - Œªw`\n",
    "   - All weights are regularized equally regardless of their gradient history ‚úÖ\n",
    "\n",
    "**Benefits of Decoupling** üéØ\n",
    "\n",
    "1. **Better Generalization** üåê: AdamW consistently achieves better generalization performance than Adam with L2 regularization\n",
    "\n",
    "2. **Simplified Hyperparameter Tuning** üéõÔ∏è: The weight decay parameter becomes more independent of the learning rate, creating a more separable hyperparameter space that's easier to optimize\n",
    "\n",
    "3. **Improved Convergence** üìà: AdamW can match or exceed the performance of SGD with momentum on tasks where Adam traditionally struggled\n",
    "\n",
    "**Key Differences Summary** üìã\n",
    "\n",
    "| Optimizer | Memory Usage | Key Features | Best For |\n",
    "|-----------|--------------|--------------|----------|\n",
    "| Adam | Medium üìä | Momentum + adaptive learning rates | Smaller networks, prototyping üî¨ |\n",
    "| AdamW | Medium üìä | Adam + decoupled weight decay | Large models, general purpose üöÄ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(X_train, tf.cast(y_train.values, tf.int32), validation_data=(X_valid, tf.cast(y_valid.values, tf.int32)), callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best setup\n",
    "model.load_weights(\"best.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function gives us real number in range <0, 1>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to map this values to discreet classes 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = [1 if x >= 0.5 else 0 for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The accuracy is not the best metric in the imbalanced situation - you already know the reason üôÇ\n",
    "* There are many more metrics we can use and one of the most common in this situation is the F1 Score, see [this](https://en.wikipedia.org/wiki/F-score) and [this](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/) for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_true=y_test, y_pred=y_pred), annot=True, cmap='Greens', fmt='.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do we need to train our own embedding from scratch? ü§î\n",
    "* üí° There are multiple embeddings available online which were trained on very large corpuses e.g. Wikipedia\n",
    "* Good examples are Word2Vec, Glove or FastText\n",
    "    * These embeddings contains fixed length vectors for words in the vocabulary\n",
    "\n",
    "* We will use GloVe embedding with 50 dimensional embedding vectors\n",
    "    * For more details see [this](https://nlp.stanford.edu/projects/glove/)\n",
    "* You can download zip with vectors from [http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip) ~ 800 MB\n",
    "\n",
    "### üìå Beware that the original text corpus was more general than the specific social media text data\n",
    "* üí° So if you deal with very specific domains it may be beneficial to train your own embedding or at least fine tune existing one in the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to download the embedding files\n",
    "~~~\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "~~~\n",
    "\n",
    "* üí°50 dims GLOVE is also avaiable at https://ai.vsb.cz/downloads/glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we need to load the file to memory and create embedding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = 'glove.6B.50d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, \"r\",  encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üí° This is how the embedding latent vector looks like for the word `analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index['analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index['analysis'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Our goal is to use the pre-trained embedding in our model\n",
    "* We need to get the vocabulary from the `TextVectorization` layer and the integer indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50 # Embedding dimension -> GloVe 50\n",
    "vocab_size = 10000 # Number of unique tokens in vocabulary\n",
    "sequence_length = 20 # Output dimension after vectorizing - words in vectorited representation are independent\n",
    "\n",
    "vect_layer = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\n",
    "vect_layer.adapt(df.Clean_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc = vect_layer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can create the embedding matrix\n",
    "* We just need to map the `int` indices to the embedding vectors and save the mapping to the matrix\n",
    "    * üí° Each row of the matrix is a one token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_matrix[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finall, we can use the GloVe embedding in the `Embedding` layer in our model\n",
    "* üí° Beware the `embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False)` part\n",
    "    * You say the model to use the GloVe vectors and that it can't modify them\n",
    "    * üí° You can also set the parameter `trainable=True` and do the fine-tuning of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(num_tokens, embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False)(x_v)\n",
    "x = LSTM(64, activation='relu', return_sequences=True)(emb)\n",
    "x = GRU(64, activation='relu', return_sequences=False)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(64, 'relu')(x)\n",
    "x = keras.layers.Dense(32, 'relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.AdamW(), loss=keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.BinaryAccuracy(name='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(X_train, tf.cast(y_train.values, tf.float32), validation_data=(X_valid, tf.cast(y_valid.values, tf.float32)), callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best setup\n",
    "model.load_weights(\"best.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé Which model is better?\n",
    "* The one using pre-trained embedding or the one that we've trained from scrath?\n",
    "* üîé Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).ravel()\n",
    "y_pred = [1 if x >= 0.5 else 0 for x in y_pred]\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred)}')\n",
    "print(f'F1 Score: {f1_score(y_true=y_test, y_pred=y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ  Tasks for the lecture (2p)\n",
    "* Try to fine-tune and switch the GloVe embedding and compare the models\n",
    "    * Fine-tune GloVe 50 embedding - **(1p)**\n",
    "    * Use GloVe 100 or higher dimentional embbeding - **(1p)**\n",
    "    * üîé Is it any different according to the **F1-Score**?\n",
    "\n",
    "# üöÄ There is a competition for bonus points this week! \n",
    "* Everyone who will send me a correct solution will be included in the F1 - Score toplist\n",
    "    * üìå **Send me a link to the notebook, not the .ipynb file (üí° e-mail filter issue)!**\n",
    "* **Deadline for the competition submission is Sunday 30th at 15:00**\n",
    "    * The toplist will be publicly available\n",
    "* There is no limitation in used layers (LSTM, CNN, ...), optimizers and so on\n",
    "    * üí° You can use any model architecture from the internet including transfer learning\n",
    "* The test set is the same as the one that we used in the lecture\n",
    "    * üí° It is marked with üéØ in the notebook\n",
    "* ‚ö° The winner with the best **F1-Score** on test set will be awarded with **10 bonus points**\n",
    "\n",
    "## üìå The only limitation is that the model has to be trained/fine-tuned on Colab/Kaggle/Your machine so online sentiment scoring services or REST-API LLMs are forbidden!\n",
    "\n",
    "![Meme03](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_06_meme_03.png?raw=true)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
