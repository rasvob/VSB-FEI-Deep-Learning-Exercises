{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilthBvnZCQto"
   },
   "source": [
    "# Deep Learning - Exercise 10\n",
    "\n",
    "This lecture is focused on using CNN for object localization tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_10.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_10.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # images\n",
    "import numpy as np #numpy\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v2 as tf #use tensorflow v2 as a main \n",
    "import tensorflow.keras as keras # required for high level applications\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“’ What is the Object Localization?\n",
    "* Object localization is the name of the task of **classification with localization**\n",
    "* Namely, given an image, classify the object that appears in it, and find its location in the image, usually by using a **bounding-box**\n",
    "* In Object Localization, only a single object can appear in the image. \n",
    "    * ðŸ’¡ If more than one object can appear, the task is called **Object Detection**\n",
    "\n",
    "![model](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_10_01.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Object Localization can be treated as a regression problem \n",
    "\n",
    "### We can represent our output (a bounding-box) as a tuple of size 4, as follows:\n",
    "* `(x, y, height, width)`\n",
    "    * `x, y`: the coordination of the left-top corner of the bounding box\n",
    "    * `height`: the height of the bounding box\n",
    "    * `width`: the width of the bounding box\n",
    "    \n",
    "![model2](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_10_02.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Network architecture in general\n",
    "* The coordinates of the left-top corner of the bounding box must be inside the image and so do x+width and y+height\n",
    "    * We will scale the image width and height to be 1.0\n",
    "    * So we make sure that the CNN outputs will be in the range `[0,1]` - we will use the sigmoid activation layer\n",
    "        * ðŸ’¡ It will enforce that `(x,y)` will be inside the image, but not necessarily x+width and y+height\n",
    "        * ðŸ’¡ This property will be learned by the network during the training process.\n",
    "\n",
    "![model3](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_10_03.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What about the loss?\n",
    "* The output of a sigmoid can be treated as probabilistic values, and therefore we can use **Binary Crossentropy** loss\n",
    "    * ðŸ“Œ You can see [this](https://www.theaidream.com/post/loss-functions-in-neural-networks) or [this](https://github.com/christianversloot/machine-learning-articles/blob/main/about-loss-and-loss-functions.md) for more informations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ We will start with purely synthetic use-case for educational purposes before we start to implement more complex one ðŸ™‚\n",
    "* ðŸ“Œ Our task will be the detection of white circles on pure black background\n",
    "    * We will assume that the white blobs will be located in square bounding boxes for simplicity\n",
    "        * ðŸ”Ž How will the output layer look like for task like this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 512\n",
    "X = np.zeros((dataset_size, 128, 128, 1))\n",
    "labels = np.zeros((dataset_size, 3))\n",
    "# fill each image\n",
    "for i in range(dataset_size):\n",
    "    x = np.random.randint(8,120)\n",
    "    y = np.random.randint(8,120)\n",
    "    a = min(128 - max(x,y), min(x,y))\n",
    "    r = np.random.randint(4,a)\n",
    "    for x_i in range(128):\n",
    "      for y_i in range(128):\n",
    "        if ((x_i - x)**2) + ((y_i - y)**2) < r**2:\n",
    "          X[i, x_i, y_i,:] = 1\n",
    "    labels[i,0] = (x-r)/128.0\n",
    "    labels[i,1] = (y-r)/128.0\n",
    "    labels[i,2] = 2*r / 128.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can check an example of the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0].reshape(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "def plot_pred(img,p):\n",
    "  fig, ax = plt.subplots(1)\n",
    "  ax.imshow(img.reshape(128, 128))\n",
    "  rect = Rectangle(xy=(p[1]*128,p[0]*128),width=p[2]*128, height=p[2]*128, linewidth=2,edgecolor='g',facecolor='none')\n",
    "  ax.add_patch(rect)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And also with the ground truth bounding-box plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred(X[0], labels[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More examples of our input data with bounding boxes incoming ðŸ™‚\n",
    "* ðŸ’¡ We can see that the circles varies in position and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8, 8, figsize=(20, 14))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        img = X[i*8 + j]\n",
    "        p = labels[i*8 + j]\n",
    "        ax[i, j].imshow(img.reshape(128, 128))\n",
    "        rect = Rectangle(xy=(p[1]*128,p[0]*128),width=p[2]*128, height=p[2]*128, linewidth=2,edgecolor='g',facecolor='none')\n",
    "        ax[i, j].add_patch(rect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Let's define our first object localization model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "colab_type": "code",
    "id": "GqGF8pxYCQt2",
    "outputId": "854e6ae7-1750-4e71-bb94-5f32ec9446d4"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(128,128,1)),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(16, (3,3), padding='same', activation='relu'),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(3, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uz_cLvzw5Iub"
   },
   "source": [
    "## Fit the model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcNubiSECQt5"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(train_x, train_y, validation_split=0.2, callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_x, train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Now we can take a look at our predictions using the model\n",
    "* We will see that sometimes the prediction is slightly off but usually not by much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "fig, ax = plt.subplots(nrows, nrows, figsize=(20, 14))\n",
    "for i in range(nrows):\n",
    "    for j in range(nrows):\n",
    "        img = test_x[i*nrows + j]\n",
    "        p = test_y[i*nrows + j]\n",
    "        predicted = y_pred[i*nrows + j]\n",
    "        ax[i, j].imshow(img.reshape(128, 128))\n",
    "        rect = Rectangle(xy=(p[1]*128,p[0]*128),width=p[2]*128, height=p[2]*128, linewidth=2,edgecolor='g',facecolor='none')\n",
    "        ax[i, j].add_patch(rect)\n",
    "        rect = Rectangle(xy=(predicted[1]*128,predicted[0]*128),width=predicted[2]*128, height=predicted[2]*128, linewidth=2,edgecolor='r',facecolor='none')\n",
    "        ax[i, j].add_patch(rect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Now we know the basics so we can focus on more interesting stuff\n",
    "* Usually you don't want to train your own model for the task, but you try to leverage transfer learning approach\n",
    "* ðŸ’¡ Object localization is no exception\n",
    "* Object localization/detection is very common task and there is already wide variety of the models focused on this task\n",
    "\n",
    "## ðŸ“Œ Current the State-of-the-Art model is [YOLOv8 by Ultralytics](https://github.com/ultralytics/ultralytics)\n",
    "* It is useful for wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks\n",
    "* YOLOv8 may be used directly in the Command Line Interface (CLI) or in a Python environment using the Python API\n",
    "* There are 5 pre-trained models available\n",
    "    * Number of parameters thus the size of the models is different\n",
    "    * Models can be downloaded from [YOLOv8 Github repository](https://github.com/ultralytics/ultralytics)\n",
    "\n",
    "### ðŸ’¡ Tensorflow 2 has high-level API available for these tasks too \n",
    "* However it is a bit more comlicated compared to YOLOv8\n",
    "* You can also use already pre-trained models which can be used directly for the inference or fine-tuned\n",
    "* You can read the [blog post](https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html) about the API or you can take a look at the [Github](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ We will start with a simple zero-shot object detection\n",
    "* ðŸ”Ž What is meant by *zero-shot* approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/misc/bus.jpg?raw=true -O bus.jpg\n",
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/raw/main/misc/yolov8n.pt -O yolov8n.pt\n",
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/releases/download/v0.0.01/video_cut.mkv -O video_cut.mkv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ The model is ready ready to use\n",
    "* The model has several parameters:\n",
    "    * `save`: Enables saving of the annotated images or videos to file. \n",
    "    * `save_txt`: Save the bounding boxes and class labels to text file\n",
    "        * ðŸ’¡ Format is [class] [x_center] [y_center] [width] [height] [confidence]\n",
    "    * `save_conf`: \tIncludes confidence scores in the saved text files (you can filter out low confidence detections later)\n",
    "    * `imgsz`: Defines the image size for inference\n",
    "        * ðŸ’¡ Can be a single integer for square resizing or a (height, width) tuple\n",
    "    * `project`: Folder name for saving output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model('bus.jpg', save_txt=True, save_conf=True, save=True, imgsz=1088, project=\"yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to work with the detected bounding boxes, you can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(res):\n",
    "    curr = result.boxes.xyxy.cpu().numpy()\n",
    "    if curr.shape[0] > 0:\n",
    "        print(f\"Image {i}\")\n",
    "        print(f\"Found {curr.shape[0]} boxes\")\n",
    "        print(f'Classes:  {result.boxes.cls.cpu().numpy()}')\n",
    "        for j, box in enumerate(curr):\n",
    "            print(f'Box {j}: {box}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š We can display boxes in the image easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"bus.jpg\")\n",
    "draw = ImageDraw.Draw(im)\n",
    "cls_to_color = {0 : 'red', 5: 'yellow'}\n",
    "for result in res:\n",
    "    for i, box in enumerate(result.boxes.xyxy.cpu().numpy()):\n",
    "        cls = result.boxes.cls.cpu().numpy()[i]\n",
    "        x, y, xx, yy = box\n",
    "        draw.rectangle([x, y, xx, yy], outline=cls_to_color[cls], width=4)\n",
    "\n",
    "# Display image in matplotlib\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo can detect 80 classes out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ You can limit the classes that are detected with the `classes` parameter\n",
    "* Let's say that we want to detect only the *bus* object\n",
    "    * We need to set the `classes` to `5` as this is the ID of *bus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model('bus.jpg', save_txt=True, save_conf=True, save=True, imgsz=1088, project=\"yolo\", classes=[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now only the *bus* was detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"bus.jpg\")\n",
    "draw = ImageDraw.Draw(im)\n",
    "cls_to_color = {0 : 'red', 5: 'yellow'}\n",
    "for result in res:\n",
    "    for i, box in enumerate(result.boxes.xyxy.cpu().numpy()):\n",
    "        cls = result.boxes.cls.cpu().numpy()[i]\n",
    "        x, y, xx, yy = box\n",
    "        draw.rectangle([x, y, xx, yy], outline=cls_to_color[cls], width=4)\n",
    "\n",
    "# Display image in matplotlib\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO is able to process video files using the same API, we can try it using the downloaded video file\n",
    "* We want to detect the boats that are in the video sequence\n",
    "    * ðŸ’¡ ID of *boat* object is `8`\n",
    "* ðŸ“Œ Set `stream=True` so inference results won't accumulate in RAM causing potential out-of-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model('video_cut.mkv', stream=True, save_txt=True, save_conf=True, save=True, imgsz=1088, project=\"yolo\", classes=[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ With `stream=True` the detection is done when we iterate over the `res` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i, result in enumerate(res):\n",
    "    curr = result.boxes.xyxy.cpu().numpy()\n",
    "    output.append({'Cls': result.boxes.cls.cpu().numpy(), 'BBoxes': curr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š The bounding boxes are stored in the output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[8:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ We can fine-tune the model using our data\n",
    "* It requires a dataset in COCO format\n",
    "* And also the configuration file, which is a modified version of the original YOLOv5 configuration file\n",
    "\n",
    "## Let's download the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/releases/download/v0.0.01/yolo_data.zip -O yolo_data_dir.zip\n",
    "!unzip yolo_data_dir.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't forget to download the config file\n",
    "* ðŸ’¡ We need to modify the `path` property in the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/raw/main/misc/coco128.yaml -O coco128.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "model.train(data='coco128.yaml', epochs=5, imgsz=1920, batch=8, pretrained=True, cache=True, workers=16, seed=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training the model, we can use it for inference, usually we want to export the model\n",
    "* You can check the export documentation [here](https://docs.ultralytics.com/modes/export/#export-formats)\n",
    "* Often the ONNX (for CPU) or TensorRT (for GPU) export format is used\n",
    "* `half` parameter enables FP16 (half-precision) quantization, reducing model size and potentially speeding up inference on supported hardware\n",
    "    * ðŸ’¡ For GPU only, you need to set `device` parameter\n",
    "* `simplify` parameter simplifies the model graph for ONNX exports, potentially improving performance and compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(format='onnx', imgsz=1920, half=True, simplify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Finally we can load the model as usual and use it in the inference mode\n",
    "* ðŸ’¡ Set the model path according to your workspace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.onnx\", verbose=True, task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model('video_cut.mkv', stream=True, save_txt=True, save_conf=True, save=True, imgsz=1920, project=\"yolo\", classes=[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i, result in enumerate(res):\n",
    "    curr = result.boxes.xyxy.cpu().numpy()\n",
    "    output.append({'Cls': result.boxes.cls.cpu().numpy(), 'BBoxes': curr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[8:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…  Tasks for the lecture (**4p**)\n",
    "\n",
    "* There are multiple YOLOv8 models available on the [Github](https://github.com/ultralytics/ultralytics)\n",
    "* ðŸ“Œ Choose another 2 versions of the model beside the `YOLOv8n` one that we used during the lecture\n",
    "* Try to fine-tune the models and export them to either ONNX or TensorRT\n",
    "    * ðŸ’¡ The fine-tuning step is optional - if you don't have enough resources, just skip this step and try the inference directly\n",
    "    * Compare the inference times of all three models using the provided video file of boat\n",
    "        * ðŸ”Ž How much they differ?\n",
    "    * Also check the output video files\n",
    "        * ðŸ”Ž Are there any differences in the detected bounding boxes?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_04.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
