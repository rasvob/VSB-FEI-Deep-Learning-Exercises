{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilthBvnZCQto"
   },
   "source": [
    "# Deep Learning - Exercise 10\n",
    "\n",
    "This exercise focuses on implementing Convolutional Neural Networks (CNNs) for object localization tasks and exploring the powerful YOLOv8 architecture. We'll learn how to detect and precisely locate objects in images and videos, then apply these concepts using a state-of-the-art model in real-world scenarios.\n",
    "\n",
    "**Core Concepts**\n",
    "* üñºÔ∏è Object localization fundamentals and bounding box regression\n",
    "* üß† CNN architectures for effective feature extraction and object detection\n",
    "* üì¶ YOLOv8 model architecture and capabilities\n",
    "* üîç Practical implementation of object localization in real-world applications\n",
    "* üõ†Ô∏è Training and fine-tuning YOLOv8 on custom datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_10.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_10.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # images\n",
    "import numpy as np #numpy\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v2 as tf #use tensorflow v2 as a main \n",
    "import tensorflow.keras as keras # required for high level applications\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìí What is the Object Localization?\n",
    "* Object localization is the name of the task of **classification with localization**\n",
    "* Namely, given an image, classify the object that appears in it, and find its location in the image, usually by using a **bounding-box**\n",
    "* In Object Localization, only a single object can appear in the image. \n",
    "    * üí° If more than one object can appear, the task is called **Object Detection**\n",
    "\n",
    "![model](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_10_01.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Object Localization can be treated as a regression problem \n",
    "\n",
    "### We can represent our output (a bounding-box) as a tuple of size 4, as follows:\n",
    "* `(x, y, height, width)`\n",
    "    * `x, y`: the coordination of the left-top corner of the bounding box\n",
    "    * `height`: the height of the bounding box\n",
    "    * `width`: the width of the bounding box\n",
    "    \n",
    "![model2](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_10_02.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Network architecture in general\n",
    "* The coordinates of the left-top corner of the bounding box must be inside the image and so do x+width and y+height\n",
    "    * We will scale the image width and height to be 1.0\n",
    "    * So we make sure that the CNN outputs will be in the range `[0,1]` - we will use the sigmoid activation layer\n",
    "        * üí° It will enforce that `(x,y)` will be inside the image, but not necessarily x+width and y+height\n",
    "        * üí° This property will be learned by the network during the training process.\n",
    "\n",
    "![model3](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_10_03.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé What about the loss?\n",
    "* The output of a sigmoid can be treated as probabilistic values, and therefore we can use **Binary Crossentropy** loss\n",
    "    * üìå You can see [this](https://www.theaidream.com/post/loss-functions-in-neural-networks) or [this](https://github.com/christianversloot/machine-learning-articles/blob/main/about-loss-and-loss-functions.md) for more informations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° We will start with purely synthetic use-case for educational purposes before we start to implement more complex one üôÇ\n",
    "* üìå Our task will be the detection of white circles on pure black background\n",
    "    * We will assume that the white blobs will be located in square bounding boxes for simplicity\n",
    "        * üîé How will the output layer look like for task like this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 512\n",
    "X = np.zeros((dataset_size, 128, 128, 1))\n",
    "labels = np.zeros((dataset_size, 3))\n",
    "# fill each image\n",
    "for i in range(dataset_size):\n",
    "    x = np.random.randint(8,120)\n",
    "    y = np.random.randint(8,120)\n",
    "    a = min(128 - max(x,y), min(x,y))\n",
    "    r = np.random.randint(4,a)\n",
    "    for x_i in range(128):\n",
    "      for y_i in range(128):\n",
    "        if ((x_i - x)**2) + ((y_i - y)**2) < r**2:\n",
    "          X[i, x_i, y_i,:] = 1\n",
    "    labels[i,0] = (x-r)/128.0\n",
    "    labels[i,1] = (y-r)/128.0\n",
    "    labels[i,2] = 2*r / 128.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can check an example of the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0].reshape(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "def plot_pred(img,p):\n",
    "  fig, ax = plt.subplots(1)\n",
    "  ax.imshow(img.reshape(128, 128))\n",
    "  rect = Rectangle(xy=(p[1]*128,p[0]*128),width=p[2]*128, height=p[2]*128, linewidth=2,edgecolor='g',facecolor='none')\n",
    "  ax.add_patch(rect)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And also with the ground truth bounding-box plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred(X[0], labels[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More examples of our input data with bounding boxes incoming üôÇ\n",
    "* üí° We can see that the circles varies in position and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8, 8, figsize=(20, 14))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        img = X[i*8 + j]\n",
    "        p = labels[i*8 + j]\n",
    "        ax[i, j].imshow(img.reshape(128, 128))\n",
    "        rect = Rectangle(xy=(p[1]*128,p[0]*128),width=p[2]*128, height=p[2]*128, linewidth=2,edgecolor='g',facecolor='none')\n",
    "        ax[i, j].add_patch(rect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Let's define our first object localization model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "colab_type": "code",
    "id": "GqGF8pxYCQt2",
    "outputId": "854e6ae7-1750-4e71-bb94-5f32ec9446d4"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(128,128,1)),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(16, (3,3), padding='same', activation='relu'),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(3, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uz_cLvzw5Iub"
   },
   "source": [
    "## Fit the model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcNubiSECQt5"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(train_x, train_y, validation_split=0.2, callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_x, train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Now we can take a look at our predictions using the model\n",
    "* We will see that sometimes the prediction is slightly off but usually not by much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "fig, ax = plt.subplots(nrows, nrows, figsize=(20, 14))\n",
    "for i in range(nrows):\n",
    "    for j in range(nrows):\n",
    "        img = test_x[i*nrows + j]\n",
    "        p = test_y[i*nrows + j]\n",
    "        predicted = y_pred[i*nrows + j]\n",
    "        ax[i, j].imshow(img.reshape(128, 128))\n",
    "        rect = Rectangle(xy=(p[1]*128,p[0]*128),width=p[2]*128, height=p[2]*128, linewidth=2,edgecolor='g',facecolor='none')\n",
    "        ax[i, j].add_patch(rect)\n",
    "        rect = Rectangle(xy=(predicted[1]*128,predicted[0]*128),width=predicted[2]*128, height=predicted[2]*128, linewidth=2,edgecolor='r',facecolor='none')\n",
    "        ax[i, j].add_patch(rect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Now we know the basics so we can focus on more interesting stuff\n",
    "* Usually you don't want to train your own model for the task, but you try to leverage transfer learning approach\n",
    "* üí° Object localization is no exception\n",
    "* Object localization/detection is very common task and there is already wide variety of the models focused on this task\n",
    "\n",
    "## üìå Current the State-of-the-Art model is [YOLOv8 by Ultralytics](https://github.com/ultralytics/ultralytics)\n",
    "* It is useful for wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks\n",
    "* YOLOv8 may be used directly in the Command Line Interface (CLI) or in a Python environment using the Python API\n",
    "* There are 5 pre-trained models available\n",
    "    * Number of parameters thus the size of the models is different\n",
    "    * Models can be downloaded from [YOLOv8 Github repository](https://github.com/ultralytics/ultralytics)\n",
    "\n",
    "### üí° Tensorflow 2 has high-level API available for these tasks too \n",
    "* However it is a bit more comlicated compared to YOLOv8\n",
    "* You can also use already pre-trained models which can be used directly for the inference or fine-tuned\n",
    "* You can read the [blog post](https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html) about the API or you can take a look at the [Github](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° We will start with a simple zero-shot object detection\n",
    "* üîé What is meant by *zero-shot* approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/misc/bus.jpg?raw=true -O bus.jpg\n",
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/raw/main/misc/yolov8n.pt -O yolov8n.pt\n",
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/releases/download/v0.0.01/video_cut.mkv -O video_cut.mkv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ The model is ready ready to use\n",
    "* The model has several parameters:\n",
    "    * `save`: Enables saving of the annotated images or videos to file. \n",
    "    * `save_txt`: Save the bounding boxes and class labels to text file\n",
    "        * üí° Format is [class] [x_center] [y_center] [width] [height] [confidence]\n",
    "    * `save_conf`: \tIncludes confidence scores in the saved text files (you can filter out low confidence detections later)\n",
    "    * `imgsz`: Defines the image size for inference\n",
    "        * üí° Can be a single integer for square resizing or a (height, width) tuple\n",
    "    * `project`: Folder name for saving output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model('bus.jpg', save_txt=True, save_conf=True, save=True, imgsz=1088, project=\"yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to work with the detected bounding boxes, you can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(res):\n",
    "    curr = result.boxes.xyxy.cpu().numpy()\n",
    "    if curr.shape[0] > 0:\n",
    "        print(f\"Image {i}\")\n",
    "        print(f\"Found {curr.shape[0]} boxes\")\n",
    "        print(f'Classes:  {result.boxes.cls.cpu().numpy()}')\n",
    "        for j, box in enumerate(curr):\n",
    "            print(f'Box {j}: {box}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä We can display boxes in the image easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"bus.jpg\")\n",
    "draw = ImageDraw.Draw(im)\n",
    "cls_to_color = {0 : 'red', 5: 'yellow'}\n",
    "for result in res:\n",
    "    for i, box in enumerate(result.boxes.xyxy.cpu().numpy()):\n",
    "        cls = result.boxes.cls.cpu().numpy()[i]\n",
    "        x, y, xx, yy = box\n",
    "        draw.rectangle([x, y, xx, yy], outline=cls_to_color[cls], width=4)\n",
    "\n",
    "# Display image in matplotlib\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo can detect 80 classes out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° You can limit the classes that are detected with the `classes` parameter\n",
    "* Let's say that we want to detect only the *bus* object\n",
    "    * We need to set the `classes` to `5` as this is the ID of *bus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model('bus.jpg', save_txt=True, save_conf=True, save=True, imgsz=1088, project=\"yolo\", classes=[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now only the *bus* was detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"bus.jpg\")\n",
    "draw = ImageDraw.Draw(im)\n",
    "cls_to_color = {0 : 'red', 5: 'yellow'}\n",
    "for result in res:\n",
    "    for i, box in enumerate(result.boxes.xyxy.cpu().numpy()):\n",
    "        cls = result.boxes.cls.cpu().numpy()[i]\n",
    "        x, y, xx, yy = box\n",
    "        draw.rectangle([x, y, xx, yy], outline=cls_to_color[cls], width=4)\n",
    "\n",
    "# Display image in matplotlib\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO is able to process video files using the same API, we can try it using the downloaded video file\n",
    "* We want to detect the boats that are in the video sequence\n",
    "    * üí° ID of *boat* object is `8`\n",
    "* üìå Set `stream=True` so inference results won't accumulate in RAM causing potential out-of-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model('video_cut.mkv', stream=True, save_txt=True, save_conf=True, save=True, imgsz=1088, project=\"yolo\", classes=[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° With `stream=True` the detection is done when we iterate over the `res` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i, result in enumerate(res):\n",
    "    curr = result.boxes.xyxy.cpu().numpy()\n",
    "    output.append({'Cls': result.boxes.cls.cpu().numpy(), 'BBoxes': curr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä The bounding boxes are stored in the output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[8:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ We can fine-tune the model using our data\n",
    "* It requires a dataset in COCO format\n",
    "* And also the configuration file, which is a modified version of the original YOLOv5 configuration file\n",
    "\n",
    "## Let's download the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/releases/download/v0.0.01/yolo_data.zip -O yolo_data_dir.zip\n",
    "!unzip yolo_data_dir.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't forget to download the config file\n",
    "* üí° We need to modify the `path` property in the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/raw/main/misc/coco128.yaml -O coco128.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "model.train(data='coco128.yaml', epochs=5, imgsz=1920, batch=8, pretrained=True, cache=True, workers=16, seed=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training the model, we can use it for inference, usually we want to export the model\n",
    "* You can check the export documentation [here](https://docs.ultralytics.com/modes/export/#export-formats)\n",
    "* Often the ONNX (for CPU) or TensorRT (for GPU) export format is used\n",
    "* `half` parameter enables FP16 (half-precision) quantization, reducing model size and potentially speeding up inference on supported hardware\n",
    "    * üí° For GPU only, you need to set `device` parameter\n",
    "* `simplify` parameter simplifies the model graph for ONNX exports, potentially improving performance and compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(format='onnx', imgsz=1920, half=True, simplify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Finally we can load the model as usual and use it in the inference mode\n",
    "* üí° Set the model path according to your workspace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.onnx\", verbose=True, task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model('video_cut.mkv', stream=True, save_txt=True, save_conf=True, save=True, imgsz=1920, project=\"yolo\", classes=[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i, result in enumerate(res):\n",
    "    curr = result.boxes.xyxy.cpu().numpy()\n",
    "    output.append({'Cls': result.boxes.cls.cpu().numpy(), 'BBoxes': curr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[8:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ  Tasks for the lecture (**4p**)\n",
    "\n",
    "* There are multiple YOLOv8 models available on the [Github](https://github.com/ultralytics/ultralytics)\n",
    "* üìå Choose another 2 versions of the model beside the `YOLOv8n` one that we used during the lecture\n",
    "* Try to fine-tune the models and export them to either ONNX or TensorRT\n",
    "    * üí° The fine-tuning step is optional - if you don't have enough resources, just skip this step and try the inference directly\n",
    "    * Compare the inference times of all three models using the provided video file of boat\n",
    "        * üîé How much they differ?\n",
    "    * Also check the output video files\n",
    "        * üîé Are there any differences in the detected bounding boxes?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_04.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
