{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm"
   },
   "source": [
    "# Deep Learning - Exercise 3\n",
    "\n",
    "This lecture is about introduction to CNNs.\n",
    "\n",
    "We use validating sets, and evaluate our models on CIFAR-10 dataset.\n",
    "\n",
    "We will also try the Batch normalization layer in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_03.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_03.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # images\n",
    "import numpy as np #numpy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import requests\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "from scipy.signal import convolve2d # convolutionof the 2D signals\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutmR93t-uis"
   },
   "source": [
    "# 📌 Defining terms for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCmwOM9q_4td"
   },
   "source": [
    "## 📒 Convolution\n",
    "A convolution is defined as the integral of the product of the two functions after one is reversed and shifted. It is a mathmematical way how to analyze behavior of the functions and the relation between the functions.\n",
    "\n",
    "In image processing, **kernel** or **convolution matrix** or **mask** is a small matrix. In general the convolution in image processing is defined as:\n",
    "\n",
    "$$g(x, y) = \\omega * f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\omega(s,t)f(x-s, y-t)$$\n",
    "\n",
    "where $g(x,y)$ is filtered image, $f(x,y)$ is original image, $\\omega$ if the filter kernel. \n",
    "\n",
    "A kernel (also called a filter) is a smaller-sized matrix in comparison to the dimensions of the input image, that consists of real valued entries.\n",
    "\n",
    "\n",
    "![Example of the Convolution](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/convolution_example.gif?raw=true \"Conv eg\")\n",
    "\n",
    "### 💡 Example filters\n",
    "\n",
    "|Name|Definition|\n",
    "|----|:--------:|\n",
    "|Identity| $\\left[\\begin{matrix}0&0&0\\\\0&1&0\\\\0&0&0\\end{matrix}\\right]$|\n",
    "|Sobel vertical edge detection| $\\left[\\begin{matrix}+1&0&-1\\\\+2&0&-2\\\\+1&0&-1\\end{matrix}\\right]$|\n",
    "|Sobel horizontal edge detection| $\\left[\\begin{matrix}+1&+2&+1\\\\0&0&0\\\\-1&-2&-1\\end{matrix}\\right]$|\n",
    "|Edge detection| $\\left[\\begin{matrix}-1&-1&-1\\\\-1&8&-1\\\\-1&-1&-1\\end{matrix}\\right]$|\n",
    "|Sharpen| $\\left[\\begin{matrix}0&-1&0\\\\-1&5&-1\\\\0&-1&0\\end{matrix}\\right]$|\n",
    "|Uniform blur|$\\frac{1}{9}\\left[\\begin{matrix}1&1&1\\\\1&1&1\\\\1&1&1\\end{matrix}\\right]$|\n",
    "|Gaussian blur 3x3| $\\frac{1}{16}\\left[\\begin{matrix}1&2&1\\\\2&4&2\\\\1&2&1\\end{matrix}\\right]$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsiIQveNL991"
   },
   "source": [
    "## 📒 Padding\n",
    "\n",
    "One tricky issue when applying convolutional is losing pixels on the edges of our image. A straightforward solution to this problem is to add extra pixels around the boundary of our input image, which increases the effective size of the image.\n",
    "\n",
    "![Padding example](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/padding_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_TpDPYXL-Cz"
   },
   "source": [
    "# 🚀 Practical example of convolution and padding without TF\n",
    "* We will download the famous Lena image and try to use some filters ourselves first\n",
    "\n",
    "#### 🤩 **Fun fact**: Have you seen the Lena image before? Do you know the story behind it? 🤩\n",
    "\n",
    "Alexander Sawchuk estimates that it was in June or July of 1973 when he, then an assistant professor of electrical engineering at the USC Signal and Image Processing Institute (SIPI), along with a graduate student and the SIPI lab manager, was hurriedly searching the lab for a good image to scan for a colleague's conference paper. They had tired of their stock of usual test images, dull stuff dating back to television standards work in the early 1960s. They wanted something glossy to ensure good output dynamic range, and they wanted a human face. Just then, somebody happened to walk in with a recent issue of Playboy.\n",
    "\n",
    "The engineers tore away the top third of the centerfold so they could wrap it around the drum of their Muirhead wirephoto scanner, which they had outfitted with analog-to-digital converters (one each for the red, green, and blue channels) and a Hewlett Packard 2100 minicomputer. The Muirhead had a fixed resolution of 100 lines per inch and the engineers wanted a 512 x 512 image, so they limited the scan to the top 5.12 inches of the picture, effectively cropping it at the subject's shoulders.\n",
    "\n",
    "* See more at [2001 paper IEEE Professional Communication Society](http://www.lenna.org/pcs_mirror/may_june01.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "lena_img_url = 'https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/lena.png?raw=true'\n",
    "\n",
    "response = requests.get(lena_img_url)\n",
    "img = Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the input image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's make it grayscale\n",
    "* 🔎 How many color channels did the original picture have? And how about grayscale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb = np.array(img)\n",
    "img = rgb2gray(rgb)\n",
    "img = img/255.0\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can try some of the filters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blur_mask = np.ones((3,3))/9.0\n",
    "edge_mask = np.ones((3,3))*-1\n",
    "edge_mask[1, 1] = 8\n",
    "mask = np.array([\n",
    "    [ 0,-1, 0],\n",
    "    [-1, 4,-1],\n",
    "    [ 0,-1, 0]\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will start with bluring the image\n",
    "* 🔎 What is the effect of the blur? What value does the uniform blur produce?\n",
    "* 🔎 Why is blur often used in the image preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_blur = convolve2d(img, blur_mask, boundary='symm', mode='same')\n",
    "plt.imshow(img_blur, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can apply another filter\n",
    "* 🔎 Do you know what will the filter in the *edge_mask* variable do?\n",
    "    * What is the effect of *np.clip()*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_blur = convolve2d(img, blur_mask, boundary='symm', mode='same')\n",
    "result = np.clip(convolve2d(img_blur, edge_mask, boundary='symm', mode='same'), 0, 1)\n",
    "plt.imshow(result, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4heY4fMFL9z3"
   },
   "source": [
    "## 📒 Pooling\n",
    "\n",
    "Pooling is a way how to decrease the amount of information transfered from one layer to another.\n",
    "The standard way ho to do it is Average Pooling and Maximum Pooling.\n",
    "\n",
    "- Pooling results in (some) invariance to translation because shifting the image slightly does not change the activation map significantly. This property is referred to as translation invariance. \n",
    "\n",
    "- The idea is that similar images often have very different relative locations of the distinctive shapes within them, and translation invariance helps in being able to classify such images in a similar way. \n",
    "    - For example, one should be able to classify a bird as a bird, irrespective of where it occurs in the image.\n",
    "    - Disadvantege is that you can for example succesfully classify image as face even though the position of eyes and mouth is switched, because model doesn't care about location of features in the image, their presence is for the model enough\n",
    "    \n",
    "- Avg. pooling is rarely used, usually we use max-pooling in the hidden layers, the only exception might be the last layer, where avg. pooling can significantly reduce the number of parameters.\n",
    "- One alternative to using fully connected layers is to use average pooling across the whole spatial area of the final set of activation maps to create a single value. \n",
    "    - Therefore, the number of features created in the final spatial layer will be exactly equal to the number of filters. In this scenario, if the final activation maps are of size 7 × 7 × 256, then 256 features will be created. \n",
    "    - Each feature will be the result of aggregating 49 values. This type of approach greatly reduces the parameter footprint of the fully connected layers\n",
    "\n",
    "![MaxPooling example](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/pooling_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡ Shape of the chosen layers have effect on the output dimension ⚡\n",
    "\n",
    "- 💡 *Convolution* - **Increases depth** thus if you start with a grayscale image with depth = 1 and apply Conv2D layer with 32 filters, output will have the depth of 32\n",
    "- 💡 *Pooling* - **Reduces width and height**, but depth stays the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKoUodQT5It4"
   },
   "source": [
    "# Tensorflow implementation of the Convolution Neural Network is quite simple\n",
    "\n",
    "### Utility functions\n",
    "There are some functions we will use later several times\n",
    "* `show_history` - show history plots of the **fit** method\n",
    "\n",
    "* `show_example` - show 10x10 image grid with input image examples\n",
    "\n",
    "* `display_activation` - CNN produces matrices as an each layer output, we can take a look at the output of the layers using this function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "def show_example(train_x, train_y, class_names):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(train_x[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[train_y[i][0]])\n",
    "    plt.show()\n",
    "    \n",
    "def display_activation(activations, col_size, row_size, act_index):\n",
    "    activation = activations[act_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            if activation_index < activation.shape[3]-1:\n",
    "                activation_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6HHl9Cb5IuB"
   },
   "source": [
    "## Load dataset\n",
    "Import dataset **CIFAR10**\n",
    "* The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. \n",
    "* The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. \n",
    "* There are 6,000 images of each class.\n",
    "\n",
    "Dataset is then:\n",
    "* splitted into train and test set,\n",
    "* converted from the range <0,255> into <0, 1>,\n",
    "* *train* is splitted into *train* and *validation* set,\n",
    "* class names are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cifar is the basic dataset for image classifaction\n",
    "dataset = tf.keras.datasets.cifar10\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 32x32 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_count = len(class_names)\n",
    "print('Class count:', class_count, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will start with checking example images of the dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_example(train_x, train_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uli2EHB85IuT"
   },
   "source": [
    "# 🚀 Definition of the model\n",
    "* The base model is defined as *Sequential* with just 2 convolutional layers.\n",
    "* We will start with very simple model first, but we will make the model better later!\n",
    "\n",
    "## 🔎 Let's find answers to these quetions first! 🔎\n",
    "\n",
    "1) What parameters can you set to Conv2D layer?\n",
    "\n",
    "2) If we have Conv2D layer with 32 filters and (32, 32, 3) image input, what will be the depth of the output? What will be the WxH shape? Will padding have any effect?\n",
    "\n",
    "3) We will apply MaxPooling2D with (2, 2) filters after the layer? What output dimension do we have now?\n",
    "\n",
    "4) Why softmax? Why accuracy metric?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uz_cLvzw5Iub",
    "tags": []
   },
   "source": [
    "# Fit the model for defined number of epochs.\n",
    "Show the history of learning, evaluate the efficiency of the model.\n",
    "\n",
    "* 🔎 Did we overfit the model? What can we do about it if so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[model_checkpoint_callback], batch_size=32, epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "# Load best setup\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "El-aMWON5Iuo"
   },
   "source": [
    "# 📊 Vizualization of the confusion matrix\n",
    "* By definition a confusion matrix $C$ is such that $C_{i,j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$\n",
    "    * 🔎 How can we read such matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(predictions, axis=1), test_y.ravel(), normalize='pred')\n",
    "plt.figure(figsize=(class_count,class_count))\n",
    "\n",
    "plt.imshow(conf_matrix)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        text = plt.text(j, i, \"{:.2f}\".format(conf_matrix[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.xticks(range(class_count), class_names)\n",
    "plt.yticks(range(class_count), class_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCnIUespE4P2"
   },
   "source": [
    "# Visualize the layers\n",
    "* Let's see what the network was able to learn from the train data. \n",
    "* For that, we need to prepare a new model and see the ouputs of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0fMPy-_rdcB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the outputs form all layers in the model\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "\n",
    "# Create the model that has single input and as an output all the outputs from the layers. \n",
    "# Because the layers are connected then the output from first layer is propagated into second layer and the output is computed of it.\n",
    "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get all the outputs from the model for 10-th input\n",
    "activations = activation_model.predict(train_x[10].reshape((1, 32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "qn1tQ1v1ulrj",
    "outputId": "ec7f97ad-743a-491a-eb63-ddd656065d93",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_x[10][:,:,0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔎 Can you describe purpose of some layers based on the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "colab_type": "code",
    "id": "RMEV2Qjpu98v",
    "outputId": "42a09ea4-126b-4c45-aef2-06f42253db67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the output from the first layer - Conv2D\n",
    "display_activation(activations, 8, 4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 702
    },
    "colab_type": "code",
    "id": "FUWTBNVEvkhk",
    "outputId": "e6ed2884-1a69-42ac-d8f9-e8a45561349d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the second convolution layer\n",
    "display_activation(activations, 4, 4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 You can see that the ANN just automated the manual image filter composition for the feature extraction used by traditional computer vision approaches\n",
    "\n",
    "![Meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_02_meme.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔎 How to make ANN better?\n",
    "* We can make out ANN better by architecture design changes or we can focus on detailts of the architecture such as activation functions or adding some helpful layers\n",
    "    * e.g. Dropout, Batch. norm, ...\n",
    "\n",
    "## 📒 Batch normalization\n",
    "- If we have some two input variables with scales from 0 to 1 and from 0 to 1000 we can normalize them\n",
    "    - Larger scale does not mean that the varible is more important than the other one\n",
    "    - It's clear that without normalization one weight will be very high and other very low to balance the scale difference\n",
    "    - This leads to slow gradient descent convergence - we need small learning rates\n",
    "        - Loss function space is not smooth - gradients may oscillate back and forth before finding optimum\n",
    "        \n",
    "- This issue is present in hidden layers as well due to the mini-batch learning\n",
    "    - Each batch is different from others from distributions point of view\n",
    "    - Covariate shift effect - distribution of data is constantly changing during training\n",
    "        - E.g. We train model on images of black cats - we learn how to map input X to output Y\n",
    "        - If we now use images of colored cats for testing purposes the model will fail - input X changed thus learned mapping is invalid \n",
    "        - This shift happens internally with mini-batches all the time\n",
    "        \n",
    "- 📌 **Batch norm. standardize activation values to have same mean and variance among batch**\n",
    "    - Slight regularization - it adds some noise to each hidden layer’s activations so less overfitting\n",
    "    - We can use higher learning rates because batch normalization makes sure that there’s no activation that’s gone really high or really low\n",
    "    - It makes the landscape of the corresponding optimization problem be significantly more smooth. This ensures, in particular, that the gradients are more predictive and thus allow for use of larger range of learning rates and faster network convergence\n",
    "    \n",
    "- 💡 Keras: tf.keras.layers.BatchNormalization\n",
    "    - https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    - Usually after Conv or Dense layers\n",
    "    - There are of course multiple different approaches where the batch norm. can be placed\n",
    " \n",
    "## 📒 Activation function choice\n",
    "* We can use any activation function we want, very common choice is ReLU or Leaky ReLU\n",
    "* Another very popular activation function nowadays is **Mish (Self Regularized Non-Monotonic Activation Function)**\n",
    "* It's not only one, there are more Swish etc.\n",
    "\n",
    "* 🚀 Definitely checkout [Mish repo](https://github.com/digantamisra98/Mish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mish(Activation):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "get_custom_objects().update({'mish': Mish(mish)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡ We can modify our network to use such tweaks very easily\n",
    "* We can start with the Mish activation function\n",
    "* Moreover our model propably overtfitted thus can we do about that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='mish', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(16, (3,3), activation='mish'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[model_checkpoint_callback], batch_size=32, epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "# Load best setup\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔎 Is the model better? What about the overfit?\n",
    "\n",
    "## 🚀 Now we will add Batch normalization as well and try slight layer order change\n",
    "* What will happen if we stack 2x Conv2D layers? \n",
    "* 🔎 Do we have more or less parameters than in the previous case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='mish', input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(16, (3,3), activation='mish'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[model_checkpoint_callback], batch_size=32, epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "# Load best setup\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data aug example](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/data_aug.png?raw=true)\n",
    "\n",
    "## 📌 ANN models need huge amount of data as an input to function properly\n",
    "### 💡 Data augmentation is very popular technique for the image data as it can help enlarge the dataset and harden the model for abnormal conditions\n",
    "\n",
    "* ❌ There might be bug present in the TF2, so beware the label encoding\n",
    "    * I had the same issue even 2 years ago, so I guess that today it's not a bug but a feature 😅\n",
    "* 💡 We need to one-hot encode the labels and change the loss function to the CategoricalCrossentropy\n",
    "    * Note that the output activation function is now set to softmax\n",
    "    * Take a look at [this](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cifar is the basic dataset for image classifaction\n",
    "dataset = tf.keras.datasets.cifar10\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_count = len(class_names)\n",
    "\n",
    "train_y = tf.keras.utils.to_categorical(train_y, class_count)\n",
    "test_y = tf.keras.utils.to_categorical(test_y, class_count)\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 32x32 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "print('Class count:', class_count, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📒 ImageDataGenerator provides variety of modifications\n",
    "* 💡 You need to decide based on **the input data and task** that you try to solve which of the image modifications can be used given the real data obtained 💡\n",
    "    * 📸 Imagine that you are designing a model for a **car detection used by the entry gate camera system** at the university parking lot entrance \n",
    "        * Your training dataset is made of images of cars, will the vertical flip (i.e. cars turned on the roof) of the images make any sense for your task?\n",
    "    * 🚒 And what if the task is **traffic accident detection** in front of the car for autonomous driving system?\n",
    "        * Can vertical flip help you now? 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "datagen.fit(train_x)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(128, (5,5), activation='mish', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='mish', padding='same'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='mish'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              loss=keras.losses.CategoricalCrossentropy(), \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(datagen.flow(train_x, train_y,batch_size=32,\n",
    "         subset='training'),  validation_data=datagen.flow(train_x, train_y,\n",
    "         batch_size=32, subset='validation'), epochs=epochs)\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "myIgGem85IvT"
   },
   "source": [
    "# ✅  Tasks for the lecture (2p)\n",
    "<!-- 1. [Data augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/) is used to expand the training dataset in order to improve the performance and ability of the model to generalize.\n",
    "    - [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) -->\n",
    "    \n",
    "1) Design a model which will be able to classify **CIFAR10** with accuracy higher than **80% - (1p)**\n",
    "\n",
    "    - 💡 Definitely train your model with more epochs than 10 (you can try 20 to 50 for example)\n",
    "\n",
    "    - 💡 I recommend experimenting with the batch sizes, lower the batch size if the training is not stable (high variance in the loss function values) or higher the number if the training is too slow \n",
    "    \n",
    "2) Try to work with MNIST and FashionMnist datasets as an image - **(1p)**\n",
    "\n",
    "    - 99% on **Mnist** is achievable using CNN\n",
    "    \n",
    "    - 94% on **Fashion-Mnist**  (https://keras.io/api/datasets/fashion_mnist/) too "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
