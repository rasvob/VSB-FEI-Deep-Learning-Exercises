{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm"
   },
   "source": [
    "# Deep Learning - Exercise 3\n",
    "The aim of the lecture is to learn how to use the basic architecture based on convolutional layers and how to classify image data.\n",
    "\n",
    "### What We'll Cover Today:\n",
    "* üéØ Convolutional Neural Networks basics\n",
    "* üìä Working with CIFAR-10 dataset\n",
    "* ‚úÖ Model validation techniques \n",
    "* üîÑ Batch normalization in Keras\n",
    "\n",
    "### Dataset Focus: CIFAR-10 \n",
    "* 60,000 32x32 color images\n",
    "* 10 different classes\n",
    "* Standard computer vision benchmark\n",
    "\n",
    "### Core Concepts:\n",
    "* Model validation importance\n",
    "* Batch normalization benefits\n",
    "* CNN architecture basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_03.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_03.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # images\n",
    "import numpy as np #numpy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import requests\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "from scipy.signal import convolve2d # convolutionof the 2D signals\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutmR93t-uis"
   },
   "source": [
    "# üìå Defining terms for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCmwOM9q_4td"
   },
   "source": [
    "## üìí Convolution\n",
    "A convolution is defined as the integral of the product of the two functions after one is reversed and shifted. It is a mathmematical way how to analyze behavior of the functions and the relation between the functions.\n",
    "\n",
    "In image processing, **kernel** or **convolution matrix** or **mask** is a small matrix. In general the convolution in image processing is defined as:\n",
    "\n",
    "$$g(x, y) = \\omega * f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\omega(s,t)f(x-s, y-t)$$\n",
    "\n",
    "where $g(x,y)$ is filtered image, $f(x,y)$ is original image, $\\omega$ if the filter kernel. \n",
    "\n",
    "A kernel (also called a filter) is a smaller-sized matrix in comparison to the dimensions of the input image, that consists of real valued entries.\n",
    "\n",
    "\n",
    "![Example of the Convolution](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/convolution_example.gif?raw=true \"Conv eg\")\n",
    "\n",
    "### üí° Example filters\n",
    "\n",
    "|Name|Definition|\n",
    "|----|:--------:|\n",
    "|Identity| $\\left[\\begin{matrix}0&0&0\\\\0&1&0\\\\0&0&0\\end{matrix}\\right]$|\n",
    "|Sobel vertical edge detection| $\\left[\\begin{matrix}+1&0&-1\\\\+2&0&-2\\\\+1&0&-1\\end{matrix}\\right]$|\n",
    "|Sobel horizontal edge detection| $\\left[\\begin{matrix}+1&+2&+1\\\\0&0&0\\\\-1&-2&-1\\end{matrix}\\right]$|\n",
    "|Edge detection| $\\left[\\begin{matrix}-1&-1&-1\\\\-1&8&-1\\\\-1&-1&-1\\end{matrix}\\right]$|\n",
    "|Sharpen| $\\left[\\begin{matrix}0&-1&0\\\\-1&5&-1\\\\0&-1&0\\end{matrix}\\right]$|\n",
    "|Uniform blur|$\\frac{1}{9}\\left[\\begin{matrix}1&1&1\\\\1&1&1\\\\1&1&1\\end{matrix}\\right]$|\n",
    "|Gaussian blur 3x3| $\\frac{1}{16}\\left[\\begin{matrix}1&2&1\\\\2&4&2\\\\1&2&1\\end{matrix}\\right]$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsiIQveNL991"
   },
   "source": [
    "## üìí Padding\n",
    "\n",
    "One tricky issue when applying convolutional is losing pixels on the edges of our image. A straightforward solution to this problem is to add extra pixels around the boundary of our input image, which increases the effective size of the image.\n",
    "\n",
    "![Padding example](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/padding_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_TpDPYXL-Cz"
   },
   "source": [
    "# üöÄ Practical example of convolution and padding without TF\n",
    "* We will download the famous Lena image and try to use some filters ourselves first\n",
    "\n",
    "#### ü§© **Fun fact**: Have you seen the Lena image before? Do you know the story behind it? ü§©\n",
    "\n",
    "Alexander Sawchuk estimates that it was in June or July of 1973 when he, then an assistant professor of electrical engineering at the USC Signal and Image Processing Institute (SIPI), along with a graduate student and the SIPI lab manager, was hurriedly searching the lab for a good image to scan for a colleague's conference paper. They had tired of their stock of usual test images, dull stuff dating back to television standards work in the early 1960s. They wanted something glossy to ensure good output dynamic range, and they wanted a human face. Just then, somebody happened to walk in with a recent issue of Playboy.\n",
    "\n",
    "The engineers tore away the top third of the centerfold so they could wrap it around the drum of their Muirhead wirephoto scanner, which they had outfitted with analog-to-digital converters (one each for the red, green, and blue channels) and a Hewlett Packard 2100 minicomputer. The Muirhead had a fixed resolution of 100 lines per inch and the engineers wanted a 512 x 512 image, so they limited the scan to the top 5.12 inches of the picture, effectively cropping it at the subject's shoulders.\n",
    "\n",
    "* See more at [2001 paper IEEE Professional Communication Society](http://www.lenna.org/pcs_mirror/may_june01.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "lena_img_url = 'https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/lena.png?raw=true'\n",
    "\n",
    "response = requests.get(lena_img_url)\n",
    "img = Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the input image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's make it grayscale\n",
    "* üîé How many color channels did the original picture have? And how about grayscale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb = np.array(img)\n",
    "img = rgb2gray(rgb)\n",
    "img = img/255.0\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can try some of the filters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blur_mask = np.ones((3,3))/9.0\n",
    "edge_mask = np.ones((3,3))*-1\n",
    "edge_mask[1, 1] = 8\n",
    "mask = np.array([\n",
    "    [ 0,-1, 0],\n",
    "    [-1, 4,-1],\n",
    "    [ 0,-1, 0]\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will start with bluring the image\n",
    "* üîé What is the effect of the blur? \n",
    "    * What value does the uniform blur produce?\n",
    "* üîé Why is blur often used in the image preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_blur = convolve2d(img, blur_mask, boundary='symm', mode='same')\n",
    "plt.imshow(img_blur, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can apply another filter\n",
    "* üîé Do you know what will the filter in the *edge_mask* variable do?\n",
    "    * What is the effect of *np.clip()*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_blur = convolve2d(img, blur_mask, boundary='symm', mode='same')\n",
    "result = np.clip(convolve2d(img_blur, edge_mask, boundary='symm', mode='same'), 0, 1)\n",
    "plt.imshow(result, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4heY4fMFL9z3"
   },
   "source": [
    "## üìí Pooling\n",
    "\n",
    "Pooling is a way how to decrease the amount of information transfered from one layer to another.\n",
    "The standard way ho to do it is Average Pooling and Maximum Pooling.\n",
    "\n",
    "- Pooling results in (some) invariance to translation because shifting the image slightly does not change the activation map significantly. This property is referred to as translation invariance. \n",
    "\n",
    "- The idea is that similar images often have very different relative locations of the distinctive shapes within them, and translation invariance helps in being able to classify such images in a similar way. \n",
    "    - For example, one should be able to classify a bird as a bird, irrespective of where it occurs in the image.\n",
    "    - Disadvantege is that you can for example succesfully classify image as face even though the position of eyes and mouth is switched, because model doesn't care about location of features in the image, their presence is for the model enough\n",
    "    \n",
    "- Avg. pooling is rarely used, usually we use max-pooling in the hidden layers, the only exception might be the last layer, where avg. pooling can significantly reduce the number of parameters.\n",
    "- One alternative to using fully connected layers is to use average pooling across the whole spatial area of the final set of activation maps to create a single value. \n",
    "    - Therefore, the number of features created in the final spatial layer will be exactly equal to the number of filters. In this scenario, if the final activation maps are of size 7 √ó 7 √ó 256, then 256 features will be created. \n",
    "    - Each feature will be the result of aggregating 49 values. This type of approach greatly reduces the parameter footprint of the fully connected layers\n",
    "\n",
    "![MaxPooling example](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/pooling_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Understanding Layer Dimensions\n",
    "\n",
    "### Convolution Effects üîç\n",
    "* Input: grayscale image (depth = 1)\n",
    "* Conv2D with 32 filters:\n",
    "  * ‚¨ÜÔ∏è Depth increases to 32\n",
    "  * ‚ÜîÔ∏è Width/height may change based on padding\n",
    "\n",
    "### Pooling Effects üîÑ\n",
    "* ‚¨áÔ∏è Reduces spatial dimensions\n",
    "  * Width √∑ pooling size\n",
    "  * Height √∑ pooling size\n",
    "* üîí Depth remains unchanged\n",
    "\n",
    "üí° Remember:\n",
    "* Conv2D ‚Üí More features (depth)\n",
    "* Pooling ‚Üí Smaller spatial dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKoUodQT5It4"
   },
   "source": [
    "# Tensorflow implementation of the Convolution Neural Network is quite simple\n",
    "\n",
    "### Utility functions\n",
    "There are some functions we will use later several times\n",
    "* `show_history` - show history plots of the **fit** method\n",
    "\n",
    "* `show_example` - show 10x10 image grid with input image examples\n",
    "\n",
    "* `display_activation` - CNN produces matrices as an each layer output, we can take a look at the output of the layers using this function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "def show_example(train_x, train_y, class_names):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(train_x[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[train_y[i][0]])\n",
    "    plt.show()\n",
    "    \n",
    "def display_activation(activations, col_size, row_size, act_index):\n",
    "    activation = activations[act_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            if activation_index < activation.shape[3]-1:\n",
    "                activation_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6HHl9Cb5IuB"
   },
   "source": [
    "## Load dataset\n",
    "Import dataset **CIFAR10**\n",
    "* The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. \n",
    "* The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. \n",
    "* There are 6,000 images of each class.\n",
    "\n",
    "Dataset is then:\n",
    "* splitted into train and test set,\n",
    "* converted from the range <0,255> into <0, 1>,\n",
    "* *train* is splitted into *train* and *validation* set,\n",
    "* class names are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cifar is the basic dataset for image classifaction\n",
    "dataset = tf.keras.datasets.cifar10\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 32x32 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_count = len(class_names)\n",
    "print('Class count:', class_count, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will start with checking example images of the dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_example(train_x, train_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uli2EHB85IuT"
   },
   "source": [
    "# üöÄ Definition of the model\n",
    "* The base model is defined as *Sequential* with just 2 convolutional layers.\n",
    "* We will start with very simple model first, but we will make the model better later!\n",
    "\n",
    "### Conv2D Layer Parameters üîß\n",
    "* filters: number of kernels\n",
    "* kernel_size: filter dimensions\n",
    "* activation: activation function\n",
    "* padding: `valid` or `same`\n",
    "* input_shape: for first layer only\n",
    "\n",
    "### Dimension Questions ‚ùì\n",
    "\n",
    "1. For (32,32,3) input + `Conv2D(32)`:\n",
    "   * Output depth = 32 (filter count)\n",
    "   * WxH depends on padding\n",
    "   * 'same' padding preserves dimensions\n",
    "\n",
    "2. After `MaxPool2D(2,2)`:\n",
    "   * Depth stays 32\n",
    "   * Width √∑ 2\n",
    "   * Height √∑ 2\n",
    "\n",
    "### Why These Choices? ü§î\n",
    "* Softmax: ‚ùì\n",
    "* Accuracy: ‚ùì\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uz_cLvzw5Iub",
    "tags": []
   },
   "source": [
    "# Fit the model for defined number of epochs.\n",
    "Show the history of learning, evaluate the efficiency of the model.\n",
    "\n",
    "* üîé Did we overfit the model? What can we do about it if so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[model_checkpoint_callback], batch_size=32, epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "# Load best setup\n",
    "model.load_weights(\"best.weights.h5\")\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "El-aMWON5Iuo"
   },
   "source": [
    "# üìä Vizualization of the confusion matrix\n",
    "* By definition a confusion matrix $C$ is such that $C_{i,j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$\n",
    "    * üîé How can we read such matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(predictions, axis=1), test_y.ravel(), normalize='pred')\n",
    "plt.figure(figsize=(class_count,class_count))\n",
    "\n",
    "plt.imshow(conf_matrix)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        text = plt.text(j, i, \"{:.2f}\".format(conf_matrix[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.xticks(range(class_count), class_names)\n",
    "plt.yticks(range(class_count), class_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCnIUespE4P2"
   },
   "source": [
    "# Visualize the layers\n",
    "* Let's see what the network was able to learn from the train data. \n",
    "* For that, we need to prepare a new model and see the ouputs of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with some input data to define the input shape\n",
    "_ = model.predict(train_x[:1])\n",
    "\n",
    "# Get the outputs form all layers in the model\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "\n",
    "# Create the model that has single input and as an output all the outputs from the layers. \n",
    "# Because the layers are connected then the output from first layer is propagated into second layer and the output is computed of it.\n",
    "activation_model = keras.models.Model(inputs=model.inputs, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get all the outputs from the model for 10-th input\n",
    "activations = activation_model.predict(train_x[10].reshape((1, 32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "qn1tQ1v1ulrj",
    "outputId": "ec7f97ad-743a-491a-eb63-ddd656065d93",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_x[10][:,:,0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Can you describe purpose of some layers based on the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "colab_type": "code",
    "id": "RMEV2Qjpu98v",
    "outputId": "42a09ea4-126b-4c45-aef2-06f42253db67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the output from the first layer - Conv2D\n",
    "display_activation(activations, 8, 4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 702
    },
    "colab_type": "code",
    "id": "FUWTBNVEvkhk",
    "outputId": "e6ed2884-1a69-42ac-d8f9-e8a45561349d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the second convolution layer\n",
    "display_activation(activations, 4, 4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° You can see that the ANN just automated the manual image filter composition for the feature extraction used by traditional computer vision approaches\n",
    "\n",
    "![Meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_02_meme.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé How to make ANN better?\n",
    "* We can make out ANN better by architecture design changes or we can focus on detailts of the architecture such as activation functions or adding some helpful layers\n",
    "    * e.g. Dropout, Batch. norm, ...\n",
    "\n",
    "## üìí Batch normalization\n",
    "- If we have some two input variables with scales from 0 to 1 and from 0 to 1000 we can normalize them\n",
    "    - Larger scale does not mean that the varible is more important than the other one\n",
    "    - It's clear that without normalization one weight will be very high and other very low to balance the scale difference\n",
    "    - This leads to slow gradient descent convergence - we need small learning rates\n",
    "        - Loss function space is not smooth - gradients may oscillate back and forth before finding optimum\n",
    "        \n",
    "- This issue is present in hidden layers as well due to the mini-batch learning\n",
    "    - Each batch is different from others from distributions point of view\n",
    "    - Covariate shift effect - distribution of data is constantly changing during training\n",
    "        - E.g. We train model on images of black cats - we learn how to map input X to output Y\n",
    "        - If we now use images of colored cats for testing purposes the model will fail - input X changed thus learned mapping is invalid \n",
    "        - This shift happens internally with mini-batches all the time\n",
    "        \n",
    "- üìå **Batch norm. standardize activation values to have same mean and variance among batch**\n",
    "    - Slight regularization - it adds some noise to each hidden layer‚Äôs activations so less overfitting\n",
    "    - We can use higher learning rates because batch normalization makes sure that there‚Äôs no activation that‚Äôs gone really high or really low\n",
    "    - It makes the landscape of the corresponding optimization problem be significantly more smooth. This ensures, in particular, that the gradients are more predictive and thus allow for use of larger range of learning rates and faster network convergence\n",
    "    \n",
    "- üí° Keras: tf.keras.layers.BatchNormalization\n",
    "    - https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    - Usually after Conv or Dense layers\n",
    "    - There are of course multiple different approaches where the batch norm. can be placed\n",
    " \n",
    "## üìí Activation function choice\n",
    "* We can use any activation function we want, very common choice is ReLU or Leaky ReLU\n",
    "* Another very popular activation function nowadays is **Mish (Self Regularized Non-Monotonic Activation Function)**\n",
    "* It's not only one, there are more Swish etc.\n",
    "\n",
    "* üöÄ Definitely checkout [Mish repo](https://github.com/digantamisra98/Mish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mish(Activation):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "get_custom_objects().update({'mish': Mish(mish)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° We can modify our network to use such tweaks very easily\n",
    "\n",
    "### Activation Update üìà\n",
    "* Replace ReLU with Mish\n",
    "  * Smoother gradients\n",
    "  * Better performance\n",
    "\n",
    "### Overfitting Solutions üõ°Ô∏è\n",
    "* Add Dropout layers\n",
    "* Use BatchNormalization\n",
    "* Increase regularization\n",
    "* Reduce model capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='mish', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(16, (3,3), activation='mish'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[model_checkpoint_callback], batch_size=32, epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "# Load best setup\n",
    "model.load_weights(\"best.weights.h5\")\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Is the model better? What about the overfit?\n",
    "\n",
    "## üöÄ Now we will add Batch normalization as well and try slight layer order change\n",
    "* What will happen if we stack 2x Conv2D layers? \n",
    "* üîé Do we have more or less parameters than in the previous case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='mish', input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(16, (3,3), activation='mish'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[model_checkpoint_callback], batch_size=32, epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "# Load best setup\n",
    "model.load_weights(\"best.weights.h5\")\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data aug example](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/data_aug.png?raw=true)\n",
    "\n",
    "## üìå ANN models need huge amount of data as an input to function properly\n",
    "### üí° Data augmentation is very popular technique for the image data as it can help enlarge the dataset and harden the model for abnormal conditions\n",
    "\n",
    "* ‚ùå There might be bug present in the TF2, so beware the label encoding\n",
    "    * I had the same issue even few years ago, so I guess that today it's not a bug but a feature üòÖ\n",
    "* üí° We need to one-hot encode the labels and change the loss function to the CategoricalCrossentropy\n",
    "    * Note that the output activation function is now set to softmax\n",
    "    * Take a look at [this](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cifar is the basic dataset for image classifaction\n",
    "dataset = tf.keras.datasets.cifar10\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_count = len(class_names)\n",
    "\n",
    "train_y = tf.keras.utils.to_categorical(train_y, class_count)\n",
    "test_y = tf.keras.utils.to_categorical(test_y, class_count)\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "# the data are in the form of 32x32 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "print('Class count:', class_count, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìí ImageDataGenerator provides variety of modifications\n",
    "* üí° You need to decide based on **the input data and task** that you try to solve which of the image modifications can be used given the real data obtained üí°\n",
    "    * üì∏ Imagine that you are designing a model for a **car detection used by the entry gate camera system** at the university parking lot entrance \n",
    "        * Your training dataset is made of images of cars, will the vertical flip (i.e. cars turned on the roof) of the images make any sense for your task?\n",
    "    * üöí And what if the task is **traffic accident detection** in front of the car for autonomous driving system?\n",
    "        * Can vertical flip help you now? üòä\n",
    "\n",
    "![Example of upside down car](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_03_car.jpg?raw=true \"dl_03_car eg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "datagen.fit(train_x)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(128, (5,5), activation='mish', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='mish', padding='same'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='mish'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              loss=keras.losses.CategoricalCrossentropy(), \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(datagen.flow(train_x, train_y,batch_size=32,\n",
    "         subset='training'),  validation_data=datagen.flow(train_x, train_y,\n",
    "         batch_size=32, subset='validation'), epochs=epochs)\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ What if we have large datasets and we are not able to fit it into the memory?\n",
    "* üí° A very simple solution is to transform the dataset into the specific folder structure and use [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "    * We can leverage `tf.keras.preprocessing.image_dataset_from_directory` method for loading\n",
    "* Let's try it with MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " DATASET_NAME = 'mnist'\n",
    " OUTPUT_DIR = 'mnist_data'  # Directory to save the MNIST images\n",
    " TRAIN_DIR = os.path.join(OUTPUT_DIR, 'train')\n",
    " TEST_DIR = os.path.join(OUTPUT_DIR, 'test')\n",
    "\n",
    " # Create the output directories if they don't exist\n",
    " os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    " os.makedirs(TEST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, labels, directory, split_name):\n",
    "    \"\"\"Saves images from NumPy arrays to disk in a directory structure.\n",
    "\n",
    "    Args:\n",
    "        images: A NumPy array of images.\n",
    "        labels: A NumPy array of corresponding labels.\n",
    "        directory: The base directory to save the images to.\n",
    "        split_name: \"train\" or \"test\", used for subdirectory creation.\n",
    "    \"\"\"\n",
    "\n",
    "    split_dir = os.path.join(directory, split_name)  # e.g., mnist_data/train\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    for i, (image, label) in tqdm(enumerate(zip(images, labels)), total=len(images)):\n",
    "        label_dir = os.path.join(split_dir, str(label))  # e.g., mnist_data/train/5\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        image = Image.fromarray(image)  # Convert NumPy array to PIL Image\n",
    "        image_path = os.path.join(label_dir, f'{i:05d}.png')  # e.g., mnist_data/train/5/00001.png\n",
    "        image.save(image_path)\n",
    "\n",
    "    print(f\"Finished saving {split_name} split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(x_train, y_train, OUTPUT_DIR, 'train')\n",
    "save_images(x_test, y_test, OUTPUT_DIR, 'test')\n",
    "\n",
    "print(\"MNIST dataset saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Now we can load the dataset from the directory as a datastream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = 32, 32  # Adjust to the image size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    labels='inferred',  # Assumes labels are based on directory structure\n",
    "    label_mode='categorical',  # One-hot encode the labels\n",
    "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,  # 20% of the training data will be used for validation\n",
    "    subset='training',  # This specifies that this is the training split\n",
    "    seed=13  # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DIR,  # Use the same training directory for validation data\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',  # This specifies that this is the validation split\n",
    "    seed=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Dataset for higher performance\n",
    "* `tf.data.AUTOTUNE` will dynamically determine the optimal buffer size at runtime, which can help improve the efficiency of data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Build the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 \n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Rescaling(1./255, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),  # Normalize pixel values\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Finally, we will train the model and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, validation_data=validation_dataset, callbacks=[model_checkpoint_callback], epochs=10)\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Calculate the accuracy using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best.weights.h5\")\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "myIgGem85IvT"
   },
   "source": [
    "# ‚úÖ  Tasks for the lecture (2p)\n",
    "<!-- 1. [Data augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/) is used to expand the training dataset in order to improve the performance and ability of the model to generalize.\n",
    "    - [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) -->\n",
    "    \n",
    "1) Design a model which will be able to classify **CIFAR10** with accuracy higher than **80% - (1p)**\n",
    "\n",
    "    - üí° Definitely train your model with more epochs than 10 (you can try 20 to 50 for example)\n",
    "\n",
    "    - üí° I recommend experimenting with the batch sizes, lower the batch size if the training is not stable (high variance in the loss function values) or higher the number if the training is too slow \n",
    "    \n",
    "2) Try to work with MNIST and FashionMnist datasets as an image - **(1p)**\n",
    "\n",
    "    - 99% on **Mnist** is achievable using CNN\n",
    "    \n",
    "    - 94% on **Fashion-Mnist**  (https://keras.io/api/datasets/fashion_mnist/) too "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
