{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 11\n",
    "\n",
    "This exercise focuses on time series forecasting using deep learning techniques. We will apply these methods to predict natural gas consumption, building upon the pre-processed dataset from previous exercises.\n",
    "\n",
    "**Core Concepts**\n",
    "* ðŸ“ˆ Time series forecasting with deep learning\n",
    "* â›½ Natural gas consumption prediction\n",
    "* ðŸ“Š Utilizing pre-processed time series datasets\n",
    "* ðŸ§  Implementing deep learning models for time series data\n",
    "* ðŸ› ï¸ Practical application of deep learning to real-world forecasting problems\n",
    "\n",
    "The raw dataset is available at [vsb.ai](https://vsb.ai/natural-gas-forecasting), and we will be using a pre-processed version for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_11.ipynb)\n",
    "\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_11.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_keras_session(seed=42):\n",
    "    \"\"\"\n",
    "    Resets the Keras session and sets the random seeds for TensorFlow, NumPy, and Python.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed to use for random number generation.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()  # Clear the previous session.\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    #Optional, but recommended for full reproducibility on some systems.\n",
    "    #This also helps with GPU determinism.\n",
    "    tf.config.experimental.reset_memory_stats('GPU:0') #if you have gpu.\n",
    "\n",
    "    #Further optional, but can help with GPU determinism.\n",
    "    # tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    # If using CUDA, also set the following environment variable:\n",
    "    # import os\n",
    "    # os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Example usage:\n",
    "reset_keras_session(seed=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have prepared common metrics for model evaluation beforehand. These functions will be used later in the notebook to assess the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes MAPE\n",
    "\"\"\"\n",
    "def mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes SMAPE\n",
    "\"\"\"\n",
    "def symetric_mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred))/2.0))) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes MAE, MSE, MAPE, SMAPE, R2\n",
    "\"\"\"\n",
    "def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    y_true, y_pred = df['y_true'].values, df['y_pred'].values\n",
    "    return compute_metrics_raw(y_true, y_pred)\n",
    "\n",
    "def compute_metrics_raw(y_true: pd.Series, y_pred: pd.Series) -> pd.DataFrame:\n",
    "    mae, mse, mape, smape, r2 = mean_absolute_error(y_true=y_true, y_pred=y_pred), mean_squared_error(y_true=y_true, y_pred=y_pred), mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), symetric_mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), r2_score(y_true=y_true, y_pred=y_pred)\n",
    "    return pd.DataFrame.from_records([{'MAE': mae, 'MSE': mse, 'MAPE': mape, 'SMAPE': smape, 'R2': r2}], index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/raw/main/datasets/ppnet_metar_v8_MAD.csv', sep=';', index_col=0)\n",
    "df = df[df.Year < 2019].copy()\n",
    "df.loc[:, 'TestSet'] = 0\n",
    "df.loc[df.Year == 2018, 'TestSet'] = 1\n",
    "df = df.loc[:, ['Year', 'Month', 'Day', 'Hour', 'Day_of_week', 'Holiday', 'Consumption', 'Temperature', 'Pressure', 'Humidity', 'TestSet']]\n",
    "df.index = pd.DatetimeIndex(df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Dataset Description\n",
    "\n",
    "This comprehensive dataset spans six complete years (January 1, 2013 to December 31, 2018), with all features recorded at hourly intervals, yielding 52,584 data points. The dataset integrates three essential components:\n",
    "\n",
    "## ðŸ­ Consumption Data\n",
    "\n",
    "The natural gas consumption data comes from Prague, the capital of the Czech Republic, with a distribution network serving 422,926 customers in 2018 and total consumption of 3.82 billion mÂ³. The consumption is distributed across:\n",
    "\n",
    "- **ðŸ˜ï¸ Residential sector**: 381,914 households (33.3% of consumption)\n",
    "- **ðŸ¢ Industrial sector**: \n",
    "  - 177 large customers (24.8%)\n",
    "  - 39,175 medium customers (18.9%)\n",
    "  - 1,652 small customers (21.9%)\n",
    "\n",
    "The remaining percentage represents operational losses during distribution (e.g., pipeline leaks).\n",
    "\n",
    "**ðŸ”¥ Heating Season Information**:\n",
    "- Period: September 1 to May 31\n",
    "- Activation criteria: Temperature below +13Â°C for two consecutive days with no forecast warming\n",
    "- Significance: Accounts for approximately 70-75% of annual consumption\n",
    "\n",
    "## ðŸŒ¡ï¸ Weather Variables\n",
    "\n",
    "Weather data was collected from the Prague LKPR airport weather station through METAR (aerodrome routine meteorological report) information. These standardized reports are regularly issued by airports and maintained in long-term archives, providing reliable meteorological data for our analysis.\n",
    "\n",
    "## ðŸ’° Economic Features\n",
    "\n",
    "The economic component consists of natural gas price data obtained from the Czech energy regulation office. This data provides crucial context for understanding consumption patterns in relation to market conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endogenous and Exogenous Variables in Forecasting Models\n",
    "\n",
    "In time series forecasting, understanding the distinction between endogenous and exogenous variables is fundamental for building effective models. This classification helps determine the structure of your forecasting approach and influences how you interpret relationships between variables.\n",
    "\n",
    "## Variable Classification\n",
    "\n",
    "### Endogenous Variable\n",
    "- **Consumption** ðŸ“Š: This is your primary target variable being forecasted within the model system. It's considered endogenous because its values are determined within the model and influenced by other variables in the system as well as its own past values.\n",
    "\n",
    "### Exogenous Variables\n",
    "These variables are determined outside your forecasting model and serve as inputs:\n",
    "\n",
    "- **Weather Variables** ðŸŒ¡ï¸: Data from the Prague LKPR airport weather station including temperature, humidity, precipitation, wind speed, and other meteorological measurements. These factors significantly influence energy consumption patterns but are not affected by consumption itself.\n",
    "\n",
    "- **Economic Indicators** ðŸ’¹: Natural gas price data from the Czech energy regulation office, which affects consumption behavior but is determined by broader market forces.\n",
    "\n",
    "- **Temporal Features** ðŸ“…: Time-related variables such as hour of day, day of week, month, and seasonal indicators derived from timestamps. These cyclical patterns help capture regular consumption behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have ~ 52k datapoints which should be sufficient even for very complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(y=df['Consumption'], x=df.index, color=df.Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(y=df['Temperature'], x=df.index, color=df.Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will start with simple next step prediction using LSTM network\n",
    "* We will limit the input to consumption variable first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will form input sequences first\n",
    "* We will use 168 hours of past data\n",
    "* Data will be scaled by `MinMaxScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, n_steps):\n",
    "    \"\"\"\n",
    "    Create sequences of data for LSTM input\n",
    "    data: input array\n",
    "    n_steps: number of time steps (lags)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(data[i + n_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Get the consumption data and convert to numpy array\n",
    "consumption = df['Consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Define the number of lags (time steps)\n",
    "n_steps = 168  # Use 168 hours of data to predict the next hour\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data = consumption[df.TestSet == 0]\n",
    "test_data = consumption[df.TestSet == 1]\n",
    "\n",
    "# Scale the input data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "X_train, y_train = create_sequences(train_data_scaled, n_steps)\n",
    "X_test, y_test = create_sequences(test_data_scaled, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the output data\n",
    "* Reshape for LSTM [samples, time steps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for np.nan or np.inf in the data\n",
    "* If there are any, the model will fail to converge - you will get ``NaNs`` in the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_train)), np.any(np.isnan(y_train)), np.any(np.isnan(X_test)), np.any(np.isnan(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = keras.layers.LSTM(128, activation='relu', return_sequences=True)(inputs)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "# x = keras.layers.LSTM(64, activation='relu')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_lstm_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to predict the test data and rescale it back to the original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best_lstm_model.h5')\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "# Scale the data back to original values\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test_rescaled,\n",
    "    'y_pred': y_pred_rescaled\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there is the forecast visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='LSTM Model: Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can very easily test the CNN model as well\n",
    "# TODO: Add the `dilation_rate` parameter description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu')(inputs)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Conv1D(filters=128, kernel_size=3, dilation_rate=2, activation='relu')(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "# Scale the data back to original values\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test_rescaled,\n",
    "    'y_pred': y_pred_rescaled\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='CNN Model with Dilated Convolution: Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can move to using other exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sin and cos features for time components\n",
    "def create_time_features(df):\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)\n",
    "    df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n",
    "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Day_of_week_sin'] = np.sin(2 * np.pi * df['Day_of_week'] / 7)\n",
    "    df['Day_of_week_cos'] = np.cos(2 * np.pi * df['Day_of_week'] / 7)\n",
    "    return df\n",
    "\n",
    "df = create_time_features(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used\n",
    "features = ['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos', \n",
    "           'Day_of_week_sin', 'Day_of_week_cos', 'Holiday', 'Consumption', \n",
    "           'Temperature', 'Pressure', 'Humidity']\n",
    "\n",
    "# Create sequences for forecasting 24 hours ahead\n",
    "def create_sequences(data, raw_data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(raw_data[i + n_steps, features.index('Consumption')])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(df[features][df['TestSet'] == 0])\n",
    "test_data_scaled = scaler.transform(df[features][df['TestSet'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM\n",
    "n_steps = 168 \n",
    "X_train, y_train = create_sequences(train_data_scaled, df[features][df['TestSet'] == 0].values, n_steps)\n",
    "X_test, y_test = create_sequences(test_data_scaled, df[features][df['TestSet'] == 1].values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the output data\n",
    "output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = output_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = output_scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = keras.layers.LSTM(128, activation='relu', return_sequences=True)(inputs)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "# x = keras.layers.LSTM(64, activation='relu')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_lstm_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to predict the test data and rescale it back to the original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best_lstm_model.h5')\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "# Scale the data back to original values\n",
    "y_pred_rescaled = output_scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "# y_test_rescaled = output_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'y_pred': y_pred_rescaled\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there is the forecast visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='LSTM Model: Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to form the next 24 hours forecast input and output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used\n",
    "features = ['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos', \n",
    "           'Day_of_week_sin', 'Day_of_week_cos', 'Holiday', 'Consumption', \n",
    "           'Temperature', 'Pressure', 'Humidity']\n",
    "\n",
    "# Create sequences for forecasting 24 hours ahead\n",
    "def create_sequences(data, raw_data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - 24 + 1):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(raw_data[i + n_steps:i + n_steps + 24, features.index('Consumption')])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sin and cos features for time components\n",
    "def create_time_features(df):\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)\n",
    "    df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n",
    "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Day_of_week_sin'] = np.sin(2 * np.pi * df['Day_of_week'] / 7)\n",
    "    df['Day_of_week_cos'] = np.cos(2 * np.pi * df['Day_of_week'] / 7)\n",
    "    return df\n",
    "\n",
    "df = create_time_features(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(df[features][df['TestSet'] == 0])\n",
    "test_data_scaled = scaler.transform(df[features][df['TestSet'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM\n",
    "n_steps = 168 \n",
    "X_train, y_train = create_sequences(train_data_scaled, df[features][df['TestSet'] == 0].values, n_steps)\n",
    "X_test, y_test = create_sequences(test_data_scaled, df[features][df['TestSet'] == 1].values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the output data\n",
    "output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = output_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1, 24)\n",
    "y_test_scaled = output_scaler.transform(y_test.reshape(-1, 1)).reshape(-1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model for 24-hour forecasting\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = keras.layers.LSTM(128, activation='relu', return_sequences=True)(inputs)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "# x = keras.layers.LSTM(64, activation='relu')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = Dense(24, activation='linear')(x)  # Output 24 values\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_forecast_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse the scaling\n",
    "y_pred_inv = output_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inv = output_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midnight_indices = [i for i in range(len(y_test)) if df[df.TestSet == 1].iloc[i + n_steps].Hour == 0]\n",
    "y_true_midnight = y_test[midnight_indices].ravel()\n",
    "y_pred_midnight = y_pred_inv[midnight_indices].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_true_midnight,\n",
    "    'y_pred': y_pred_midnight\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can repeat the same process for the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model for 24-hour forecasting\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu')(inputs)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Conv1D(filters=128, kernel_size=3, dilation_rate=2, activation='relu')(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(24, activation='linear')(x)  # Output 24 values\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_forecast_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse the scaling\n",
    "y_pred_inv = output_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inv = output_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midnight_indices = [i for i in range(len(y_test)) if df[df.TestSet == 1].iloc[i + n_steps].Hour == 0]\n",
    "y_true_midnight = y_test[midnight_indices].ravel()\n",
    "y_pred_midnight = y_pred_inv[midnight_indices].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_true_midnight,\n",
    "    'y_pred': y_pred_midnight\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used\n",
    "features = ['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos', \n",
    "           'Day_of_week_sin', 'Day_of_week_cos', 'Holiday', 'Consumption', \n",
    "           'Temperature', 'Pressure', 'Humidity']\n",
    "\n",
    "# Create sequences for forecasting 24 hours ahead\n",
    "def create_sequences(data, raw_data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(raw_data[i + n_steps, features.index('Consumption')])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(df[features][df['TestSet'] == 0])\n",
    "test_data_scaled = scaler.transform(df[features][df['TestSet'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM\n",
    "n_steps = 168 \n",
    "X_train, y_train = create_sequences(train_data_scaled, df[features][df['TestSet'] == 0].values, n_steps)\n",
    "X_test, y_test = create_sequences(test_data_scaled, df[features][df['TestSet'] == 1].values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the output data\n",
    "output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = output_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = output_scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Multi-head self attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    attention_output = layers.Dropout(dropout)(attention_output)\n",
    "    x = layers.Add()([inputs, attention_output])\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    ff = layers.Dense(ff_dim, activation=\"relu\")(ff)\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    return layers.Add()([x, ff])\n",
    "\n",
    "# Positional encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.position = position\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.cast(tf.range(position)[:, tf.newaxis], tf.float32),\n",
    "            i=tf.cast(tf.range(d_model)[tf.newaxis, :], tf.float32),\n",
    "            d_model=d_model\n",
    "        )\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # Apply cos to odd indices\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        \n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions from your data\n",
    "seq_length = X_train.shape[1]  # time steps\n",
    "feature_dim = X_train.shape[2]  # number of features\n",
    "\n",
    "# Define hyperparameters\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 4 * head_size\n",
    "dropout_rate = 0.1\n",
    "num_transformer_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(seq_length, feature_dim))\n",
    "\n",
    "# Add positional encoding\n",
    "x = PositionalEncoding(position=seq_length, d_model=feature_dim)(inputs)\n",
    "\n",
    "# Transformer blocks\n",
    "for _ in range(num_transformer_blocks):\n",
    "    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "# Global average pooling\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Final prediction layer\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_forecast_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best_forecast_model.h5')\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "# Scale the data back to original values\n",
    "y_pred_rescaled = output_scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "# y_test_rescaled = output_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'y_pred': y_pred_rescaled\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there is the forecast visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='LSTM Model: Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…  Tasks for the lecture (2p)\n",
    "\n",
    "* Try to modify any of the proposed models or create a new one from scratch to get a lower forecasting error **(1p)**\n",
    "    * Compare the new model with the original one. Did the MAE, MSE etc changed? If it did, how (is it better or worse, do you have any idea why)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
