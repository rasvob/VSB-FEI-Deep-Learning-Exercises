{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 11\n",
    "\n",
    "This exercise focuses on time series forecasting using deep learning techniques. We will apply these methods to predict natural gas consumption, building upon the pre-processed dataset from previous exercises.\n",
    "\n",
    "**Core Concepts**\n",
    "* ðŸ“ˆ Time series forecasting with deep learning\n",
    "* â›½ Natural gas consumption prediction\n",
    "* ðŸ“Š Utilizing pre-processed time series datasets\n",
    "* ðŸ§  Implementing deep learning models for time series data\n",
    "* ðŸ› ï¸ Practical application of deep learning to real-world forecasting problems\n",
    "\n",
    "The raw dataset is available at [vsb.ai](https://vsb.ai/natural-gas-forecasting), and we will be using a pre-processed version for this exercise.\n",
    "\n",
    "![meme1](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/df_11_meme_02.jpg?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_11.ipynb)\n",
    "\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_11.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_keras_session(seed=42):\n",
    "    \"\"\"\n",
    "    Resets the Keras session and sets the random seeds for TensorFlow, NumPy, and Python.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed to use for random number generation.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()  # Clear the previous session.\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    #Optional, but recommended for full reproducibility on some systems.\n",
    "    #This also helps with GPU determinism.\n",
    "    tf.config.experimental.reset_memory_stats('GPU:0') #if you have gpu.\n",
    "\n",
    "    #Further optional, but can help with GPU determinism.\n",
    "    # tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    # If using CUDA, also set the following environment variable:\n",
    "    # import os\n",
    "    # os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Example usage:\n",
    "reset_keras_session(seed=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have prepared common metrics for model evaluation beforehand. These functions will be used later in the notebook to assess the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes MAPE\n",
    "\"\"\"\n",
    "def mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes SMAPE\n",
    "\"\"\"\n",
    "def symetric_mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred))/2.0))) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes MAE, MSE, MAPE, SMAPE, R2\n",
    "\"\"\"\n",
    "def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    y_true, y_pred = df['y_true'].values, df['y_pred'].values\n",
    "    return compute_metrics_raw(y_true, y_pred)\n",
    "\n",
    "def compute_metrics_raw(y_true: pd.Series, y_pred: pd.Series) -> pd.DataFrame:\n",
    "    mae, mse, mape, smape, r2 = mean_absolute_error(y_true=y_true, y_pred=y_pred), mean_squared_error(y_true=y_true, y_pred=y_pred), mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), symetric_mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), r2_score(y_true=y_true, y_pred=y_pred)\n",
    "    return pd.DataFrame.from_records([{'MAE': mae, 'MSE': mse, 'MAPE': mape, 'SMAPE': smape, 'R2': r2}], index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/raw/main/datasets/ppnet_metar_v8.csv', sep=';', index_col=0)\n",
    "df = df[df.Year < 2019].copy()\n",
    "df.loc[:, 'TestSet'] = 0\n",
    "df.loc[df.Year == 2018, 'TestSet'] = 1\n",
    "df = df.loc[:, ['Year', 'Month', 'Day', 'Hour', 'Day_of_week', 'Holiday', 'Consumption', 'Temperature', 'Pressure', 'Humidity', 'TestSet']]\n",
    "df.index = pd.DatetimeIndex(df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Dataset Description\n",
    "\n",
    "This comprehensive dataset spans six complete years (January 1, 2013 to December 31, 2018), with all features recorded at hourly intervals, yielding 52,584 data points. The dataset integrates three essential components:\n",
    "\n",
    "## ðŸ­ Consumption Data\n",
    "\n",
    "The natural gas consumption data comes from Prague, the capital of the Czech Republic, with a distribution network serving 422,926 customers in 2018 and total consumption of 3.82 billion mÂ³. The consumption is distributed across:\n",
    "\n",
    "- **ðŸ˜ï¸ Residential sector**: 381,914 households (33.3% of consumption)\n",
    "- **ðŸ¢ Industrial sector**: \n",
    "  - 177 large customers (24.8%)\n",
    "  - 39,175 medium customers (18.9%)\n",
    "  - 1,652 small customers (21.9%)\n",
    "\n",
    "The remaining percentage represents operational losses during distribution (e.g., pipeline leaks).\n",
    "\n",
    "**ðŸ”¥ Heating Season Information**:\n",
    "- Period: September 1 to May 31\n",
    "- Activation criteria: Temperature below +13Â°C for two consecutive days with no forecast warming\n",
    "- Significance: Accounts for approximately 70-75% of annual consumption\n",
    "\n",
    "## ðŸŒ¡ï¸ Weather Variables\n",
    "\n",
    "Weather data was collected from the Prague LKPR airport weather station through METAR (aerodrome routine meteorological report) information. These standardized reports are regularly issued by airports and maintained in long-term archives, providing reliable meteorological data for our analysis.\n",
    "\n",
    "## ðŸ’° Economic Features\n",
    "\n",
    "The economic component consists of natural gas price data obtained from the Czech energy regulation office. This data provides crucial context for understanding consumption patterns in relation to market conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endogenous and Exogenous Variables in Forecasting Models\n",
    "\n",
    "In time series forecasting, understanding the distinction between endogenous and exogenous variables is fundamental for building effective models. This classification helps determine the structure of your forecasting approach and influences how you interpret relationships between variables.\n",
    "\n",
    "## Variable Classification\n",
    "\n",
    "### Endogenous Variable\n",
    "- **Consumption** ðŸ“Š: This is your primary target variable being forecasted within the model system. It's considered endogenous because its values are determined within the model and influenced by other variables in the system as well as its own past values.\n",
    "\n",
    "### Exogenous Variables\n",
    "These variables are determined outside your forecasting model and serve as inputs:\n",
    "\n",
    "- **Weather Variables** ðŸŒ¡ï¸: Data from the Prague LKPR airport weather station including temperature, humidity, precipitation, wind speed, and other meteorological measurements. These factors significantly influence energy consumption patterns but are not affected by consumption itself.\n",
    "\n",
    "- **Economic Indicators** ðŸ’¹: Natural gas price data from the Czech energy regulation office, which affects consumption behavior but is determined by broader market forces.\n",
    "\n",
    "- **Temporal Features** ðŸ“…: Time-related variables such as hour of day, day of week, month, and seasonal indicators derived from timestamps. These cyclical patterns help capture regular consumption behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have ~ 52k datapoints which should be sufficient even for very complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(y=df['Consumption'], x=df.index, color=df.Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(y=df['Temperature'], x=df.index, color=df.Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will start with simple next step prediction using LSTM network\n",
    "* We will limit the input to consumption variable first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Sequence Formation & Feature Scaling\n",
    "\n",
    "### ðŸ“Š Data Windowing Strategy (168-hour Lookback)\n",
    "We'll structure our data using a sliding window approach to create input-output pairs suitable for LSTM networks:\n",
    "\n",
    "```python\n",
    "sequence_length = 168  # 24*7 = weekly cycle\n",
    "```\n",
    "\n",
    "- ðŸ•° **168-hour Window**: Captures weekly patterns (24 hours Ã— 7 days)\n",
    "- âž¡ï¸ Each input sequence contains 168 consecutive hourly measurements\n",
    "- ðŸŽ¯ Corresponding output is the next immediate value (t+1)\n",
    "- ðŸ”„ Creates temporal relationship: X[t-168:t] â†’ y[t+1]\n",
    "\n",
    "### âš–ï¸ MinMax Scaling Essentials\n",
    "Data normalization is crucial for neural network performance:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "```\n",
    "\n",
    "- ðŸ“‰ **Scales values** between 0 and 1: x' = (x - min)/(max - min)\n",
    "- ðŸ§  **Benefits**:\n",
    "  - Accelerates model convergence\n",
    "  - Prevents large value dominance in weights\n",
    "  - Improves numerical stability\n",
    "  \n",
    "### ðŸ›  Implementation Considerations\n",
    "1. **Temporal Order Preservation** ðŸ•°ï¸  \n",
    "   Maintain chronological order when splitting sequences\n",
    "   \n",
    "2. **Train-Test Separation** ðŸ”  \n",
    "   Fit scaler **only** on training data to prevent information leakage\n",
    "\n",
    "3. **3D Reshaping** ðŸ“¦  \n",
    "   LSTM requires input shape: (samples, timesteps, features)  \n",
    "   Final shape: (num_sequences, 168, 1)\n",
    "\n",
    "**ðŸ’¡ Pro Tip:** The 168-hour window acts as a hyperparameter. You might experiment with different lengths (24h, 72h, 336h) depending on your specific dataset's characteristics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, n_steps):\n",
    "    \"\"\"\n",
    "    Create sequences of data for LSTM input\n",
    "    data: input array\n",
    "    n_steps: number of time steps (lags)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(data[i + n_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Get the consumption data and convert to numpy array\n",
    "consumption = df['Consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Define the number of lags (time steps)\n",
    "n_steps = 168  # Use 168 hours of data to predict the next hour\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data = consumption[df.TestSet == 0]\n",
    "test_data = consumption[df.TestSet == 1]\n",
    "\n",
    "# Scale the input data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "X_train, y_train = create_sequences(train_data_scaled, n_steps)\n",
    "X_test, y_test = create_sequences(test_data_scaled, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for np.nan or np.inf in the data\n",
    "* If there are any, the model will fail to converge - you will get ``NaNs`` in the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_train)), np.any(np.isnan(y_train)), np.any(np.isnan(X_test)), np.any(np.isnan(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = keras.layers.LSTM(128, activation='relu', return_sequences=True)(inputs)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "# x = keras.layers.LSTM(64, activation='relu')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Prediction and Rescaling Process\n",
    "\n",
    "After training our LSTM model, we need to generate predictions and convert them back to their original scale:\n",
    "\n",
    "### ðŸ“ˆ Key Steps\n",
    "\n",
    "- **Prediction**: Feed test sequences through trained model\n",
    "- **Rescaling**: Apply `inverse_transform()` to convert normalized values back to original range\n",
    "- **Reshape if needed**: Ensure dimensions match scaler's expected input format\n",
    "\n",
    "This process restores our predictions to the original consumption units (e.g., kWh), making them interpretable and ready for evaluation against actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best.weights.h5')\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "# Scale the data back to original values\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test_rescaled,\n",
    "    'y_pred': y_pred_rescaled\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Our Forecast ðŸ“Š\n",
    "\n",
    "The visualization of our time series forecast provides a clear picture of how our model performs against the actual data. This graphical representation allows us to:\n",
    "\n",
    "- Evaluate the **out-of-sample performance** in the test period\n",
    "- Identify any **patterns or anomalies** that might not be obvious from numerical metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='LSTM Model: Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the CNN Model ðŸ§ª\n",
    "\n",
    "The Convolutional Neural Network (CNN) model can be easily tested using TensorFlow's Keras API. Let's focus on the `Conv1D` layer, which is a fundamental building block of CNNs for image processing tasks.\n",
    "\n",
    "### Key Parameters of Conv1D\n",
    "\n",
    "The `Conv1D` layer in TensorFlow offers several important parameters:\n",
    "\n",
    "- **filters**: The number of output channels (convolution kernels) in the layer\n",
    "- **kernel_size**: The height and width of the 2D convolution window\n",
    "- **strides**: The step size of the convolution along each dimension\n",
    "- **padding**: The padding strategy, either 'valid' or 'same'\n",
    "- **activation**: The activation function to use after the convolution\n",
    "\n",
    "### Dilation Rate Parameter\n",
    "\n",
    "The `dilation_rate` parameter in `Conv1D` is an important feature that allows for:\n",
    "\n",
    "- Expanding the receptive field without increasing the number of parameters\n",
    "- Capturing wider context in the input features\n",
    "\n",
    "It works by inserting spaces between the kernel elements, effectively enlarging the kernel's field of view without increasing its parameter count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu')(inputs)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Conv1D(filters=128, kernel_size=3, dilation_rate=2, activation='relu')(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best.weights.h5')\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "# Scale the data back to original values\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test_rescaled,\n",
    "    'y_pred': y_pred_rescaled\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='CNN Model with Dilated Convolution: Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Exogenous Variables ðŸŒŸ\n",
    "\n",
    "Exogenous variables are external factors that can influence the target variable but are not part of the time series itself. By including these variables in our model, we aim to improve its predictive power by capturing additional information that affects the system.\n",
    "\n",
    "### Why Use Exogenous Variables?\n",
    "\n",
    "- **Enhanced Forecast Accuracy**: Exogenous variables often provide context or drivers for the target variable, leading to more accurate predictions.\n",
    "- **Capturing External Influences**: These variables account for factors like economic indicators, weather conditions, or other domain-specific data.\n",
    "- **Model Generalization**: Including exogenous inputs can make the model more robust to changes in external conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sin and cos features for time components\n",
    "def create_time_features(df):\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)\n",
    "    df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n",
    "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Day_of_week_sin'] = np.sin(2 * np.pi * df['Day_of_week'] / 7)\n",
    "    df['Day_of_week_cos'] = np.cos(2 * np.pi * df['Day_of_week'] / 7)\n",
    "    return df\n",
    "\n",
    "df = create_time_features(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used\n",
    "features = ['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos', \n",
    "           'Day_of_week_sin', 'Day_of_week_cos', 'Holiday', 'Consumption', \n",
    "           'Temperature', 'Pressure', 'Humidity']\n",
    "\n",
    "# Create sequences for forecasting 24 hours ahead\n",
    "def create_sequences(data, raw_data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(raw_data[i + n_steps, features.index('Consumption')])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(df[features][df['TestSet'] == 0])\n",
    "test_data_scaled = scaler.transform(df[features][df['TestSet'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM\n",
    "n_steps = 168 \n",
    "X_train, y_train = create_sequences(train_data_scaled, df[features][df['TestSet'] == 0].values, n_steps)\n",
    "X_test, y_test = create_sequences(test_data_scaled, df[features][df['TestSet'] == 1].values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the output data\n",
    "output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = output_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = output_scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = keras.layers.LSTM(128, activation='relu', return_sequences=True)(inputs)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "# x = keras.layers.LSTM(64, activation='relu')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to predict the test data and rescale it back to the original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best.weights.h5')\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "# Scale the data back to original values\n",
    "y_pred_rescaled = output_scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "# y_test_rescaled = output_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'y_pred': y_pred_rescaled\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there is the forecast visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='LSTM Model: Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building 24-Hour Forecast Sequences â³ðŸ”®\n",
    "\n",
    "Creating proper input-output sequences is crucial for training time series models. Here's how to structure them:\n",
    "\n",
    "### Input Sequence (X)\n",
    "- Contains **historical data** used to predict the future\n",
    "- Typically includes:\n",
    "  - Past values of the target variable (lag features)\n",
    "  - Exogenous variables (weather, prices, events)\n",
    "  - Time-based features (hour of day, day of week)\n",
    "- Example structure:  \n",
    "  `[t-23h, t-22h,..., t-1h]` â†’ predicts `[t+1h,..., t+24h]`\n",
    "\n",
    "### Output Sequence (y)\n",
    "- The **24-hour forecast horizon** we want to predict\n",
    "- Should align temporally with the input sequence\n",
    "- Contains only the target variable's future values\n",
    "\n",
    "### Key Considerations\n",
    "1. **Time Alignment** âš–ï¸  \n",
    "   Ensure each input window perfectly precedes its output window without overlaps\n",
    "\n",
    "2. **Sequence Length** ðŸ“  \n",
    "   Choose appropriate look-back period (e.g., 24-72 hours) based on data patterns\n",
    "\n",
    "3. **Feature Scaling** ðŸ”„  \n",
    "   Normalize/standardize all features to similar numerical ranges\n",
    "\n",
    "4. **Data Windowing** ðŸªŸ  \n",
    "   Use sliding window technique to create sequential samples while preserving time order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used\n",
    "features = ['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos', \n",
    "           'Day_of_week_sin', 'Day_of_week_cos', 'Holiday', 'Consumption', \n",
    "           'Temperature', 'Pressure', 'Humidity']\n",
    "\n",
    "# Create sequences for forecasting 24 hours ahead\n",
    "def create_sequences(data, raw_data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - 24 + 1):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(raw_data[i + n_steps:i + n_steps + 24, features.index('Consumption')])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sin and cos features for time components\n",
    "def create_time_features(df):\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)\n",
    "    df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n",
    "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Day_of_week_sin'] = np.sin(2 * np.pi * df['Day_of_week'] / 7)\n",
    "    df['Day_of_week_cos'] = np.cos(2 * np.pi * df['Day_of_week'] / 7)\n",
    "    return df\n",
    "\n",
    "df = create_time_features(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(df[features][df['TestSet'] == 0])\n",
    "test_data_scaled = scaler.transform(df[features][df['TestSet'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM\n",
    "n_steps = 168 \n",
    "X_train, y_train = create_sequences(train_data_scaled, df[features][df['TestSet'] == 0].values, n_steps)\n",
    "X_test, y_test = create_sequences(test_data_scaled, df[features][df['TestSet'] == 1].values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the output data\n",
    "output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = output_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1, 24)\n",
    "y_test_scaled = output_scaler.transform(y_test.reshape(-1, 1)).reshape(-1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model for 24-hour forecasting\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = keras.layers.LSTM(128, activation='relu', return_sequences=True)(inputs)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "# x = keras.layers.LSTM(64, activation='relu')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = Dense(24, activation='linear')(x)  # Output 24 values\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best.weights.h5')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse the scaling\n",
    "y_pred_inv = output_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inv = output_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midnight_indices = [i for i in range(len(y_test)) if df[df.TestSet == 1].iloc[i + n_steps].Hour == 0]\n",
    "y_true_midnight = y_test[midnight_indices].ravel()\n",
    "y_pred_midnight = y_pred_inv[midnight_indices].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_true_midnight,\n",
    "    'y_pred': y_pred_midnight\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š We can repeat the same process for the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model for 24-hour forecasting\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu')(inputs)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Conv1D(filters=128, kernel_size=3, dilation_rate=2, activation='relu')(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(24, activation='linear')(x)  # Output 24 values\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamW', loss='mse', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best.weights.h5')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse the scaling\n",
    "y_pred_inv = output_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inv = output_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midnight_indices = [i for i in range(len(y_test)) if df[df.TestSet == 1].iloc[i + n_steps].Hour == 0]\n",
    "y_true_midnight = y_test[midnight_indices].ravel()\n",
    "y_pred_midnight = y_pred_inv[midnight_indices].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_true_midnight,\n",
    "    'y_pred': y_pred_midnight\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a detailed yet simple explanation of the Transformer implementation using relatable examples:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ How the Transformer Works (Step-by-Step)\n",
    "\n",
    "### 1. **Transformer Block: Like a Team of Time Detectives** ðŸ•µï¸â™‚ï¸ðŸ•µï¸â™€ï¸  \n",
    "*Imagine 3 experts analyzing a week's weather data to predict tomorrow's temperature:*\n",
    "\n",
    "```python\n",
    "# Simplified analogy:\n",
    "def transformer_encoder(inputs):\n",
    "    1. Normalize data â†’ \"Standardize measurements between -1 and 1\"\n",
    "    2. MultiHeadAttention â†’ \"Experts compare Tuesday vs Friday humidity patterns\"\n",
    "    3. Combine insights â†’ \"They agree rain on Monday affects Friday's pressure\"\n",
    "    4. Feed-Forward â†’ \"Convert findings to temperature predictions\"\n",
    "    5. Final Combine â†’ \"Merge new insights with original observations\"\n",
    "```\n",
    "\n",
    "#### Key Components:\n",
    "- **Layer Normalization** ðŸ“  \n",
    "  *Example:* Scales all temperature readings to comparable ranges (-1 to 1) before analysis  \n",
    "  *Why?* Prevents Monday's 30Â°C from dominating Friday's 15Â°C in calculations  \n",
    "\n",
    "- **Multi-Head Attention** ðŸ‘¥  \n",
    "  *Analogy:* 4 specialists (num_heads=4) each study different relationships:\n",
    "  - Head 1: Hourly temperature changes\n",
    "  - Head 2: Weekly season patterns\n",
    "  - Head 3: Weather â†” electricity demand links\n",
    "  - Head 4: Holiday effect correlations  \n",
    "\n",
    "- **Feed-Forward Network** ðŸ§   \n",
    "  *Example:* Takes detected patterns and transforms them through:  \n",
    "  `Dense Layer 1 (FFN)`: \"If humidity > 70% and temp < 5Â°C â†’ snow likely\"  \n",
    "  `Dense Layer 2`: \"Convert snow prediction to energy demand estimate\"  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Positional Encoding: The Time Calendar** ðŸ“…â°  \n",
    "*Why needed?* Transformers don't automatically understand sequence order.  \n",
    "\n",
    "#### How It Works:\n",
    "```python\n",
    "# For a 24-hour time series:\n",
    "Position 0 (Midnight): [sin(0/10000â°), cos(0/10000Â¹), sin(0/10000Â²), ...]\n",
    "Position 23 (11 PM): [sin(23/10000â°), cos(23/10000Â¹), sin(23/10000Â²), ...]\n",
    "```\n",
    "\n",
    "*Real-world analogy:*  \n",
    "- Each position (hour) gets unique \"time ID\" using alternating sine/cosine waves  \n",
    "- Even positions use sine â†’ \"How unique is this hour?\"  \n",
    "- Odd positions use cosine â†’ \"Where does this hour fall in daily cycle?\"  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Complete Workflow Example** ðŸ› ï¸  \n",
    "*Predicting tomorrow's temperature using 7-day history:*\n",
    "\n",
    "1. **Input Shape**: (7 days Ã— 24 hours, 3 features) â†’ [168 timesteps, temp/humidity/pressure]  \n",
    "2. **Add Positions**:  \n",
    "   ```python\n",
    "   # Encodes that hour 12 is midday, 0 is midnight\n",
    "   encoded_data = PositionalEncoding(position=168, d_model=3)(input_data)\n",
    "   ```\n",
    "3. **First Transformer Block**:  \n",
    "   - 4 attention heads find:  \n",
    "     - Cold nights â†’ frost next morning  \n",
    "     - High humidity â†’ afternoon rain likelihood  \n",
    "4. **Second Transformer Block**:  \n",
    "   - Combines patterns:  \n",
    "     \"Frost + morning sun â†’ rapid temperature rise\"  \n",
    "5. **Final Prediction**:  \n",
    "   Output Layer converts learned patterns to 24 temperature values  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Why This Works for Time Series?\n",
    "\n",
    "| Component          | Time Series Benefit                          | Real-World Example                     |\n",
    "|--------------------|----------------------------------------------|----------------------------------------|\n",
    "| Multi-Head Attention | Finds complex time relationships            | \"Heatwave 3 days ago â†’ current demand spike\" |\n",
    "| Positional Encoding | Preserves timing context                    | \"2 AM vs 2 PM treated differently\"     |\n",
    "| Residual Connections | Maintains original data patterns           | Keeps raw temperature data accessible  |\n",
    "| LayerNorm           | Stabilizes learning across varying scales   | Handles Â°C and hPa pressure together   |\n",
    "\n",
    "**Simple Tuning Guide** ðŸ”§:  \n",
    "- `num_heads=4`: Good for basic daily patterns  \n",
    "- `ff_dim=64`: Handles 8-10 features well  \n",
    "- `dropout=0.1`: Prevents overfitting for noisy weather data  \n",
    "\n",
    "This architecture learns both immediate (\"last hour's rain\") and long-term (\"summer vs winter\") patterns simultaneously! ðŸŒ¦ï¸ðŸ”®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used\n",
    "features = ['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos', \n",
    "           'Day_of_week_sin', 'Day_of_week_cos', 'Holiday', 'Consumption', \n",
    "           'Temperature', 'Pressure', 'Humidity']\n",
    "\n",
    "# Create sequences for forecasting 24 hours ahead\n",
    "def create_sequences(data, raw_data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(raw_data[i + n_steps, features.index('Consumption')])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(df[features][df['TestSet'] == 0])\n",
    "test_data_scaled = scaler.transform(df[features][df['TestSet'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM\n",
    "n_steps = 168 \n",
    "X_train, y_train = create_sequences(train_data_scaled, df[features][df['TestSet'] == 0].values, n_steps)\n",
    "X_test, y_test = create_sequences(test_data_scaled, df[features][df['TestSet'] == 1].values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the output data\n",
    "output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = output_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = output_scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme1](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/df_11_meme_01.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Multi-head self attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    attention_output = layers.Dropout(dropout)(attention_output)\n",
    "    x = layers.Add()([inputs, attention_output])\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    ff = layers.Dense(ff_dim, activation=\"relu\")(ff)\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    return layers.Add()([x, ff])\n",
    "\n",
    "# Positional encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.position = position\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.cast(tf.range(position)[:, tf.newaxis], tf.float32),\n",
    "            i=tf.cast(tf.range(d_model)[tf.newaxis, :], tf.float32),\n",
    "            d_model=d_model\n",
    "        )\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # Apply cos to odd indices\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        \n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions from your data\n",
    "seq_length = X_train.shape[1]  # time steps\n",
    "feature_dim = X_train.shape[2]  # number of features\n",
    "\n",
    "# Define hyperparameters\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 4 * head_size\n",
    "dropout_rate = 0.1\n",
    "num_transformer_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(seq_length, feature_dim))\n",
    "\n",
    "# Add positional encoding\n",
    "x = PositionalEncoding(position=seq_length, d_model=feature_dim)(inputs)\n",
    "\n",
    "# Transformer blocks\n",
    "for _ in range(num_transformer_blocks):\n",
    "    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "# Global average pooling\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Final prediction layer\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_weights('best.weights.h5')\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "# Scale the data back to original values\n",
    "y_pred_rescaled = output_scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "# y_test_rescaled = output_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for evaluation\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'y_pred': y_pred_rescaled\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = compute_metrics(results_df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there is the forecast visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "fig = px.line(\n",
    "    results_df,\n",
    "    title='LSTM Model: Actual vs Predicted Consumption'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Consumption')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…  Tasks for the lecture (2p)\n",
    "\n",
    "* ðŸ› ï¸ Modify any of the proposed models or create a new one from scratch to get a lower forecasting error\n",
    "    * ðŸ“¶ Compare the new model with the original one. \n",
    "    * ðŸ”„Did the MAE, MSE etc changed? \n",
    "        * ðŸ’¡ If it did, how (is it better or worse, do you have any idea why)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
