{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e99064-36ef-4c7d-9591-cc93ef206e2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Deep Learning - Exercise 12\n",
    "\n",
    "The aim of the lecture is to get an overview of possibilities in the generative artificial intelligence (GenAI) domain\n",
    "\n",
    "## ðŸ”Ž Do you know any famous models from this area?\n",
    "\n",
    "* We will use [Huggingface](https://huggingface.co/) library\n",
    "\n",
    "![meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/ai_meme_02.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6799b8-1842-42a2-9bea-a1a131a7f1cd",
   "metadata": {},
   "source": [
    "## âš¡ Let's install the basic libraries first\n",
    "\n",
    "* We will use HuggingFace library for the **Stable Diffusion** model\n",
    "    * https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n",
    "    * ðŸ”Ž What is a **Text-to-Image** task?\n",
    "\n",
    "* You can download pre-trained models from the Hub and use them leveraging simple unified API\n",
    "    * But I get that you already know this ðŸ™‚\n",
    "\n",
    "* The main one is `diffusers`\n",
    "    * The `diffusers` library, developed by Hugging Face, is designed for running, training, and deploying diffusion models\n",
    "    * ðŸ“Œ They work by gradually denoising a signal from a random distribution to generate data that resembles the distribution of a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9d18c-466f-417d-89ea-aaabd1d7c78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers diffusers\n",
    "!pip install invisible_watermark transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff7a0f-a289-403b-8964-c1d46c47c7e8",
   "metadata": {},
   "source": [
    "## Running own Stable Diffusion instance is quite easy\n",
    "* You just need to download the pretrained model and load it into the GPU\n",
    "* There are many different models in the [HuggingFace Models Hub](https://huggingface.co/models)\n",
    "    * ðŸ’¡ Filter by task, e.g. Text-to-Image\n",
    "\n",
    "* We can see that the `DiffusionPipeline.from_pretrained` has several parameters set\n",
    "    * `torch_dtype`: This parameter specifies the data type for the tensors used in the model. `torch.float16` is used here to indicate that the model should use 16-bit floating-point numbers\n",
    "        * This is often done to reduce memory usage and potentially speed up computations, at the cost of some precision\n",
    "    * `use_safetensors`: SafeTensors are a feature designed to ensure that tensor operations are performed in a way that minimizes the risk of out-of-memory errors and other issues related to tensor management\n",
    "    * `variant`: This parameter allows you to specify a variant of the model to use. In this case, `fp16` is specified, which likely indicates that the model variant optimized for 16-bit floating-point operations should be used. \n",
    "        * ðŸ’¡ This is consistent with the choice of `torch.float16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7df121-adb8-4aac-8e61-6d7af55b25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "## You can specify the device to use\n",
    "# pipe.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7730fba-19f8-4640-8c03-42728217c68f",
   "metadata": {},
   "source": [
    "## Now the model is ready and you can start to use it\n",
    "* ðŸ’¡ The most important part is so-called **prompt** definition - the same concept as in the ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b9fed-8979-404f-a639-7f287bcb4d73",
   "metadata": {},
   "source": [
    "# Let's create our own image using the model\n",
    "\n",
    "#### ðŸ’¡ TIP: Run the code multiple times if you do not like the result ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f2cb6-9fe3-46ef-becb-bf61767d99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A knight riding a majestic lion\"\n",
    "\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c383046-c2cf-4fa8-bea8-55dc37cd95cf",
   "metadata": {},
   "source": [
    "## ðŸ“’ The image can be very easily saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d11b6a-7e88-4cc2-8cb8-d1f5b881486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save('sd_output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4ea7c-6e4d-42cb-9627-0fbe519ad7a4",
   "metadata": {},
   "source": [
    "# ðŸ“Œ The most difficult part is to define the prompt\n",
    "* There are several *tips&tricks* how to get maximum out of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354c468-e58a-4f5a-ab4f-380b427154b4",
   "metadata": {},
   "source": [
    "## You can add keywords after the main prompt delimited by commas to be more specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283ae84-a8f7-4d12-9430-01cacdb614ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A knight riding a majestic lion, cyberpunk, japan city background\"\n",
    "\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204a979-8ab7-4026-bfb9-01090dd110e0",
   "metadata": {},
   "source": [
    "## You can put an emphasis on a keyword by adding `[]` - e.g. `[cyberpunk]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52a009-e8b6-4547-8c46-5fa3676c2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A knight riding a majestic lion, cyberpunk, japan city background, [black and white]\"\n",
    "\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e32b6-9415-435c-bada-d8ac288dc6fd",
   "metadata": {},
   "source": [
    "# Funny use-case is to use some specific painter style\n",
    "* ðŸ’¡ You can see results of multiple styles at https://www.urania.ai/top-sd-artists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef345a05-ee87-452c-b41f-e28e227cf85b",
   "metadata": {},
   "source": [
    "## Leonid Afremov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d405b57-79be-41a7-993a-a7df9d1bebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A knight riding a majestic lion, [Leonid Afremov]\"\n",
    "\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8ac00-c9ab-407a-8d80-81f3a2d87a7a",
   "metadata": {},
   "source": [
    "## Vincent Van Gogh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c151b8c-88fa-4156-8569-037e24ad8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A knight riding a majestic lion, [Vincent Van Gogh], cyberpunk\"\n",
    "\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f9f1f",
   "metadata": {},
   "source": [
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/ai_meme_01.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c78532",
   "metadata": {},
   "source": [
    "## âš¡ We need to reset the session not to run OOM now\n",
    "\n",
    "## We will load the base model and refiner separately\n",
    "* ðŸ’¡ The base model is used to generate (noisy) latent vectors\n",
    "* ðŸ’¡ Refiner is specialized for the final denoising steps of the latent vector thus it will generate our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b61483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "base.to(\"cuda\")\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "refiner.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840bd96a",
   "metadata": {},
   "source": [
    "## We can now tune parameters of the models\n",
    "* The most important parameters are these two:\n",
    "    * `n_steps`\n",
    "    * `high_noise_frac`\n",
    "\n",
    "* Stable Diffusion XL base is trained on timesteps 0-999 and Stable Diffusion XL refiner is finetuned from the base model on low noise timesteps 0-199\n",
    "* We use the base model for the first 800 timesteps (high noise) and the refiner for the last 200 timesteps (low noise)\n",
    "    * ðŸ“Œ This is set byt setting `high_noise_frac` to 0.8\n",
    "\n",
    "* `n_steps` represents the number of inference steps to be used in the generative process by both the base and refiner functions\n",
    "    * ðŸ’¡ In the context of generative models each inference step typically involves a denoising operation\n",
    "    * Starting from a noisy state or latent space representation, the model iteratively refines this input through a series of steps, gradually reducing noise and adding detail\n",
    "* ðŸ“Œ The n_steps parameter controls the granularity of this processâ€”the more steps, the more gradual and potentially detailed the transformation\n",
    "\n",
    "* ðŸ’¡ We can set `high_noise_frac = 1` to obtain very low-detail unrefined image and also we can set `n_steps = 1` to get just the noise \n",
    "\n",
    "### ðŸš€ Nice experiment is to set `n_steps` parameter sequentially to 1, 3, 5, 10 and 20 to see how the noise is refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "high_noise_frac = 0.8\n",
    "prompt = \"A knight riding a majestic lion\"\n",
    "\n",
    "# run both experts\n",
    "latent_vector = base(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_end=high_noise_frac,\n",
    "    output_type=\"latent\",\n",
    ").images\n",
    "\n",
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_start=high_noise_frac,\n",
    "    image=latent_vector,\n",
    ").images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012f367",
   "metadata": {},
   "source": [
    "## ðŸ“Œ In case that we have an OoM error, here are the outputs:\n",
    "\n",
    "* n_steps = 1\n",
    "\n",
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/n_steps_1.png?raw=true)\n",
    "\n",
    "* n_steps = 3\n",
    "\n",
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/n_steps_3.png?raw=true)\n",
    "\n",
    "* n_steps = 5\n",
    "\n",
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/n_steps_5.png?raw=true)\n",
    "\n",
    "* n_steps = 10\n",
    "\n",
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/n_steps_10.png?raw=true)\n",
    "\n",
    "* n_steps = 20\n",
    "\n",
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/n_steps_20.png?raw=true)\n",
    "\n",
    "* high_noise_frac = 1.0\n",
    "\n",
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/high_noise_frac.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248ebe1-cb3c-4be5-be74-8949e8a7a106",
   "metadata": {},
   "source": [
    "### âš¡ If you are interested in this topic I recommend to visit [lexica.art](https://lexica.art/) or [https://www.reddit.com/r/StableDiffusion/](https://www.reddit.com/r/StableDiffusion/) ðŸ™‚\n",
    "* ðŸ’¡ You can try models even using web-based GUI on [HuggingFace Spaces](https://huggingface.co/spaces/stabilityai/stable-diffusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
