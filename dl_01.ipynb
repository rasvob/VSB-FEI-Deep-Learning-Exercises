{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z4lsdTXjdMG"
   },
   "source": [
    "# Deep Learning - Exercise 1\n",
    "This lecture is about basics of the Tensorflow, we will discuss the minimal example on the MNIST dataset.\n",
    "\n",
    "We also investigate a meaning of the validation sets and different complexity of the model. \n",
    "\n",
    "Moreover, we will look on the regulariozation and we will try to find optimal model for the MNIST dataset that is based on fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_meme_01.jpg?raw=true \"AI Meme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCXkXCkBjdMH"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_01.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_01.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGYmu2RCjdMH"
   },
   "source": [
    "### Import of the TensorFlow\n",
    "The main version of the TensorFlow (TF) is a in the Version package in the field VERSION\n",
    "Since the TensformFlow 2.0 everything was encapsulaed under the KERAS api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I798rQYbjdMI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWDIlb9UjdMI"
   },
   "source": [
    "### Import a dataset\n",
    "Datasets are stored in the keras.datasets submodule. Few testing datasets are stored here and installed together with the TF package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OK0bAkspjdMI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mnist is the basic dataset with handwritten digits\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# the data are in the form of 28x28 pixes with values 0-255.\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Test data shape:  ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jRqCq9jklBE"
   },
   "source": [
    "The dataset consists of 60,000 training images and 10,000 testing images. All of the are 28x28 pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACxVYmJGjdMI"
   },
   "source": [
    "### Lets look on the data how do they look like.\n",
    "Look closely on the value scale - it os from 0 to 255 as usual in grayscale  images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the numbers are already centered - you won't see this feature in the real world images often ðŸ™‚\n",
    "Some of the images are quite easy to classify but on the other hand there is a lot of noise in the data as well.\n",
    "\n",
    "Take a look at these two examples below? The 1st one is clear, but the 2nd one is 1 or 7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZaKJJf3jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[12])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZaKJJf3jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[42])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z_rfPvrjdMJ"
   },
   "source": [
    "### The conversion into range 0-1 is done by the division\n",
    "* Lets normalize the values into the range \\(0,1\\) by dividing it 255.\n",
    "\n",
    "* Why is this step helpful? (Hint: input magnitude and weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCNrX5rqjdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The conversion into range 0-1 is done by the division\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyhe6lUKjdMJ"
   },
   "source": [
    "### Make better visualization of the data to better understand how complex they are\n",
    "* You can see that \"noisy\" digit and different handwriting style is really no exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIf8LNL7jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = [str(x) for x in range(10)]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we dive in the deep learning territory, let's try to create some baseline model using a machine learning model so we can compare the approaches\n",
    "* Are ML models capable of processing image data?\n",
    "* How to deal with a matrix input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg = DecisionTreeClassifier()\n",
    "alg.fit(x_train.reshape(-1, 28*28), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = alg.predict(x_test.reshape(-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that we were able to create a classifier very easily.\n",
    "* However it is no secret, that DL models accuracy on MNIST can be >= 98% even with simple architecture\n",
    "* Given the fact, our result is not very impresive ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3uY8cfbjdMJ"
   },
   "source": [
    "## Basic model - a NN with very simple hierarchy\n",
    "* Model is created using layers, many layers exists in the [layer submodule](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "* Each layer uses a activation functions collected in the [module nn](https://www.tensorflow.org/api_docs/python/tf/nn)\n",
    " \n",
    "#### There are 2 ways of using the Keras API, Sequential and Functional, we will start with the sequential one\n",
    "\n",
    "* Why do we use activation functions?\n",
    "* How is ANN different from lin. regression?\n",
    "\n",
    "## Let's design our first ANN\n",
    "* Can you draw such network on a table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FRfWkocjdMK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),# Flatten module flatten the multidimension input into single vector 28x28 = 784 float numbers\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # standard dense-fully connected layer with the rectified lineaar function as an activation\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax), # another fully-connected layer with softmax activation function\n",
    "])\n",
    "\n",
    "model.summary() # prints the summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the meaning of the *Total params* number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxkeKMLdjdMK"
   },
   "source": [
    "### Compilation of the model\n",
    "Each model need to be compiled to be able to fit to the data and predict the labels\n",
    "\n",
    "#### Optimizers\n",
    "\n",
    "* There are many optimizers available, you can experiment with different algorithms, most of them are based on gradient descent algorithm\n",
    "* And of course gradient descent is not a limit, gradient-free methods are available as well, e.g. [Nevergrad library by META](https://github.com/facebookresearch/nevergrad)\n",
    "\n",
    "**Selected optimizers:**\n",
    "* Gradient descent\n",
    "   * Works for the whole dataset and it is not suitable for large data\n",
    "* Stochastic Gradiend Descent (SGD)\n",
    "   * Approximate the real gradiend from selested subset of data (Stochasticity)\n",
    "* Root Mean Square Propagation (RMSPRop)\n",
    "   * Adapts the learnign rate with the running average of the recent gradients.\n",
    "* Adamptive Moment Estimation (ADAM)\n",
    "   * Averages gradients and secodn moment of the gradient and adapts the learning rate.\n",
    "\n",
    "#### Loss functions\n",
    "**Training of the ANN is about weights optimization. We need to some formula that says us if the optimization process is making the ANN better or not.**\n",
    "\n",
    "**The choice of a loss function depends on tha task and network architecture. Below are the most common loss functions mentioned.**\n",
    "\n",
    "* Mean Squared Error\n",
    "   * a classical measure to be used in regression\n",
    "   * a logarithmic version exists\n",
    "* Mean Absolute Error (MAE)\n",
    "   * take an absolute values instead of their squared version\n",
    "* Binary classification Loss\n",
    "   * a loss for binary problems only\n",
    "   * predicts the probability of the class 1\n",
    "* Binary Cross-Entropy\n",
    "   * predict the class from the set {0,1}\n",
    "   * requires a sigmoind activation function\n",
    "* Categorical Cross-Entropy\n",
    "   * default for mutli-class classification problems\n",
    "   * requires the softmax function on output layer to compute probability of each layer\n",
    "   * train labels have to be one-hot-encoded\n",
    "* Sparse Categorical Cross-Entropy\n",
    "   * the same as above but the tran lables are just labels not encoded.\n",
    "\n",
    "#### Metrics\n",
    "**Used metrics for the model prediction accuracy evaluation are the same as in the ML area, i.e. accuracy, f1-score, recall, etc. The choice depends on the task and the labels distribution.**\n",
    "\n",
    "### Do you remember in which cases is the *accuracy* metric not a best choice? Which metrics are more suitable for these cases?\n",
    "\n",
    "* Regression metrics\n",
    "   * Mean Squared Error (MSE)\n",
    "   * Mean Absolute Error (MAE)\n",
    "   * Mean Absolute Percentage Error (MAPE)\n",
    "* Classification metrics\n",
    "   * Binary Accuracy\n",
    "   * Categorical Accuracy\n",
    "   * Sparse Categorical Accuracy\n",
    "   * Top k Categorical Accuracy\n",
    "   * Sparse Top k Categorical Accuracy\n",
    "   * Accuracy - a general version that is modified based on the data analyzed autmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can compile the model now, we will use the accuracy metric\n",
    "* Why do we use SparseCategoricalCrossentropy? What is from_logits parameter?\n",
    "\n",
    "* SparseCategoricalCrossentropy = We expect labels to be provided as integers.\n",
    "\n",
    "The output of the Dense layer will either return:\n",
    "* probabilities: The output is passed through a SoftMax function which normalizes the output into a set of probabilities over n, that all add up to 1.\n",
    "* logits: n activations.\n",
    "\n",
    "Your loss function has to be informed as to whether it should expect a normalized distribution (output passed through a SoftMax function) or logits. \n",
    "If your output layer has a 'softmax' activation, from_logits should be False. If your output layer doesn't have a 'softmax' activation, from_logits should be True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rh03ZvLjdMK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA09suMrjdMK"
   },
   "source": [
    "### Model visualization\n",
    "Model then compiles and it is ready for fitting to the data. \n",
    "\n",
    "The model may be printed into image like the following image of our model:\n",
    "\n",
    "<!-- ![model](https://github.com/jplatos/2019-2020-DA4/raw/master/images/da4_01_base.png \"Base model of the neural network\") -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAKg-e-OjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An4hyfD7jdML"
   },
   "source": [
    "# Now we can fit the model to the input data\n",
    "* The *fit()* method fit the model to the data, the parameters are *data* and *labels* from the train set and number of *epoch* to be trained.\n",
    "* The *validation_split* parameter is also very common. What does the parameter do?\n",
    "    * What is the validation set? How is it different from the test set?\n",
    "\n",
    "## FAQ section ðŸ™‚\n",
    "* How does the ANN training procedure look like?\n",
    "* What is the *epoch*?\n",
    "* What is the *batch*?\n",
    "* Why do we use batches? \n",
    "    * Is possible to have batch of size *len(data)* or os size *1*? \n",
    "    * What are caveates of these cases?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the callbacks\n",
    "* Always use ModelCheckpoint callback so you overcome the possible overfitting in the last few epochs!\n",
    "### The .fit() API is pretty powerful\n",
    "- It is common to use some sort of a callback, we will use ModelCheckpoint callback which saves the best weights configuration obtained during training so the overfitting at the final phase of training will be suppressed\n",
    "\n",
    "- **The best weights are determined using the validation loss value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixodDENSjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, epochs=3, callbacks=[model_checkpoint_callback], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Let's take a look at the accuracy and loss function values for both - the train and validation set\n",
    "* What can you see in the plot?\n",
    "* Is OK that loss is getting lower and accuracy higher?\n",
    "    * Can the loss function value go higher? \n",
    "* What if the training set loss is getting lower, but validation set loss higher? \n",
    "    * How do we call this situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj3duiFljdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for key in history.history.keys():\n",
    "    plt.plot(history.epoch, history.history[key], label=key)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware that ANN are prone to overfitting!\n",
    "\n",
    "### How does overfit look like?\n",
    "* Y axis = Accuracy\n",
    "* X axis = Epoch\n",
    "![overfit](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/overfit_acc.png?raw=true \"Overfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we solve the overfitting issue?\n",
    "\n",
    "### 1 - Use simpler model\n",
    "* The model often overfit because there is a lot of parameters for the amount of input data\n",
    "\n",
    "### 2 - Use Dropout layer\n",
    "- Node sampling instead of edge sampling \n",
    "- If a node is dropped, then all incoming and outgoing connections from that node need to be dropped as well\n",
    "- We sample sub-networks from the original one - basically ensemble of networks\n",
    "- There won't be some \"alpha\" node in the individual layer with huge weight coefficient\n",
    "    - Responsibility for prediction will be shared among multiple nodes\n",
    "\n",
    "![dropout](https://github.com/rasvob/2020-21-ARD/raw/master/images/dropout.jpeg \"Dropout\")\n",
    "\n",
    "## Keras API\n",
    "- keras.layers.Dropout(0.5)\n",
    "- Take a look [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3hqCxJ8jdML"
   },
   "source": [
    "## Beware that the best weights needs to be loaded after the training is finished!\n",
    "* **Otherwise you use the weights from the last epoch!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkuf66s5jdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can use *evaluate()* function for obtaining the accuracy using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_meme_02.jpg?raw=true \"AI Meme 02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0QAkwi7jdMM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we obtain the labels and use it for our own evaluation without Keras? Sure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will obtain the raw softmax outputs\n",
    "* What is the range of the vector values?\n",
    "* Do they sum-up to some number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_proba[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sum(y_pred_proba[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can extract the index of the highest probability and get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Is the model any better than the ML baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now re-build the same network using a Functional API\n",
    "* You can use any API that you like however I recomment the **Functional** one as it is more versatile, e.g. building ANN architecture with skip-connect synaptic links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ann_input = keras.layers.Input(shape=(28, 28))\n",
    "flatten = keras.layers.Flatten()(ann_input)\n",
    "hidden = keras.layers.Dense(32, activation=tf.nn.relu)(flatten)\n",
    "ann_output = keras.layers.Dense(10, activation=tf.nn.softmax)(hidden)\n",
    "\n",
    "model = keras.Model(ann_input, ann_output)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy'],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rest is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.2, epochs=3, callbacks=[model_checkpoint_callback], batch_size=128)\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now you have the model fully trained and now can be used for inference\n",
    "* Imagine you want to use DL model in you web app, how would you do it? \n",
    "* Can you name some use cases for the DL models?\n",
    "\n",
    "1) You can save the model to file so you don't need to train the model each time you want to use it\n",
    "2) You can then just load the trained model and use is for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "* Folder *mnist_model* will be created for the model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('mnist_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model back to memory and test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('mnist_model')\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tasks for the rest of the lecture (2p)\n",
    "* Experiment with batch_sizes, use these two settings and compare the results (e.g. compare training time and accuracy) - **(0.5p)**\n",
    "    1) Use batch_size=1\n",
    "    2) Use batch_size=1024\n",
    "    \n",
    "* Add *Dropout()* layer just before the SoftMax output layer in you architecture use these two settings and compare the results (hint: take a look at the train and val accuracy) - **(0.5p)**\n",
    "    1) Dropout(0.2)\n",
    "    2) Dropout(0.95)\n",
    "    \n",
    "* Define your own architecture using either one of the showed API and try to beat the basic model - **(1p)**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
