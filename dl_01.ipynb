{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z4lsdTXjdMG"
   },
   "source": [
    "# Deep Learning - Exercise 1\n",
    "In this lecture, we'll explore essential TensorFlow 2 and Keras concepts through hands-on examples with the MNIST dataset - the \"Hello World\" of deep learning. We'll cover:\n",
    "\n",
    "**Core Concepts**\n",
    "- üöÄ Building and training a basic neural network for digit classification\n",
    "- üìí Understanding validation strategies for model evaluation\n",
    "- üìä Exploring model complexity and its impact on performance\n",
    "- ‚úÖ Designing optimal architectures using fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_meme_01.jpg?raw=true \"AI Meme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCXkXCkBjdMH"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_01.ipynb)\n",
    "\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_01.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGYmu2RCjdMH"
   },
   "source": [
    "### Import of the TensorFlow\n",
    "The main version of the TensorFlow (TF) is a in the Version package in the field VERSION\n",
    "Since the TensformFlow 2.0 everything was encapsulaed under the KERAS api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I798rQYbjdMI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWDIlb9UjdMI"
   },
   "source": [
    "# Import a dataset\n",
    "* Datasets are stored in the keras.datasets submodule. Few testing datasets are stored here and installed together with the TF package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OK0bAkspjdMI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mnist is the basic dataset with handwritten digits\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# the data are in the form of 28x28 pixes with values 0-255.\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Test data shape:  ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jRqCq9jklBE"
   },
   "source": [
    "* The dataset consists of 60,000 training images and 10,000 testing images. All of the are 28x28 pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACxVYmJGjdMI"
   },
   "source": [
    "## Let's take look on the data\n",
    "* üí° Look closely on the value scale - it is from 0 to 255 as usual in grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the numbers are already centered - you won't see this feature in the real world images often üôÇ\n",
    "* Some of the images are quite easy to classify but on the other hand there is a lot of noise in the data as well.\n",
    "\n",
    "**Example Analysis**\n",
    "- Some digits exhibit characteristics of multiple classes\n",
    "- **Sample 1**: Clear, well-defined digit with distinct features\n",
    "- **Sample 2**: Ambiguous sample showing characteristics of both \"1\" and \"7\"\n",
    "    - Missing the horizontal stroke typical of \"7\"\n",
    "    - Slight angle resembling a slanted \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZaKJJf3jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[12], cmap='gray', vmin = 0, vmax = 255)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZaKJJf3jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[42], cmap='gray', vmin = 0, vmax = 255)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z_rfPvrjdMJ"
   },
   "source": [
    "## Normalizing Pixel Values\n",
    "\n",
    "- Convert pixel values from 0-255 to \\(0,1\\) by dividing each value by 255.\n",
    "- **üîé Why Normalize?**\n",
    "  - Smaller input values stabilize gradient updates.\n",
    "  - Consistent input scale improves the adjustment of network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCNrX5rqjdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyhe6lUKjdMJ"
   },
   "source": [
    "# Make better visualization of the data to better understand how complex they are\n",
    "* You can see that \"noisy\" digit and different handwriting style is really no exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIf8LNL7jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = [str(x) for x in range(10)]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≥ Before we dive in the deep learning territory, let's try to create some baseline model using a machine learning so we can compare the approaches\n",
    "* üîé Are ML models capable of processing image data?\n",
    "    * How to deal with a matrix input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg = DecisionTreeClassifier()\n",
    "alg.fit(x_train.reshape(-1, 28*28), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = alg.predict(x_test.reshape(-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that we were able to create a classifier very easily.\n",
    "* üí° However it is no secret, that DL models accuracy on MNIST can be >= 98% even with simple architecture\n",
    "    * Given the fact, our result is not very impresive üôÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3uY8cfbjdMJ"
   },
   "source": [
    "# Basic model - ANN with very simple hierarchy\n",
    "   * Model is created using layers, many layers exists in the [layer submodule](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "   * Each layer uses a activation functions collected in the [module nn](https://www.tensorflow.org/api_docs/python/tf/nn)\n",
    " \n",
    "### üí° There are 2 ways of using the Keras API - **Sequential** and **Functional**\n",
    "   * We will start with the sequential one\n",
    "\n",
    "* üîé Why do we use activation functions?\n",
    "* üîé How is ANN different from lin. regression?\n",
    "\n",
    "# üöÄ Let's design our first ANN\n",
    "   * Can you draw such network on a table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FRfWkocjdMK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),# Flatten module flatten the multidimension input into single vector 28x28 = 784 float numbers\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # standard dense-fully connected layer with the rectified lineaar function as an activation\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax), # another fully-connected layer with softmax activation function\n",
    "])\n",
    "\n",
    "model.summary() # prints the summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîé What is the meaning of the *Total params* number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxkeKMLdjdMK"
   },
   "source": [
    "# üìí Model Compilation\n",
    "\n",
    "Each model must be compiled to fit the data and predict labels.\n",
    "\n",
    "## ‚öôÔ∏è Optimizers\n",
    "\n",
    "- A variety of optimizers are available, many based on gradient descent, with gradient-free options (e.g., [Nevergrad by META](https://github.com/facebookresearch/nevergrad)).\n",
    "\n",
    "### üîë Selected Optimizers\n",
    "\n",
    "- **üìâ Gradient Descent**  \n",
    "  Uses the full dataset; not ideal for large datasets.\n",
    "  \n",
    "- **üé≤ Stochastic Gradient Descent (SGD)**  \n",
    "  Estimates the gradient using a subset of data.\n",
    "  \n",
    "- **üìä RMSProp**  \n",
    "  Adapts the learning rate using the running average of recent gradients.\n",
    "  \n",
    "- **‚ö° ADAM**  \n",
    "  Combines gradient average and second moment to adapt the learning rate.\n",
    "\n",
    "## üßÆ Loss Functions\n",
    "\n",
    "üí° *Training aims to optimize weights by minimizing a loss function. The choice depends on the task and network architecture.*\n",
    "\n",
    "- **üìê Mean Squared Error (MSE)** ‚Äì For regression tasks (logarithmic variant available).\n",
    "- **üìè Mean Absolute Error (MAE)** ‚Äì Uses absolute differences instead of squared values.\n",
    "- **üî¢ Binary Cross-Entropy** ‚Äì For binary tasks; requires a sigmoid activation function.\n",
    "- **üåà Categorical Cross-Entropy** ‚Äì For multi-class tasks; requires softmax and one-hot encoded labels.\n",
    "- **üåü Sparse Categorical Cross-Entropy** ‚Äì Similar to categorical loss but works with integer labels.\n",
    "\n",
    "## üìä Metrics\n",
    "\n",
    "üí° *Metrics evaluate model performance. Choose metrics based on the task and label distribution.*\n",
    "\n",
    "### üìà Regression Metrics\n",
    "- Mean Squared Error (MSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "### üß© Classification Metrics\n",
    "- Binary Accuracy\n",
    "- Categorical Accuracy\n",
    "- Sparse Categorical Accuracy\n",
    "- Top-k Categorical Accuracy\n",
    "- Sparse Top-k Categorical Accuracy\n",
    "\n",
    "‚ö° *Remember*: Accuracy may not always be the best metric, especially for imbalanced datasets. Consider alternatives like F1-score or recall in such cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Model Compilation\n",
    "\n",
    "- **Accuracy Metric:** We evaluate our model using the accuracy metric.\n",
    "- **Loss Function:** We use `SparseCategoricalCrossentropy` because our labels are provided as integers.\n",
    "\n",
    "## Dense Layer Output Options\n",
    "\n",
    "- **Probabilities:**  \n",
    "  The output is passed through a softmax function, producing a normalized probability distribution over the classes (sums to 1).\n",
    "  \n",
    "- **Logits:**  \n",
    "  The layer produces raw activations, which are not normalized.\n",
    "\n",
    "## Configuring `from_logits`\n",
    "\n",
    "Your loss function needs to know what kind of output to expect:\n",
    "- üí° If the output layer uses a **softmax** activation, set `from_logits` to `False`.\n",
    "- üí° If the output layer does **not** use softmax, set `from_logits` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rh03ZvLjdMK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA09suMrjdMK"
   },
   "source": [
    "## üìä Model visualization\n",
    "* The model may be printed into image like the following image of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAKg-e-OjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An4hyfD7jdML"
   },
   "source": [
    "# üöÄ Now we can fit the model to the input data\n",
    "* The `fit()` method fit the model to the data, the parameters are *data* and *labels* from the train set and number of *epoch* to be trained.\n",
    "* The `validation_split` parameter is also very common. What does the parameter do?\n",
    "    * What is the validation set? How is it different from the test set?\n",
    "\n",
    "## FAQ section üôÇ\n",
    "* How does the ANN training procedure look like?\n",
    "* What is the *epoch*?\n",
    "* What is the *batch*?\n",
    "* Why do we use batches? \n",
    "    * Is possible to have batch of size `len(data)` or of size `1`? \n",
    "    * What are caveates of these cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the callbacks\n",
    "* üí°  Always use `ModelCheckpoint` callback so you overcome the possible overfitting in the last few epochs!\n",
    "\n",
    "## The `.fit()` API is pretty powerful\n",
    "* It is common to use some sort of a callback, we will use `ModelCheckpoint` callback which saves the best weights configuration obtained during training so the overfitting at the final phase of training will be suppressed\n",
    "\n",
    "* **üí° The best weights are determined using the validation loss value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixodDENSjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, epochs=3, callbacks=[model_checkpoint_callback], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üìà Plot Analysis: Accuracy and Loss\n",
    "The plot displays both training and validation trends, with accuracy increasing and loss decreasing over epochs.\n",
    "\n",
    "- **Lower Loss & Higher Accuracy:**  \n",
    "  It's normal for the loss to decrease and accuracy to increase during training, which indicates that the model is learning. Occasional fluctuations‚Äîwhere the loss may spike briefly‚Äîare also common due to the stochastic nature of optimization.\n",
    "\n",
    "- **Loss Function Behavior:**  \n",
    "  The loss can sometimes increase in short intervals because of noise in the training process, but overall a downward trend is desired.\n",
    "\n",
    "- **Overfitting Indicator:**  \n",
    "  If the training loss continues to decrease while the validation loss starts increasing, this situation is known as **overfitting**. Overfitting occurs when the model learns the training data too well and fails to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj3duiFljdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for key in history.history.keys():\n",
    "    plt.plot(history.epoch, history.history[key], label=key)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Beware that ANNs are prone to overfitting!\n",
    "\n",
    "### How does overfit look like?\n",
    "* Y axis = Accuracy\n",
    "* X axis = Epoch\n",
    "\n",
    "![overfit](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/overfit_acc.png?raw=true \"Overfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé How can we solve the overfitting issue?\n",
    "\n",
    "## 1 - Use simpler model\n",
    "* The model often overfit because there is a lot of parameters for the amount of input data\n",
    "\n",
    "## 2 - Use Dropout layer\n",
    "* Node sampling instead of edge sampling \n",
    "* If a node is dropped, then all incoming and outgoing connections from that node need to be dropped as well\n",
    "* We sample sub-networks from the original one - basically ensemble of networks\n",
    "* There won't be some \"alpha\" node in the individual layer with huge weight coefficient\n",
    "    * Responsibility for prediction will be shared among multiple nodes\n",
    "\n",
    "![dropout](https://github.com/rasvob/2020-21-ARD/raw/master/images/dropout.jpeg \"Dropout\")\n",
    "\n",
    "## üí° Keras API\n",
    "- `keras.layers.Dropout(0.5)`\n",
    "- Take a look [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3hqCxJ8jdML"
   },
   "source": [
    "## Beware that the best weights needs to be loaded after the training is finished!\n",
    "* **üí° Otherwise you use the weights from the last epoch!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkuf66s5jdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can use *evaluate()* function for obtaining the accuracy using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_meme_02.jpg?raw=true \"AI Meme 02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0QAkwi7jdMM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we obtain the labels and use it for our own evaluation without Keras? Sure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will obtain the raw softmax outputs\n",
    "* üîé What is the range of the vector values?\n",
    "* üîé Do they sum-up to some number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_proba[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sum(y_pred_proba[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can extract the index of the highest probability and get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Is the model any better than the ML baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now re-build the same network using a Functional API\n",
    "* You can use any API that you like however I recommend the **Functional** one as it is more versatile in advanced use-cases, e.g. building ANN architecture with skip-connect synaptic links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ann_input = keras.layers.Input(shape=(28, 28))\n",
    "flatten = keras.layers.Flatten()(ann_input)\n",
    "hidden = keras.layers.Dense(32, activation='relu')(flatten)\n",
    "ann_output = keras.layers.Dense(10, activation='softmax')(hidden)\n",
    "\n",
    "model = keras.Model(ann_input, ann_output)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy'],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° The rest of the training process is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.2, epochs=3, callbacks=[model_checkpoint_callback], batch_size=128)\n",
    "model.load_weights(\"best.weights.h5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Deployment\n",
    "\n",
    "- **Save the Model:**  \n",
    "  üìå Once training is complete, save your model to a file. This avoids retraining every time you need to make predictions.\n",
    "\n",
    "- **Load the Model for Inference:**  \n",
    "  üìÇ In your web app, load the saved model and use it to process incoming data and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "* We are using the new `.keras` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('mnist_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model back to memory and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('mnist_model.keras')\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ‚úÖ Task (2p)\n",
    "1) Experiment with batch_sizes, use these two settings and compare the results (üìå Compare training time and accuracy) - **(0.5p)**\n",
    "\n",
    "    1) Use batch_size=1\n",
    "\n",
    "    2) Use batch_size=1024\n",
    "    \n",
    "2) Add `Dropout` layer just before the `SoftMax` output layer in you architecture use these two settings and compare the results (üìå Take a look at the train and val accuracy) - **(0.5p)**\n",
    "\n",
    "    1) Dropout(0.2)\n",
    "    \n",
    "    2) Dropout(0.95)\n",
    "    \n",
    "3) Define your own architecture using either one of the showed API and try to beat the basic model - **(1p)**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
