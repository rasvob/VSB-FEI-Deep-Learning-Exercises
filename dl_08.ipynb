{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm",
    "tags": []
   },
   "source": [
    "# Deep Learning - Exercise 8\n",
    "\n",
    "The aim of this exercise is to learn how to implement and utilize attention mechanisms in deep learning models, focusing on how these techniques allow models to selectively focus on the most relevant parts of input data.\n",
    "\n",
    "**Core Concepts**\n",
    "* ðŸ§  Attention mechanism fundamentals and mathematical foundations\n",
    "* ðŸ” Types of attention mechanisms (Self-attention, Dot-product)\n",
    "* ðŸ“Š Applications in natural language processing\n",
    "* âš™ï¸ Implementation of attention-based models\n",
    "\n",
    "We recommend reading [this post](https://analyticsindiamag.com/a-beginners-guide-to-using-attention-layer-in-neural-networks/) for more detailed information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_08.ipynb)\n",
    "[Download from Github](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_08.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install keract\n",
    "!pip install attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import string as tf_string\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Layer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import os\n",
    "\n",
    "# from keract import get_activations\n",
    "from keras import Input, Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "\n",
    "os.environ['KERAS_ATTENTION_DEBUG'] = '1'\n",
    "from attention import Attention\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "keras.utils.set_random_seed(13)\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“’ What is the Attention mechanism?\n",
    "\n",
    "* When we think about the English word **Attention**, we know that it means **directing your focus at something** and taking greater notice\n",
    "* The Attention mechanism in Deep Learning is based off this concept of directing your focus, and it pays greater attention to certain factors when processing the data\n",
    "    * ðŸ“Œ Paying attention to important information is necessary and it can improve the performance of the model\n",
    "* **Attention mechanism can help a neural network to memorize long sequences of the information**\n",
    "    * ðŸ”Ž Remember the RNN and even LSTM long-context issues?\n",
    "* ðŸ”Ž Can you imagine some use-cases where it can help us?\n",
    "\n",
    "> ðŸ’¡ In very simple terms the Attention mechanism makes sure that the forget mechanism of LSTM layers is not applied over the important pieces of information\n",
    "\n",
    "### The process is usually computed in these few steps\n",
    "\n",
    "![Img00](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_04.png?raw=true)\n",
    "\n",
    "* Letâ€™s say that we have an input with n sequences and output y with m sequence in a network\n",
    "    * $x=[x_1, x_2, ..., x_n]$\n",
    "    * $y = [y_1, y_2, ..., y_n]$\n",
    "    \n",
    "* The encoder which we are using in the network is a bidirectional LSTM network where it has a forward hidden state and a backward hidden state\n",
    "    * Representation of the encoder state can be done by concatenation of these forward and backward states\n",
    "    * $h_i = [h_i^{L2R}, h_i^{R2L}]$\n",
    "\n",
    "* The hidden state is:\n",
    "    * $s_t=f(s_{t-1}, y_{t-1}, c_t)$\n",
    "    \n",
    "* For the output word at position t, the context vector $C_t$ can be the sum of the hidden states of the input sequence\n",
    "* Thus we have:\n",
    "\n",
    "![Img02](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_02.png?raw=true)\n",
    "\n",
    "* Here we can see that the sum of the hidden state is weighted by the alignment scores\n",
    "* ðŸ’¡ We can say that ${\\alpha_{t,i}}$  are the weights that are responsible for defining how much of each sourceâ€™s hidden state should be taken into consideration for each output\n",
    "\n",
    "* ðŸ’¡ There can be various types of alignment scores according to their geometry\n",
    "    * It can be either linear or in the curve geometry\n",
    "\n",
    "### ðŸ“Œ Below are some of the popular attention mechanisms:\n",
    "\n",
    "![Img03](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_03.png?raw=true)\n",
    "\n",
    "#### ðŸ’¡ There are many variants of the mechanism in the wild but the basic computation process is the same\n",
    "\n",
    "### The very common and easy to understand example is **Self-Attention Mechanism**\n",
    "* When an attention mechanism is applied to the network so that it can relate to different positions of a single sequence and can compute the representation of the same sequence, it can be considered as self-attention\n",
    "\n",
    "![Img01](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_01.png?raw=true)\n",
    "\n",
    "* Here in the image, the red color represents the word which is currently learning and the blue color is of the memory, and the intensity of the color represents the degree of memory activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ We will use Attention layer from the library first and try to solve the *Find-Max task*\n",
    "* What is the *Find-Max* task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples, seq_length):\n",
    "    X = np.random.uniform(0, 10, (num_samples, seq_length, 1))\n",
    "    y = np.max(X, axis=1)  # Find maximum value in each sequence\n",
    "    return X, y\n",
    "\n",
    "seq_length = 15\n",
    "# Generate 10,000 sequences of length 15\n",
    "X_train, y_train = generate_data(10000, seq_length)\n",
    "X_test, y_test = generate_data(1000, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ We will employ a simple LSTM-based model with attention layer stacked to it\n",
    "* ðŸ”Ž What is the intuition behind using attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(seq_length, 1))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = Attention(units=32)(x)  # Bahdanau-style additive attention\n",
    "outputs = Dense(1)(x)\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to access the attention weights so we need to create a custom model to access it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_model = Model(\n",
    "    inputs=inputs,\n",
    "    outputs=[model.layers[-1].output, model.layers[4].output]  # Access attention layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can predict the values for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "preds.ravel()[:10], y_test.ravel()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ How should the predicted vs. true values scatter plot look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test.ravel(), y=preds.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample sequence\n",
    "prediction_arr, attention_weights_arr = [], []\n",
    "for i in range(12):\n",
    "    prediction, attention_weights = visualization_model.predict(X_test[i])\n",
    "    prediction_arr.append(prediction)\n",
    "    attention_weights_arr.append(attention_weights)\n",
    "# prediction, attention_weights = visualization_model.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(18, 15))\n",
    "gs = gridspec.GridSpec(4, 3)\n",
    "\n",
    "for i in range(12):\n",
    "    ax = plt.subplot(gs[i//3, i%3])\n",
    "    ax.plot(X_test[i], 'b-o', label='Sequence Values')\n",
    "    ax.plot(attention_weights_arr[i], 'r--s', label='Attention Weights')\n",
    "    ax.set_title(f'Max = {y_test[i][0]:.2f}, MaxPos = {np.argmax(X_test[i])}, AttMaxPos = {np.argmax(attention_weights_arr[i])}')\n",
    "    ax.set_xlabel('Sequence Position')\n",
    "    if i % 3 == 0:\n",
    "        ax.set_ylabel('Value / Weight')\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## âš¡ Now as we know how the Attention layer works we can employ it for the sentiment analysis task\n",
    "* We will use Yelp dataset which contains reviews of restaurants with either positive (1) or negative (0) labels assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('yelp_labelled.txt', 'https://raw.githubusercontent.com/lubsar/VSB-FEI-Deep-Learning-Exercises/main/datasets/yelp_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(path_to_file) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [x.rstrip() for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines_dict = [{'Text': x[:-1].rstrip(), 'Label': int(x[-1])} for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(lines_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ We will use TextVectorization layer as usuall and we will create baseline model without the Attention layer first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64 # Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN\n",
    "max_tokens = 3000\n",
    "sequence_length = 32 # Output dimension after vectorizing - words in vectorited representation are independent\n",
    "\n",
    "vect_layer = TextVectorization(max_tokens=max_tokens, output_mode='int', output_sequence_length=sequence_length)\n",
    "vect_layer.adapt(df.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = vect_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The dataset is balanced\n",
    "* ðŸ’¡ We will use `stratify` parameter of the `train_test_split` to make sure that it stays balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Text, df.Label, test_size=0.20, random_state=13, stratify=df.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(str).to_numpy()\n",
    "X_test = X_test.astype(str).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train')\n",
    "print(y_train.value_counts())\n",
    "print('Test')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define very simple model first\n",
    "\n",
    "This LSTM layer processes sequential data from the preceding embedding layer (`emb`), generates a 50-dimensional output vector, and applies two types of dropout regularization to combat overfitting\n",
    "\n",
    "\n",
    "### Key Components of LSTM\n",
    "1. **Units (50)**  \n",
    "   - The LSTM layer contains 50 memory cells, each capable of learning long-term dependencies in sequential data\n",
    "   - The final output (`x`) is a 50-dimensional vector summarizing the sequenceâ€™s contextual information\n",
    "\n",
    "2. **Dropout (0.3)**  \n",
    "   - **Input Dropout**: Randomly drops 30% of input units during training to prevent overfitting\n",
    "   - Example: If the embedding layer outputs 300-dimensional vectors, 90 features (30% of 300) are masked in each training step \n",
    "\n",
    "3. **Recurrent Dropout (0.4)**  \n",
    "   - Drops 40% of the recurrent connections (hidden state transitions between timesteps) during training\n",
    "   - This specifically regularizes the LSTMâ€™s internal memory mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(len(vocab), output_dim=embedding_dim, embeddings_regularizer=keras.regularizers.l2(.001))(x_v)\n",
    "x = LSTM(50, dropout=0.3, recurrent_dropout=0.4)(emb)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.AdamW(), loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(X_train, y_train.values, validation_split=0.2, callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test.values)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will create our own Attention layer and add it to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme01](https://github.com/lubsar/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_meme_01.jpg?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")        \n",
    "        super(MyAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        dot = tf.matmul(x, self.W) + self.b\n",
    "        th = tf.math.tanh(dot)\n",
    "        et = tf.squeeze(th, axis=-1) # Remove last dimension, similar to np.squeeze\n",
    "        at = tf.nn.softmax(et)\n",
    "        at = tf.expand_dims(at, axis=-1) # Add last dimension, similar to np.expand_dims\n",
    "        output = x * at\n",
    "        res = tf.reduce_sum(output, axis=1) # Sum along the sequence length\n",
    "        return res \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(MyAttention, self).get_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ The shapes for our layer will look like this:\n",
    "* input (None, 32, 128) W: (128, 1) b: (32, 1)\n",
    "* dot (None, 32, 1)\n",
    "* th (None, 32, 1)\n",
    "* squeeze (None, 32)\n",
    "* softmax (None, 32)\n",
    "* expand_dims (None, 32, 1)\n",
    "* output (None, 32, 128)\n",
    "* res (None, 128)\n",
    "\n",
    "### ðŸ”Ž Why do we have `32` biases and `128` weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(len(vocab), output_dim=embedding_dim, embeddings_regularizer=keras.regularizers.l2(.001))(x_v)\n",
    "x = LSTM(128, dropout=0.3,recurrent_dropout=0.2, return_sequences=True)(emb)\n",
    "x = MyAttention()(x)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.AdamW(), loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(X_train, y_train.values, validation_split=0.2, callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test.values)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…  Tasks for the lecture (2p)\n",
    "\n",
    "* `Attention` layer from the [library](https://github.com/philipperemy/keras-attention) has 2 `score` variants (1p)\n",
    "    * Use the layer in your model and test both `score` variants\n",
    "    * Is there any difference in the performance?\n",
    "\n",
    "* It is possible to make LSTM/GRU layers `Bidirectional` using the [Bidirectional layer\n",
    "](https://keras.io/api/layers/recurrent_layers/bidirectional/) (1p)\n",
    "    * Use it in your model - what happened to the number of weights? Was there any improvement?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
