{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm",
    "tags": []
   },
   "source": [
    "# Deep Learning - Exercise 8\n",
    "\n",
    "This lecture is focused on using the attention mechanism in deep learning models.\n",
    "\n",
    "We recomment reading [this](https://analyticsindiamag.com/a-beginners-guide-to-using-attention-layer-in-neural-networks/) for more detailed information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_08.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_08.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install keract\n",
    "!pip install attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import string as tf_string\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Layer\n",
    "\n",
    "from sklearn.model_selection import train_test_split # \n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import scipy\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "import tqdm\n",
    "import io\n",
    "import os\n",
    "\n",
    "import unicodedata, re, string\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "from keract import get_activations\n",
    "from keras import Input, Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "\n",
    "os.environ['KERAS_ATTENTION_DEBUG'] = '1'\n",
    "from attention import Attention\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”Ž What is Attention mechanism?\n",
    "\n",
    "- When we think about the English word â€œAttentionâ€, we know that it means directing your focus at something and taking greater notice. \n",
    "- The Attention mechanism in Deep Learning is based off this concept of directing your focus, and it pays greater attention to certain factors when processing the data.\n",
    "- Paying attention to important information is necessary and it can improve the performance of the model. \n",
    "- **Attention mechanism can help a neural network to memorize long sequences of the information**\n",
    "    - Remember the RNN and even LSTM long-context issues?\n",
    "\n",
    "## ðŸ”Ž Can you imagine some use-cases where it can help us? \n",
    "\n",
    "\n",
    "\n",
    "### The process is usually computed in these few steps\n",
    "\n",
    "![Img00](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_04.png?raw=true)\n",
    "\n",
    "- Letâ€™s say that we have an input with n sequences and output y with m sequence in a network.\n",
    "    - $x=[x_1, x_2, ..., x_n]$\n",
    "    - $y = [y_1, y_2, ..., y_n]$\n",
    "    \n",
    "- The encoder which we are using in the network is a bidirectional LSTM network where it has a forward hidden state and a backward hidden state.\n",
    "    - Representation of the encoder state can be done by concatenation of these forward and backward states. \n",
    "    - $h_i = [h_i^{L2R}, h_i^{R2L}]$\n",
    "\n",
    "- The hidden state is:\n",
    "    - $s_t=f(s_{t-1}, y_{t-1}, c_t)$\n",
    "    \n",
    "- For the output word at position t, the context vector $C_t$ can be the sum of the hidden states of the input sequence.\n",
    "- Thus we have:\n",
    "\n",
    "![Img02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_02.png?raw=true)\n",
    "\n",
    "- Here we can see that the sum of the hidden state is weighted by the alignment scores. \n",
    "- We can say that ${\\alpha_{t,i}}$  are the weights that are responsible for defining how much of each sourceâ€™s hidden state should be taken into consideration for each output.\n",
    "\n",
    "- There can be various types of alignment scores according to their geometry. \n",
    "    - It can be either linear or in the curve geometry. \n",
    "\n",
    "- Below are some of the popular attention mechanisms:\n",
    "\n",
    "![Img03](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_03.png?raw=true)\n",
    "\n",
    "## There are many variants of the mechanism in the wild but the basic computation process is the same\n",
    "\n",
    "## The very common and easy to understand example is **Self-Attention Mechanism**\n",
    "- When an attention mechanism is applied to the network so that it can relate to different positions of a single sequence and can compute the representation of the same sequence, it can be considered as self-attention\n",
    "\n",
    "![Img01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_01.png?raw=true)\n",
    "\n",
    "- Here in the image, the red color represents the word which is currently learning and the blue color is of the memory, and the intensity of the color represents the degree of memory activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will use Attention layer from the library first and demonstrate the usage for Find Max Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to define callback for vizualizing the attentions maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisualizeAttentionMap(Callback):\n",
    "\n",
    "    def __init__(self, model, x):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.x = x\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        attention_map = get_activations(self.model, self.x, layer_names='attention_weight')['attention_weight']\n",
    "        x = self.x[..., 0]\n",
    "        plt.close()\n",
    "        fig, axes = plt.subplots(nrows=3, figsize=(10, 8))\n",
    "        maps = [attention_map, create_argmax_mask(attention_map), create_argmax_mask(x)]\n",
    "        maps_names = ['attention layer (continuous)', 'attention layer - argmax (discrete)', 'ground truth (discrete)']\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            im = ax.imshow(maps[i], interpolation='none', cmap='jet')\n",
    "            ax.set_ylabel(maps_names[i] + '\\n#sample axis')\n",
    "            ax.set_xlabel('sequence axis')\n",
    "            ax.xaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticks([])\n",
    "        cbar_ax = fig.add_axes([0.75, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(im, cax=cbar_ax)\n",
    "        fig.suptitle(f'Epoch {epoch} - training\\nEach plot shows a 2-D matrix x-axis: sequence length * y-axis: '\n",
    "                     f'batch/sample axis. \\nThe first matrix contains the attention weights (softmax).'\n",
    "                     f'\\nWe manually apply argmax on the attention weights to see which time step ID has '\n",
    "                     f'the strongest weight. \\nFinally, the last matrix displays the ground truth. The task '\n",
    "                     f'is solved when the second and third matrix match.')\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "\n",
    "def create_argmax_mask(x):\n",
    "    mask = np.zeros_like(x)\n",
    "    for i, m in enumerate(x.argmax(axis=1)):\n",
    "        mask[i, m] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will create training examples first\n",
    "- Goal of the task is to predict the maximum value of the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "num_samples = 100000\n",
    "# https://stats.stackexchange.com/questions/485784/which-distribution-has-its-maximum-uniformly-distributed\n",
    "# Choose beta(1/N,1) to have max(X_1,...,X_n) ~ U(0, 1) => minimizes amount of knowledge.\n",
    "# If all the max(s) are concentrated around 1, then it makes the task easy for the model.\n",
    "x_data = np.random.beta(a=1 / seq_length, b=1, size=(num_samples, seq_length, 1))\n",
    "y_data = np.max(x_data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_data[0], y_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_data[1], y_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use just very simple LSTM-based model with attention applied to it\n",
    "\n",
    "## ðŸ’¡ What is the intuition behind using attention? ðŸ’¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(seq_length, 1))\n",
    "x = LSTM(128, return_sequences=True)(model_input)\n",
    "x = Attention()(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "model = Model(model_input, x)\n",
    "\n",
    "model.compile(loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train the model\n",
    "## ðŸ”Ž Take a look at the attention output - what is the ideal state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epoch = 100\n",
    "# visualize the attention on the first 12 samples.\n",
    "visualize = VisualizeAttentionMap(model, x_data[0:12])\n",
    "model.fit(x_data, y_data, epochs=max_epoch, validation_split=0.2, callbacks=[visualize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now as we know how the mechanism works we can employ it for the sentiment analysis task\n",
    "- We will use Yelp dataset which contains reviews of restaurants with either positive (1) or negative (0) labels assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('yelp_labelled.txt', 'https://raw.githubusercontent.com/rasvob/VSB-FEI-Deep-Learning-Exercises/main/datasets/yelp_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(path_to_file) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [x.rstrip() for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines_dict = [{'Text': x[:-1].rstrip(), 'Label': int(x[-1])} for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(lines_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use TextVectorization layer as usuall and create baseline model without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64 # Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN\n",
    "max_tokens = 3000\n",
    "sequence_length = 32 # Output dimension after vectorizing - words in vectorited representation are independent\n",
    "\n",
    "vect_layer = TextVectorization(max_tokens=max_tokens, output_mode='int', output_sequence_length=sequence_length)\n",
    "vect_layer.adapt(df.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = vect_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Vocabulary example: ', vocab[:10])\n",
    "print('Vocabulary shape: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Text, df.Label, test_size=0.20, random_state=13, stratify=df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train')\n",
    "print(y_train.value_counts())\n",
    "print('Test')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define very simple model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(len(vocab), output_dim=embedding_dim, embeddings_regularizer=keras.regularizers.l2(.001))(x_v)\n",
    "x = LSTM(50, dropout=0.3,recurrent_dropout=0.4)(emb)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.tf',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train.values, y_train.values, validation_data=(X_test.values, y_test.values), callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will create our own Attention layer and add it to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_meme_01.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyAttention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(MyAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        dot = K.dot(x,self.W)+self.b\n",
    "        print('dot', dot.shape)\n",
    "        th = K.tanh(dot)\n",
    "        print('th', th.shape)\n",
    "        et=K.squeeze(th,axis=-1)\n",
    "        print('squeeze', et.shape)\n",
    "        at=K.softmax(et)\n",
    "        print('softmax', et.shape)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        print('expand_dims', et.shape)\n",
    "        output=x*at\n",
    "        print('output', output.shape)\n",
    "        res = K.sum(output,axis=1)\n",
    "        print('res', res.shape)\n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(MyAttention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(len(vocab), output_dim=embedding_dim, embeddings_regularizer=keras.regularizers.l2(.001))(x_v)\n",
    "x = LSTM(128, dropout=0.3,recurrent_dropout=0.2, return_sequences=True)(emb)\n",
    "x = MyAttention()(x)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.tf',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train.values, y_train.values, validation_data=(X_test.values, y_test.values),callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”Ž Can you notice any difference in the model accuracy or training? ðŸ”Ž "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
