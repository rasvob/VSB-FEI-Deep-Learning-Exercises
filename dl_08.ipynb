{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm",
    "tags": []
   },
   "source": [
    "# Deep Learning - Exercise 8\n",
    "\n",
    "This lecture is focused on using the attention mechanism in deep learning models.\n",
    "\n",
    "We recommend reading [this post](https://analyticsindiamag.com/a-beginners-guide-to-using-attention-layer-in-neural-networks/) for more detailed information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_08.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_08.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install keract\n",
    "!pip install attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import string as tf_string\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Layer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import os\n",
    "\n",
    "from keract import get_activations\n",
    "from keras import Input, Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "\n",
    "os.environ['KERAS_ATTENTION_DEBUG'] = '1'\n",
    "from attention import Attention\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“’ What is the Attention mechanism?\n",
    "\n",
    "* When we think about the English word **Attention**, we know that it means **directing your focus at something** and taking greater notice\n",
    "* The Attention mechanism in Deep Learning is based off this concept of directing your focus, and it pays greater attention to certain factors when processing the data\n",
    "    * ðŸ“Œ Paying attention to important information is necessary and it can improve the performance of the model\n",
    "* **Attention mechanism can help a neural network to memorize long sequences of the information**\n",
    "    * ðŸ”Ž Remember the RNN and even LSTM long-context issues?\n",
    "* ðŸ”Ž Can you imagine some use-cases where it can help us?\n",
    "\n",
    "> ðŸ’¡ In very simple terms the Attention mechanism makes sure that the forget mechanism of LSTM layers is not applied over the important pieces of information\n",
    "\n",
    "### The process is usually computed in these few steps\n",
    "\n",
    "![Img00](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_04.png?raw=true)\n",
    "\n",
    "* Letâ€™s say that we have an input with n sequences and output y with m sequence in a network\n",
    "    * $x=[x_1, x_2, ..., x_n]$\n",
    "    * $y = [y_1, y_2, ..., y_n]$\n",
    "    \n",
    "* The encoder which we are using in the network is a bidirectional LSTM network where it has a forward hidden state and a backward hidden state\n",
    "    * Representation of the encoder state can be done by concatenation of these forward and backward states\n",
    "    * $h_i = [h_i^{L2R}, h_i^{R2L}]$\n",
    "\n",
    "* The hidden state is:\n",
    "    * $s_t=f(s_{t-1}, y_{t-1}, c_t)$\n",
    "    \n",
    "* For the output word at position t, the context vector $C_t$ can be the sum of the hidden states of the input sequence\n",
    "* Thus we have:\n",
    "\n",
    "![Img02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_02.png?raw=true)\n",
    "\n",
    "* Here we can see that the sum of the hidden state is weighted by the alignment scores\n",
    "* ðŸ’¡ We can say that ${\\alpha_{t,i}}$  are the weights that are responsible for defining how much of each sourceâ€™s hidden state should be taken into consideration for each output\n",
    "\n",
    "* ðŸ’¡ There can be various types of alignment scores according to their geometry\n",
    "    * It can be either linear or in the curve geometry\n",
    "\n",
    "### ðŸ“Œ Below are some of the popular attention mechanisms:\n",
    "\n",
    "![Img03](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_03.png?raw=true)\n",
    "\n",
    "#### ðŸ’¡ There are many variants of the mechanism in the wild but the basic computation process is the same\n",
    "\n",
    "### The very common and easy to understand example is **Self-Attention Mechanism**\n",
    "* When an attention mechanism is applied to the network so that it can relate to different positions of a single sequence and can compute the representation of the same sequence, it can be considered as self-attention\n",
    "\n",
    "![Img01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_01.png?raw=true)\n",
    "\n",
    "* Here in the image, the red color represents the word which is currently learning and the blue color is of the memory, and the intensity of the color represents the degree of memory activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ We will use Attention layer from the library first and try to solve the *Find-Max task*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ We need to define callback for vizualizing the attentions maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisualizeAttentionMap(Callback):\n",
    "\n",
    "    def __init__(self, model, x):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.x = x\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        attention_map = get_activations(self.model, self.x, layer_names='attention_weight')['attention_weight']\n",
    "        x = self.x[..., 0]\n",
    "        plt.close()\n",
    "        fig, axes = plt.subplots(nrows=3, figsize=(10, 8))\n",
    "        maps = [attention_map, create_argmax_mask(attention_map), create_argmax_mask(x)]\n",
    "        maps_names = ['attention layer (continuous)', 'attention layer - argmax (discrete)', 'ground truth (discrete)']\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            im = ax.imshow(maps[i], interpolation='none', cmap='jet')\n",
    "            ax.set_ylabel(maps_names[i] + '\\n#sample axis')\n",
    "            ax.set_xlabel('sequence axis')\n",
    "            ax.xaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticks([])\n",
    "        cbar_ax = fig.add_axes([0.75, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(im, cax=cbar_ax)\n",
    "        fig.suptitle(f'Epoch {epoch} - training\\nEach plot shows a 2-D matrix x-axis: sequence length * y-axis: '\n",
    "                     f'batch/sample axis. \\nThe first matrix contains the attention weights (softmax).'\n",
    "                     f'\\nWe manually apply argmax on the attention weights to see which time step ID has '\n",
    "                     f'the strongest weight. \\nFinally, the last matrix displays the ground truth. The task '\n",
    "                     f'is solved when the second and third matrix match.')\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "\n",
    "def create_argmax_mask(x):\n",
    "    mask = np.zeros_like(x)\n",
    "    for i, m in enumerate(x.argmax(axis=1)):\n",
    "        mask[i, m] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to create the training examples first\n",
    "* ðŸ“Œ Goal of the task is to predict the maximum value of the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "num_samples = 100000\n",
    "# https://stats.stackexchange.com/questions/485784/which-distribution-has-its-maximum-uniformly-distributed\n",
    "# Choose beta(1/N,1) to have max(X_1,...,X_n) ~ U(0, 1) => minimizes amount of knowledge.\n",
    "# If all the max(s) are concentrated around 1, then it makes the task easy for the model.\n",
    "x_data = np.random.beta(a=1 / seq_length, b=1, size=(num_samples, seq_length, 1))\n",
    "y_data = np.max(x_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.histplot(x_data.flatten(), kde=False)\n",
    "fig.set_ylim([0, 10000])\n",
    "plt.title('Histogram of input values sampled from Beta distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Input:\\n {x_data[0]},\\n Output:\\n {y_data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Input:\\n {x_data[1]},\\n Output:\\n {y_data[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ We will employ a simple LSTM-based model with attention layer stacked to it\n",
    "* ðŸ”Ž What is the intuition behind using attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(seq_length, 1))\n",
    "x = LSTM(128, return_sequences=True)(model_input)\n",
    "x = Attention()(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "model = Model(model_input, x)\n",
    "\n",
    "model.compile(loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Let's train the model\n",
    "* ðŸ”Ž Take a look at the attention mask output - what is the ideal state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epoch = 100\n",
    "# visualize the attention on the first 12 samples.\n",
    "visualize = VisualizeAttentionMap(model, x_data[0:12])\n",
    "model.fit(x_data, y_data, epochs=max_epoch, validation_split=0.2, callbacks=[visualize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## âš¡ Now as we know how the Attention layer works we can employ it for the sentiment analysis task\n",
    "* We will use Yelp dataset which contains reviews of restaurants with either positive (1) or negative (0) labels assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('yelp_labelled.txt', 'https://raw.githubusercontent.com/rasvob/VSB-FEI-Deep-Learning-Exercises/main/datasets/yelp_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(path_to_file) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [x.rstrip() for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines_dict = [{'Text': x[:-1].rstrip(), 'Label': int(x[-1])} for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(lines_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ We will use TextVectorization layer as usuall and we will create baseline model without the Attention layer first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64 # Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN\n",
    "max_tokens = 3000\n",
    "sequence_length = 32 # Output dimension after vectorizing - words in vectorited representation are independent\n",
    "\n",
    "vect_layer = TextVectorization(max_tokens=max_tokens, output_mode='int', output_sequence_length=sequence_length)\n",
    "vect_layer.adapt(df.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = vect_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The dataset is balanced\n",
    "* ðŸ’¡ We will use `stratify` parameter of the `train_test_split` to make sure that it stays balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Text, df.Label, test_size=0.20, random_state=13, stratify=df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train')\n",
    "print(y_train.value_counts())\n",
    "print('Test')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define very simple model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(len(vocab), output_dim=embedding_dim, embeddings_regularizer=keras.regularizers.l2(.001))(x_v)\n",
    "x = LSTM(50, dropout=0.3,recurrent_dropout=0.4)(emb)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.tf',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train.values, y_train.values, validation_split=0.2, callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test.values, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will create our own Attention layer and add it to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_08_meme_01.jpg?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyAttention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(MyAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        # print('input', x.shape, \"W:\", self.W.shape, \"b:\", self.b.shape)\n",
    "        dot = K.dot(x,self.W)+self.b\n",
    "        # print('dot', dot.shape)\n",
    "        th = K.tanh(dot)\n",
    "        # print('th', th.shape)\n",
    "        et=K.squeeze(th,axis=-1)\n",
    "        # print('squeeze', et.shape)\n",
    "        at=K.softmax(et)\n",
    "        # print('softmax', at.shape)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        # print('expand_dims', at.shape)\n",
    "        output=x*at\n",
    "        # print('output', output.shape)\n",
    "        res = K.sum(output,axis=1)\n",
    "        # print('res', res.shape)\n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(MyAttention,self).get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ If we uncomment the `print` statements the output for our model will look like this:\n",
    "* input (None, 32, 128) W: (128, 1) b: (32, 1)\n",
    "* dot (None, 32, 1)\n",
    "* th (None, 32, 1)\n",
    "* squeeze (None, 32)\n",
    "* softmax (None, 32)\n",
    "* expand_dims (None, 32, 1)\n",
    "* output (None, 32, 128)\n",
    "* res (None, 128)\n",
    "\n",
    "### ðŸ”Ž Why do we have `32` biases and `128` weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n",
    "x_v = vect_layer(input_layer)\n",
    "emb = keras.layers.Embedding(len(vocab), output_dim=embedding_dim, embeddings_regularizer=keras.regularizers.l2(.001))(x_v)\n",
    "x = LSTM(128, dropout=0.3,recurrent_dropout=0.2, return_sequences=True)(emb)\n",
    "x = MyAttention()(x)\n",
    "output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.tf',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train.values, y_train.values, validation_split=0.2, callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test.values, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Can you notice any difference in the model accuracy or training process progress?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ…  Tasks for the lecture (2p)\n",
    "\n",
    "* `Attention` layer from the [library](https://github.com/philipperemy/keras-attention) has 2 `score` variants (1p)\n",
    "    * Use the layer in your model and test both `score` variants\n",
    "    * Is there any difference in the performance?\n",
    "\n",
    "* It is possible to make LSTM/GRU layers `Bidirectional` using the [Bidirectional layer\n",
    "](https://keras.io/api/layers/recurrent_layers/bidirectional/) (1p)\n",
    "    * Use it in your model - what happened to the number of weights? Was there any improvement?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
