{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm"
   },
   "source": [
    "# Deep Learning - Exercise 2\n",
    "\n",
    "This lecture is about introduction to CNNs.\n",
    "\n",
    "We use validating sets, and evaluate our models on CIFAR-10 dataset.\n",
    "\n",
    "We will also try the Batch normalization layer in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_02.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_02.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 13:37:53.093631: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-06 13:37:53.624280: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-06 13:37:53.624321: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-06 13:37:53.624326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # images\n",
    "import numpy as np #numpy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "from scipy.signal import convolve2d # convolutionof the 2D signals\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutmR93t-uis"
   },
   "source": [
    "# Defining terms for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCmwOM9q_4td"
   },
   "source": [
    "## Convolution\n",
    "A convolution is defined as the integral of the product of the two functions after one is reversed and shifted. It is a mathmematical way how to analyze behavior of the functions and the relation between the functions.\n",
    "\n",
    "In image processing, **kernel** or **convolution matrix** or **mask** is a small matrix. In general the convolution in image processing is defined as:\n",
    "\n",
    "$$g(x, y) = \\omega * f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\omega(s,t)f(x-s, y-t)$$\n",
    "\n",
    "where $g(x,y)$ is filtered image, $f(x,y)$ is original image, $\\omega$ if the filter kernel. \n",
    "\n",
    "A kernel (also called a filter) is a smaller-sized matrix in comparison to the dimensions of the input image, that consists of real valued entries.\n",
    "\n",
    "![Example of the Convolution](https://raw.githubusercontent.com/jplatos/2019-2020-DA4/master/images/convolution_example.gif)\n",
    "\n",
    "### Example filters\n",
    "\n",
    "|Name|Definition|\n",
    "|----|:--------:|\n",
    "|Identity| $\\left[\\begin{matrix}0&0&0\\\\0&1&0\\\\0&0&0\\end{matrix}\\right]$|\n",
    "|Sobel vertical edge detection| $\\left[\\begin{matrix}+1&0&-1\\\\+2&0&-2\\\\+1&0&-1\\end{matrix}\\right]$|\n",
    "|Sobel horizontal edge detection| $\\left[\\begin{matrix}+1&+2&+1\\\\0&0&0\\\\-1&-2&-1\\end{matrix}\\right]$|\n",
    "|Edge detection| $\\left[\\begin{matrix}-1&-1&-1\\\\-1&8&-1\\\\-1&-1&-1\\end{matrix}\\right]$|\n",
    "|Sharpen| $\\left[\\begin{matrix}0&-1&0\\\\-1&5&-1\\\\0&-1&0\\end{matrix}\\right]$|\n",
    "|Uniform blur|$\\frac{1}{9}\\left[\\begin{matrix}1&1&1\\\\1&1&1\\\\1&1&1\\end{matrix}\\right]$|\n",
    "|Gaussian blur 3x3| $\\frac{1}{16}\\left[\\begin{matrix}1&2&1\\\\2&4&2\\\\1&2&1\\end{matrix}\\right]$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsiIQveNL991"
   },
   "source": [
    "## Padding\n",
    "\n",
    "One tricky issue when applying convolutional is losing pixels on the edges of our image. A straightforward solution to this problem is to add extra pixels around the boundary of our input image, which increases the effective size of the image.\n",
    "\n",
    "![Padding example](https://raw.githubusercontent.com/jplatos/2019-2020-DA4/master/images/padding_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_TpDPYXL-Cz"
   },
   "source": [
    "### Practical example of convolution and padding without TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please open the URL for reading and pass the result to Pillow, e.g. with ``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(rgb[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:\u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m0.2989\u001b[39m, \u001b[38;5;241m0.5870\u001b[39m, \u001b[38;5;241m0.1140\u001b[39m])\n\u001b[1;32m      4\u001b[0m lena_img_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/jplatos/2019-2020-DA4/master/images/lena.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m rgb2gray(\u001b[43mmpimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlena_img_url\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m      7\u001b[0m img\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Documents/VSB-FEI-Deep-Learning-Exercises/venv/lib/python3.10/site-packages/matplotlib/image.py:1556\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1552\u001b[0m img_open \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1553\u001b[0m     PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen)\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parse\u001b[38;5;241m.\u001b[39murlparse(fname)\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;66;03m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1560\u001b[0m         )\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m img_open(fname) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[1;32m   1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1563\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m             pil_to_array(image))\n",
      "\u001b[0;31mValueError\u001b[0m: Please open the URL for reading and pass the result to Pillow, e.g. with ``np.array(PIL.Image.open(urllib.request.urlopen(url)))``."
     ]
    }
   ],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "lena_img_url = 'https://raw.githubusercontent.com/jplatos/2019-2020-DA4/master/images/lena.png'\n",
    "img = rgb2gray(mpimg.imread(lena_img_url))\n",
    "img = img/255.0\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_mask = np.ones((3,3))/9.0\n",
    "edge_mask = np.ones((3,3))*-1\n",
    "edge_mask[1, 1] = 8\n",
    "mask = np.array([\n",
    "    [ 0,-1, 0],\n",
    "    [-1, 4,-1],\n",
    "    [ 0,-1, 0]\n",
    "    ]) \n",
    "\n",
    "img_blur = convolve2d(img, blur_mask, boundary='symm', mode='same')\n",
    "result = np.clip(convolve2d(img_blur, mask, boundary='symm', mode='same'), 0, 1)\n",
    "plt.imshow(result, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4heY4fMFL9z3"
   },
   "source": [
    "## Pooling\n",
    "\n",
    "Pooling is a way how to decrease the amount of information transfered from one layer to another.\n",
    "The standard way ho to do it is Average Pooling and Maximum Pooling.\n",
    "\n",
    "- Pooling results in (some) invariance to translation because shifting the image slightly does not change the activation map significantly. This property is referred to as translation invariance. \n",
    "\n",
    "- The idea is that similar images often have very different relative locations of the distinctive shapes within them, and translation invariance helps in being able to classify such images in a similar way. \n",
    "    - For example, one should be able to classify a bird as a bird, irrespective of where it occurs in the image.\n",
    "    - Disadvantege is that you can for example succesfully classify image as face even though the position of eyes and mouth is switched, because model doesn't care about location of features in the image, their presence is for the model enough\n",
    "    \n",
    "- Avg. pooling is rarely used, usually we use max-pooling in the hidden layers, the only exception might be the last layer, where avg. pooling can significantly reduce the number of parameters.\n",
    "- One alternative to using fully connected layers is to use average pooling across the whole spatial area of the final set of activation maps to create a single value. \n",
    "    - Therefore, the number of features created in the final spatial layer will be exactly equal to the number of filters. In this scenario, if the final activation maps are of size 7 × 7 × 256, then 256 features will be created. \n",
    "    - Each feature will be the result of aggregating 49 values. This type of approach greatly reduces the parameter footprint of the fully connected layers\n",
    "\n",
    "![MaxPooling example](https://raw.githubusercontent.com/jplatos/2019-2020-DA4/master/images/pooling_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note about shapes\n",
    "- Convolution - increases depth - e.g. If you start with grayscale image with depth = 1 and apply Conv2D with 32 filters, output will have depth = 32\n",
    "- Pooling - reduces width and height, but depth stays the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKoUodQT5It4"
   },
   "source": [
    "## Tensorflow implementation of the Convolution Neural Network\n",
    "\n",
    "### Some utility functions\n",
    "Here is some functions we will use later several times\n",
    "* **show_history** - show history of the **fit** method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "def show_example(train_x, train_y, class_names):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(train_x[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[train_y[i][0]])\n",
    "    plt.show()\n",
    "    \n",
    "def display_activation(activations, col_size, row_size, act_index):\n",
    "    activation = activations[act_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            if activation_index < activation.shape[3]-1:\n",
    "                activation_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6HHl9Cb5IuB"
   },
   "source": [
    "#### Dataset load\n",
    "Importing dataset **CIFAR10** a collection of images 28x28 grayscale of typical clothes and accesories.\n",
    "Dataset is:\n",
    "* downloaded\n",
    "* splitted into train and test set\n",
    "* converted from the range <0,255> into <0, 1>\n",
    "* *train* is splitted into *train* and *validation* set \n",
    "* class names are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar is the basic dataset for image classifaction\n",
    "dataset = tf.keras.datasets.cifar10\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 32x32 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_count = len(class_names)\n",
    "print('Class count:', class_count, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show example images of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(train_x, train_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uli2EHB85IuT"
   },
   "source": [
    "#### Definition of the model\n",
    "The base model is defined as *Sequential* with 2 convolutional layers.\n",
    "\n",
    "Summarization of the model and compilation is done similarly as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use any activation function we want, very common choice is ReLU or Leaky ReLU\n",
    "#### Another very popular activation function nowadays is Mish (it's not only one, there are more Swish etc.)\n",
    "[Mish](https://github.com/digantamisra98/Mish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(Activation):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "get_custom_objects().update({'mish': Mish(mish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(class_count)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uz_cLvzw5Iub"
   },
   "source": [
    "#### Fit the model for defined number of epochs.\n",
    "Show the history of learning, evaluate the efficiency of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_x, train_y, batch_size=32, validation_data=(valid_x, valid_y), epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "El-aMWON5Iuo"
   },
   "source": [
    "#### Vizualization of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)\n",
    "conf_matrix = np.zeros((class_count, class_count))\n",
    "for idx, pred in enumerate(predictions):\n",
    "    row = test_y[idx]\n",
    "    col = np.argmax(pred)\n",
    "    conf_matrix[row, col] += 1\n",
    "\n",
    "# print(conf_matrix)\n",
    "conf_matrix = normalize(conf_matrix, axis=1, norm='l1')\n",
    "# print(conf_matrix)\n",
    "\n",
    "plt.figure(figsize=(class_count,class_count))\n",
    "\n",
    "plt.imshow(conf_matrix)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        text = plt.text(j, i, \"{:.2f}\".format(conf_matrix[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.xticks(range(class_count), class_names)\n",
    "plt.yticks(range(class_count), class_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load of the model\n",
    "The model I/O works as follows. The topology of the model is usually stored separately from the weights. The reason is that the topology is usually used once, but the weights are used frequently and they develop during the learning process. \n",
    "\n",
    "The topology is stored as JSON file format, as is demonstrated later in more human readable form. As you may see, the document is very simple and just summarize the layers and its types - app parameters set during the model creation.\n",
    "\n",
    "The weights are stored using the HDF5 file format a format developed for fast unlimited storage with portable and distributable capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = 'model.json'\n",
    "file_weight = 'model.h5'\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(file_model, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(file_weight)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"class_name\": \"Sequential\",\n",
    "    \"config\": {\n",
    "        \"name\": \"sequential\",\n",
    "        \"layers\": [\n",
    "            {\n",
    "                \"class_name\": \"Flatten\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"flatten\",\n",
    "                    \"trainable\": true,\n",
    "                    \"batch_input_shape\": [\n",
    "                        null,\n",
    "                        28,\n",
    "                        28\n",
    "                    ],\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"data_format\": \"channels_last\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class_name\": \"Dense\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"dense\",\n",
    "                    \"trainable\": true,\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"units\": 32,\n",
    "                    \"activation\": \"relu\",\n",
    "                    \"use_bias\": true,\n",
    "                    \"kernel_initializer\": {\n",
    "                        \"class_name\": \"GlorotUniform\",\n",
    "                        \"config\": {\n",
    "                            \"seed\": null\n",
    "                        }\n",
    "                    },\n",
    "                    \"bias_initializer\": {\n",
    "                        \"class_name\": \"Zeros\",\n",
    "                        \"config\": {}\n",
    "                    },\n",
    "                    \"kernel_regularizer\": null,\n",
    "                    \"bias_regularizer\": null,\n",
    "                    \"activity_regularizer\": null,\n",
    "                    \"kernel_constraint\": null,\n",
    "                    \"bias_constraint\": null\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class_name\": \"Dense\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"dense_1\",\n",
    "                    \"trainable\": true,\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"units\": 10,\n",
    "                    \"activation\": \"softmax\",\n",
    "                    \"use_bias\": true,\n",
    "                    \"kernel_initializer\": {\n",
    "                        \"class_name\": \"GlorotUniform\",\n",
    "                        \"config\": {\n",
    "                            \"seed\": null\n",
    "                        }\n",
    "                    },\n",
    "                    \"bias_initializer\": {\n",
    "                        \"class_name\": \"Zeros\",\n",
    "                        \"config\": {}\n",
    "                    },\n",
    "                    \"kernel_regularizer\": null,\n",
    "                    \"bias_regularizer\": null,\n",
    "                    \"activity_regularizer\": null,\n",
    "                    \"kernel_constraint\": null,\n",
    "                    \"bias_constraint\": null\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"keras_version\": \"2.2.4-tf\",\n",
    "    \"backend\": \"tensorflow\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model\n",
    "The model is also loadable separatelly. Topology and weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model need to be compiled to be applicable to the data.As you may see the summary of the model is the same as was for the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = loaded_model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCnIUespE4P2"
   },
   "source": [
    "## Visualize the layers\n",
    "Lest see what the network was able to learn from the train data. For that, we need to prepare a new model and see the ouputs of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0fMPy-_rdcB"
   },
   "outputs": [],
   "source": [
    "# get the outputs form all layers in the model\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "# create the model that has single input and as an output all the outputs from the layers. \n",
    "# Because the layers are connected then the output from first layer is propagated into second layer and the output is computed o it.\n",
    "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the outputs from the model for 10-th input\n",
    "activations = activation_model.predict(train_x[10].reshape((1, 32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCnIUespE4P2"
   },
   "source": [
    "## Visualize the layers\n",
    "Lest see what the network was able to learn from the train data. For that, we need to prepare a new model and see the ouputs of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "qn1tQ1v1ulrj",
    "outputId": "ec7f97ad-743a-491a-eb63-ddd656065d93"
   },
   "outputs": [],
   "source": [
    "# show the input image\n",
    "plt.imshow(train_x[10][:,:,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "colab_type": "code",
    "id": "RMEV2Qjpu98v",
    "outputId": "42a09ea4-126b-4c45-aef2-06f42253db67"
   },
   "outputs": [],
   "source": [
    "# show the output from the first layer - CNN2D\n",
    "display_activation(activations, 8, 4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 702
    },
    "colab_type": "code",
    "id": "FUWTBNVEvkhk",
    "outputId": "e6ed2884-1a69-42ac-d8f9-e8a45561349d"
   },
   "outputs": [],
   "source": [
    "# show the second convolution layer\n",
    "display_activation(activations, 4, 4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "- If we have some two input variables with scales from 0 to 1 and from 0 to 1000 we can normalize them\n",
    "    - Larger scale does not mean that the varible is more important than the other one\n",
    "    - It's clear that without normalization one weight will be very high and other very low to balance the scale difference\n",
    "    - This leads to slow gradient descent convergence - we need small learning rates\n",
    "        - Loss function space is not smooth - gradients may oscillate back and forth before finding optimum\n",
    "        \n",
    "- This issue is present in hidden layers as well due to the mini-batch learning\n",
    "    - Each batch is different from others from distributions point of view\n",
    "    - Covariate shift effect - distribution of data is constantly changing during training\n",
    "        - E.g. We train model on images of black cats - we learn how to map input X to output Y\n",
    "        - If we now use images of colored cats for testing purposes the model will fail - input X changed thus learned mapping is invalid \n",
    "        - This shift happens internally with mini-batches all the time\n",
    "        \n",
    "- Batch norm. standardize activation values to have same mean and variance among batch\n",
    "    - Slight regularization - it adds some noise to each hidden layer’s activations so less overfitting\n",
    "    - We can use higher learning rates because batch normalization makes sure that there’s no activation that’s gone really high or really low\n",
    "    - It makes the landscape of the corresponding optimization problem be significantly more smooth. This ensures, in particular, that the gradients are more predictive and thus allow for use of larger range of learning rates and faster network convergence\n",
    "    \n",
    "- Keras: tf.keras.layers.BatchNormalization\n",
    "    - Usually after Conv or Dense layers\n",
    "    - There are of course multiple different approaches where the batch norm. can be placed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use Data augmentation?\n",
    "- There might be bug present in the TF2 currently, so beware the label encoding\n",
    "- We need to one-hot encode the labels and change the loss function to the CategoricalCrossentropy, note that the output activation function is now set to softmax\n",
    "- Take a look at [this](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar is the basic dataset for image classifaction\n",
    "dataset = tf.keras.datasets.cifar10\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_count = len(class_names)\n",
    "\n",
    "train_y = tf.keras.utils.to_categorical(train_y, class_count)\n",
    "test_y = tf.keras.utils.to_categorical(test_y, class_count)\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 32x32 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "print('Class count:', class_count, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "datagen.fit(train_x)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(128, (5,5), activation='mish', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='mish', padding='same'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='mish'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              loss=keras.losses.CategoricalCrossentropy(), \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(datagen.flow(train_x, train_y,batch_size=32,\n",
    "         subset='training'),  validation_data=datagen.flow(train_x, train_y,\n",
    "         batch_size=32, subset='validation'), epochs=epochs)\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "myIgGem85IvT"
   },
   "source": [
    "## Tasks for Lecture \n",
    "<!-- 1. [Data augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/) is used to expand the training dataset in order to improve the performance and ability of the model to generalize.\n",
    "    - [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) -->\n",
    "1. Design a model which will be able to classify **CIFAR10** with accuracy higher than **80%**.\n",
    "2. Try to work with MNIST and FashionMnist datasets asan image - 99% on **Mnist** is achievable using CNN, 94% on **Fashion-Mnist** too."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
