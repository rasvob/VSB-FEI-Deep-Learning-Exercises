{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm"
   },
   "source": [
    "# Deep Learning - Exercise 2\n",
    "\n",
    "This lecture is about introduction to using ANN for regression tasks.\n",
    "\n",
    "We will use our models on [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset.\n",
    "\n",
    "This dataset contains fule consumptions of several vehicles in miles per gallon. So, we need to predict the fuel efficiencies of various vehicles from the data that has been provided.\n",
    "\n",
    "**Core Concepts**\n",
    "* ‚õΩ Regression task of predicting fuel consumption\n",
    "* üíæ Auto MPG dataset from UCI Machine Learning Repository\n",
    "* üöó Predicting fuel efficiency of vehicles\n",
    "* üß™ Using provided data to train ANN regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_02.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_02.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import seaborn as sns # plotting\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes MAPE\n",
    "\"\"\"\n",
    "def mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes SMAPE\n",
    "\"\"\"\n",
    "def symetric_mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred))/2.0))) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes MAE, MSE, MAPE, SMAPE, R2\n",
    "\"\"\"\n",
    "def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    y_true, y_pred = df['y_true'].values, df['y_pred'].values\n",
    "    return compute_metrics_raw(y_true, y_pred)\n",
    "\n",
    "def compute_metrics_raw(y_true: pd.Series, y_pred: pd.Series) -> pd.DataFrame:\n",
    "    mae, mse, mape, smape, r2 = mean_absolute_error(y_true=y_true, y_pred=y_pred), mean_squared_error(y_true=y_true, y_pred=y_pred), mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), symetric_mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), r2_score(y_true=y_true, y_pred=y_pred)\n",
    "    return pd.DataFrame.from_records([{'MAE': mae, 'MSE': mse, 'MAPE': mape, 'SMAPE': smape, 'R2': r2}], index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history_loss(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        if 'loss' not in key:\n",
    "            continue\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Questions to explore before we dive in! \n",
    "\n",
    "1Ô∏è‚É£ **Regression vs Classification**\n",
    "* What is the key difference between predicting a continuous value vs assigning a category?\n",
    "* Can you think of real examples where regression would be more appropriate than classification?\n",
    "\n",
    "2Ô∏è‚É£ **Solving Regression Tasks**\n",
    "* What steps would you include in your ML pipeline for regression?\n",
    "* Which model architecture would you choose and why?\n",
    "* How would you measure if your predictions are good?\n",
    "\n",
    "3Ô∏è‚É£ **ANN vs Linear Regression**\n",
    "* What makes neural networks more powerful than simple linear models?\n",
    "* When would the added complexity of an ANN be worth it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset first\n",
    "\n",
    "## Dataset info\n",
    "* Number of Instances: 398\n",
    "* Number of Attributes: 9 including the class attribute\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "| # | Feature | Type | Description |\n",
    "|---|---------|------|-------------|\n",
    "| 1 | mpg | continuous | Miles per gallon (higher = better) |\n",
    "| 2 | cylinders | discrete | Number of engine cylinders |\n",
    "| 3 | displacement | continuous | Engine displacement volume |\n",
    "| 4 | horsepower | continuous | Engine power output |\n",
    "| 5 | weight | continuous | Vehicle weight |\n",
    "| 6 | acceleration | continuous | Time to accelerate 0-60 mph |\n",
    "| 7 | model year | discrete | Year of manufacture |\n",
    "| 8 | origin | discrete | Manufacturing region |\n",
    "| 9 | car name | string | Unique vehicle identifier |\n",
    "\n",
    "\n",
    "* Missing Attribute Values:  horsepower has 6 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/rasvob/VSB-FEI-Deep-Learning-Exercises/main/datasets/auto-mpg.csv'\n",
    "rel_path = 'datasets/auto-mpg.csv'\n",
    "df = pd.read_csv(url, na_values='?', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exploring the Data Visually\n",
    "\n",
    "Let's analyze our dataset through these key questions:\n",
    "\n",
    "1. ü§î Which row/columns carry the most significance and why?\n",
    "\n",
    "2. üîç Can you spot the categorical features from these visualizations?\n",
    "   * Look for discrete values\n",
    "   * Check for non-numeric patterns\n",
    "\n",
    "3. üìè Numeric Features Analysis:\n",
    "   * Are the scales consistent across features?\n",
    "   * How might different ranges impact our model?\n",
    "\n",
    "4. üîó Feature Relationships:\n",
    "   * Look for potential correlations\n",
    "   * Identify possible colinear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you see any colinearity in the data?\n",
    "* Can it cause any issue? How to deal with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(numeric_only=True), cmap='Greens', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can plot the categorical data using boxplots\n",
    "* Beware that the data are about cars from 80s, we won't see many 6 or 8 cylinder cars nowadays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x='cylinders', y='mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x='origin', y='mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° There is no info about the *origin* feature = detective work incoming üôÇ\n",
    "\n",
    "### What do you think that the origin means based on the printed data?\n",
    "* And what car origin is your favourite? üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df.origin == 1, 'car_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df.origin == 2, 'car_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df.origin == 3, 'car_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, now we have the basic understanding of the data we can start to try some models\n",
    "* We need to deal with the NA values first, as is it just a few rows, we will drop the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.loc[~df.horsepower.isna(), :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Handling Categorical Features\n",
    "\n",
    "Let's examine our categorical variables:\n",
    "\n",
    "### Origin Feature üåç\n",
    "* Even though *origin* appears numerical (1, 2, 3)\n",
    "* These numbers are actually codes representing:\n",
    "  * 1 ‚Üí American\n",
    "  * 2 ‚Üí European\n",
    "  * 3 ‚Üí Asian\n",
    "* ‚ö†Ô∏è Why treat as categorical? Numbers don't represent order or magnitude!\n",
    "\n",
    "### Car Name Feature üöó\n",
    "* Text data that needs encoding\n",
    "* Contains brand/model information\n",
    "\n",
    "‚ùì Key Question:\n",
    "* Why is *origin* categorical despite being numerical? What's the catch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### car_name is problematic beacause we have quite a few brands so one-hot encoding would add too many columns\n",
    "* We will drop the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['car_name'].apply(lambda x: x.split(' ')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop('car_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['origin'] = df['origin'].replace({1: 'USA', 2: 'EUR', 3: 'JAP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['origin'], prefix=['origin_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into input and output part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = df.drop('mpg', axis=1), df.mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the train/test in ratio 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üéØ Creating Baseline Model\n",
    "\n",
    "We'll start with Linear Regression as our foundation:\n",
    "\n",
    "### Linear Regression vs ANN ü§î\n",
    "* Linear regression:\n",
    "  * Simple mathematical formula\n",
    "  * Clear coefficients for each feature\n",
    "  * Direct feature importance interpretation\n",
    "\n",
    "* Neural Network:\n",
    "  * Complex layered structure\n",
    "  * Hidden transformations\n",
    "  * \"Black box\" nature\n",
    "\n",
    "‚ùì Key Question:\n",
    "* Which model provides better explainability - ANN or Linear Regression? Why?\n",
    "\n",
    "![Meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_meme_reg_01.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use just *horsepower* and *model_year* features because the high correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg = LinearRegression()\n",
    "alg.fit(X_train.loc[:, ['horsepower', 'model_year']], y_train)\n",
    "y_pred = alg.predict(X_test.loc[:, ['horsepower', 'model_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Model Evaluation in Regression\n",
    "\n",
    "### Common Regression Metrics üìè\n",
    "\n",
    "1. Basic Metrics:\n",
    "* MAE (Mean Absolute Error)\n",
    "* RMSE (Root Mean Square Error)\n",
    "\n",
    "2. Advanced Metrics:\n",
    "* R¬≤ (R-squared)\n",
    "* MAPE (Mean Absolute Percentage Error)\n",
    "* sMAPE (Symmetric MAPE)\n",
    "\n",
    "‚ùì Key Question:\n",
    "* Can you write mathematical formulas for any of these metrics?\n",
    "  * Think about:\n",
    "    * Actual values (y)\n",
    "    * Predicted values (≈∑)\n",
    "    * Number of samples (n)\n",
    "\n",
    "üí° Note: I've prepared evaluation functions to help you calculate all metrics easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can create our first deep learning model and compare it to the baseline\n",
    "* The ANN model can use more features as it is designed for bigger datasets and multicolinearity is not so big issue as in the LR case\n",
    "* We will start with a raw data\n",
    "* The evaluation step is the same\n",
    "\n",
    "![Meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_meme_reg_04.jpg?raw=true)\n",
    "\n",
    "## üîé Why do we use *linear* activation in the output layer?\n",
    "\n",
    "# üìí NOTE for Task 2: This is the benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(X_train.shape[1],))\n",
    "                         \n",
    "hidden_1 = keras.layers.Dense(128, activation='relu')(inp)\n",
    "hidden_2 = keras.layers.Dense(32, activation='relu')(hidden_1)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='linear')(hidden_2)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "\n",
    "model.compile(loss=keras.losses.MeanSquaredError(),  \n",
    "              optimizer=keras.optimizers.RMSprop(), \n",
    "              metrics=[keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, callbacks=[model_checkpoint_callback], batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Comparing Model Metrics\n",
    "\n",
    "### Model Comparison üîÑ\n",
    "* Compare metrics between:\n",
    "  * Linear Regression\n",
    "  * Our Neural Network\n",
    "\n",
    "‚ùì Key Questions:\n",
    "* Is the model performing better? Why?\n",
    "* What's the purpose of .ravel()?\n",
    "  * Hint: Think about array dimensions! \n",
    "\n",
    "üí° Note: .ravel() transforms multi-dimensional arrays into 1D arrays, which is often required for metric calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.weights.h5\")\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is very good practice to check the loss function values of train/validation data during the training and not only the metrics\n",
    "* Do you see any issue with the val_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_history_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The loss function plot show clear instability of learning\n",
    "* This is a big issue in the regression tasks and it is pretty common one\n",
    "* It is caused by the features magnitude differences\n",
    "* We can solve the matter with feature scaling (normalization)\n",
    "* A https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization layer can be used for solving the matter\n",
    "\n",
    "### Why is magnitude difference an issue?\n",
    "\n",
    "* You can see that the gradient of the slope is orders of magnitude larger than the intercept.\n",
    "\n",
    "![Grad01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_reg_noscale.png?raw=true)\n",
    "\n",
    "* If we take a look at the one optimization step values change you can see that only the slope changed in value (we see a vertical line in the plot above, with no change in the intercept parameter). \n",
    "    * That‚Äôs because the slope gradient is way bigger than the intercept gradient.\n",
    "    * Gradient actually points in the direction of steepest ascent.\n",
    "    * Gradient is the vector of all partial derivatives of the loss function with respect to all the model weights.\n",
    "        * **Basically these values will tell you in which direction (+ or - delta) and how much you should change the individual weights values to lower the loss function value**\n",
    "        * The amount we adjust our slope each iteration is controlled by a *learning rate* parameter\n",
    "    \n",
    "![Grad02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_reg_noscale_grad.png?raw=true)\n",
    "\n",
    "### There are a few ways we can solve our problem above. The most common way is to simply scale your features before gradient descent.\n",
    "\n",
    "![Grad03](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_reg_scale.png?raw=true)\n",
    "\n",
    "* We can see that not the optimization process is not stuck and computed gradients in the individual steps points in the right direction.\n",
    "\n",
    "![Grad04](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_reg_scale_grad.png?raw=true)\n",
    "\n",
    "\n",
    "* **I recommend visiting https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter1_gradient-descent.html for more details about the topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Data Normalization\n",
    "\n",
    "### Why Normalize? üéØ\n",
    "* Neural networks are sensitive to input scales\n",
    "* Features with different ranges can cause:\n",
    "  * Slower convergence\n",
    "  * Poor model performance\n",
    "  * Training instability\n",
    "\n",
    "### Process üîß\n",
    "1. Normalize features to similar ranges\n",
    "2. Retrain the model\n",
    "3. Compare results with previous version\n",
    "\n",
    "‚ùì Key Questions:\n",
    "* Will normalized data improve our model?\n",
    "* How will the metrics change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can take a look at the mean and variance used in the normalization process for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Mean: ', np.array(norm_layer.variables[0]))\n",
    "print('Variance: ', np.array(norm_layer.variables[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(X_train.shape[1],))\n",
    "norm = norm_layer(inp)                  \n",
    "hidden_1 = keras.layers.Dense(128, activation='relu')(norm)\n",
    "hidden_2 = keras.layers.Dense(32, activation='relu')(hidden_1)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='linear')(hidden_2)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "\n",
    "model.compile(loss=keras.losses.MeanSquaredError(),  \n",
    "              optimizer=keras.optimizers.RMSprop(), \n",
    "              metrics=[keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, callbacks=[model_checkpoint_callback], batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.weights.h5\")\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ Analyzing the Impact of Normalization\n",
    "\n",
    "### Before vs After Comparison üìä\n",
    "* Training behavior\n",
    "* Convergence speed\n",
    "* Final metrics\n",
    "\n",
    "### Why Normalization Matters üéØ\n",
    "* Similar scales ‚Üí Stable gradients\n",
    "* Benefits:\n",
    "  * ‚ö° Faster convergence\n",
    "  * üìà Higher learning rates possible\n",
    "  * üéØ Better numerical stability\n",
    "\n",
    "‚ùì Key Question:\n",
    "* Do you notice any differences in model performance after normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_history_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can transform the output as well\n",
    "* There are multiple scaling options\n",
    "    * MinMax, Std. scale, Log, BoxCox, ...\n",
    "    \n",
    "### We will test *MinMaxScaler* into (-1;1) range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_train_scaled = scaler.fit_transform(np.array(y_train).reshape((-1, 1))).ravel()\n",
    "y_test_scaled = scaler.transform(np.array(y_test).reshape((-1, 1))).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_scaled[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Output Activation Function Warning\n",
    "\n",
    "### The Activation Range Problem üéØ\n",
    "* Sigmoid ‚Üí [0,1] range only\n",
    "* Can't produce negative values\n",
    "* Real data may need wider range\n",
    "\n",
    "‚ùì Key Question:\n",
    "* What happens when activation function range doesn't match our target variable range?\n",
    "\n",
    "üí° Remember: Always match your output activation to your target variable range!\n",
    "* Linear ‚Üí unbounded values\n",
    "* ReLU ‚Üí positive values\n",
    "* Sigmoid ‚Üí [0,1]\n",
    "* Tanh ‚Üí [-1,1]\n",
    "\n",
    "### Anti-Pattern Example ‚ò£Ô∏è\n",
    "* Using sigmoid for unbounded regression\n",
    "* Model will be limited to positive values\n",
    "* Can't predict full range of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(X_train.shape[1],))\n",
    "norm = norm_layer(inp)                  \n",
    "hidden_1 = keras.layers.Dense(128, activation='relu')(norm)\n",
    "hidden_2 = keras.layers.Dense(32, activation='relu')(hidden_1)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='sigmoid')(hidden_2)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "\n",
    "model.compile(loss=keras.losses.MeanSquaredError(),  \n",
    "              optimizer=keras.optimizers.RMSprop(), \n",
    "              metrics=[keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_scaled, validation_split=0.2, callbacks=[model_checkpoint_callback], batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.weights.h5\")\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "df_pred = pd.DataFrame({'y_true': y_test_scaled, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can transfer the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = scaler.inverse_transform(y_pred.reshape((-1, 1))).ravel()\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Analyzing Predictions vs Reality\n",
    "\n",
    "### Spotting the Problem üîç\n",
    "* Predictions limited to [0,1] range\n",
    "* Actual values much wider range\n",
    "* Clear mismatch visible in plot\n",
    "\n",
    "### Ideal Plot Should Show üìà\n",
    "* Points following diagonal line\n",
    "* No range restrictions\n",
    "* Even distribution above/below line\n",
    "\n",
    "‚ùì Key Questions:\n",
    "* Can you identify the sigmoid limitation in the plot?\n",
    "\n",
    "üí° Remember: A good regression plot should show points clustered along y=x line without artificial boundaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test, y=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_history_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ‚úÖ Now we will try to fix the issue and replace sigmoid function with the correct one\n",
    "* What function can we use? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(X_train.shape[1],))\n",
    "norm = norm_layer(inp)                  \n",
    "hidden_1 = keras.layers.Dense(128, activation='relu')(norm)\n",
    "hidden_2 = keras.layers.Dense(32, activation='relu')(hidden_1)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='tanh')(hidden_2)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "\n",
    "model.compile(loss=keras.losses.MeanSquaredError(),  \n",
    "              optimizer=keras.optimizers.RMSprop(), \n",
    "              metrics=[keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_scaled, validation_split=0.2, callbacks=[model_checkpoint_callback], batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.weights.h5\")\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "df_pred = pd.DataFrame({'y_true': y_test_scaled, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can transfer the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = scaler.inverse_transform(y_pred.reshape((-1, 1))).ravel()\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the y_test vs. y_pred\n",
    "* Is it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test, y=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convergence was quite fast\n",
    "* We can see that there is an issue with the val_loss stability as the changes are very low now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_history_loss(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "myIgGem85IvT"
   },
   "source": [
    "## ‚úÖ  Tasks for the lecture (2p)\n",
    "\n",
    "1) Try to use [PowerTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html) for the output values in a similar manner \n",
    "as the MinMaxScaler - **(1p)**\n",
    "\n",
    "    - When do we use it? Why?\n",
    "    \n",
    "    - If you wanted to guess if it helps, what do you think? \n",
    "        * Plot histogram of the output (*mpg*), you can make an educated guess based on it üôÇ\n",
    "    \n",
    "2) Try to design your own network and beat the **benchmark** network used in the lecture - **(1p)**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
