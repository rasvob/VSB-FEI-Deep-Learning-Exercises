{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86f2EBR75Itm"
   },
   "source": [
    "# Deep Learning - Exercise 2\n",
    "\n",
    "This lecture is about introduction to using ANN for regression tasks.\n",
    "\n",
    "We will use our models on [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset.\n",
    "\n",
    "This dataset contains fule consumptions of several vehicles in miles per gallon. So, we need to predict the fuel efficiencies of various vehicles from the data that has been provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_02.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/dl_02.ipynb)\n",
    "\n",
    "##### Remember to set **GPU** runtime in Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import seaborn as sns # plotting\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes MAPE\n",
    "\"\"\"\n",
    "def mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes SMAPE\n",
    "\"\"\"\n",
    "def symetric_mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred))/2.0))) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes MAE, MSE, MAPE, SMAPE, R2\n",
    "\"\"\"\n",
    "def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    y_true, y_pred = df['y_true'].values, df['y_pred'].values\n",
    "    return compute_metrics_raw(y_true, y_pred)\n",
    "\n",
    "def compute_metrics_raw(y_true: pd.Series, y_pred: pd.Series) -> pd.DataFrame:\n",
    "    mae, mse, mape, smape, r2 = mean_absolute_error(y_true=y_true, y_pred=y_pred), mean_squared_error(y_true=y_true, y_pred=y_pred), mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), symetric_mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), r2_score(y_true=y_true, y_pred=y_pred)\n",
    "    return pd.DataFrame.from_records([{'MAE': mae, 'MSE': mse, 'MAPE': mape, 'SMAPE': smape, 'R2': r2}], index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_history_loss(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        if 'loss' not in key:\n",
    "            continue\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Let's find answers to these quetions first! ðŸ”Ž\n",
    "1) How is regression different from the classification task?\n",
    "2) How would you solve regression tasks? \n",
    "    - Describe briefly the pipeline, model and metrics used\n",
    "2) How is ANN different from Linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset first\n",
    "\n",
    "## Dataset info\n",
    "* Number of Instances: 398\n",
    "* Number of Attributes: 9 including the class attribute\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "    1. mpg:           continuous\n",
    "        * miles per gallon - higher the better\n",
    "    2. cylinders:     multi-valued discrete\n",
    "    3. displacement:  continuous\n",
    "    4. horsepower:    continuous\n",
    "    5. weight:        continuous\n",
    "    6. acceleration:  continuous\n",
    "    7. model year:    multi-valued discrete\n",
    "    8. origin:        multi-valued discrete\n",
    "    9. car name:      string (unique for each instance)\n",
    "\n",
    "* Missing Attribute Values:  horsepower has 6 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/rasvob/VSB-FEI-Deep-Learning-Exercises/main/datasets/auto-mpg.csv'\n",
    "rel_path = 'datasets/auto-mpg.csv'\n",
    "df = pd.read_csv(url, na_values='?', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Plot the data first \n",
    "* Which row/columns is the most important for us? Why? ðŸ“ˆ\n",
    "* Can you identify **categorical** features from these plot? How?\n",
    "* Are the numeric ranges of the features the same? \n",
    "    * Can the difference among it make our task harder?\n",
    "* What about other features? Is there any covariance/colinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you see any colinearity in the data?\n",
    "* Can it cause any issue? How to deal with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), cmap='Greens', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can plot the categorical data using boxplots\n",
    "* Beware that the data are about cars from 80s, we won't see many 6 or 8 cylinder cars nowadays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x='cylinders', y='mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x='origin', y='mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ There is no info about the *origin* feature = detective work incoming ðŸ™‚\n",
    "\n",
    "### What do you think that the origin means based on the printed data?\n",
    "* And what car origin is your favourite? ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df.origin == 1, 'car_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df.origin == 2, 'car_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df.origin == 3, 'car_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, now we have the basic understanding of the data we can start to try some models\n",
    "* We need to deal with the NA values first, as is it just a few rows, we will drop the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.loc[~df.horsepower.isna(), :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finishing touch is the categorical features encoding\n",
    "* We will need to deal with *origin* and *car_name* features\n",
    "    * Why is *origin* categorical? Isn't it numerical value already? What's the catch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### car_name is problematic beacause we have quite a few brands so one-hot encoding would add too many columns\n",
    "* We will drop the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['car_name'].apply(lambda x: x.split(' ')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop('car_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['origin'] = df['origin'].replace({1: 'USA', 2: 'EUR', 3: 'JAP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['origin'], prefix=['origin_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into input and output part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = df.drop('mpg', axis=1), df.mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the train/test in ratio 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now we can create a baseline model now\n",
    "* We will use traditional linear regression\n",
    "\n",
    "## Which model is better explainable from your point of view?\n",
    "* ANN or Lin. regression? Why?\n",
    "\n",
    "![Meme01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_meme_reg_01.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use just *horsepower* and *model_year* features because the high correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg = LinearRegression()\n",
    "alg.fit(X_train.loc[:, ['horsepower', 'model_year']], y_train)\n",
    "y_pred = alg.predict(X_test.loc[:, ['horsepower', 'model_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can evaluate the model now\n",
    "\n",
    "* We evaluated quality of classification models using its accuracy in the previous lectures.\n",
    "\n",
    "* Regression tasks are not different - we have to evaluate model quality as well, but now we use different types of metrics\n",
    "    * Most basic ones are MAE, RMSE\n",
    "    * There are many more metrics - R2, MAPE, sMAPE etc\n",
    "    \n",
    "### ðŸ”Ž Can you define any of the metrics using mathematical formula?\n",
    "\n",
    "* We have prepared the evaluation functions API for you so you can evaluate all the metrics in one function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can create our first deep learning model and compare it to the baseline\n",
    "* The ANN model can use more features as it is designed for bigger datasets and multicolinearity is not so big issue as in the LR case\n",
    "* We will start with a raw data\n",
    "* The evaluation step is the same\n",
    "\n",
    "![Meme02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_meme_reg_04.jpg?raw=true)\n",
    "\n",
    "## ðŸ”Ž Why do we use *linear* activation in the output layer?\n",
    "\n",
    "# ðŸ“’ NOTE for Task 2: This is the benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(X_train.shape[1]))\n",
    "                         \n",
    "hidden_1 = keras.layers.Dense(128, activation='relu')(inp)\n",
    "hidden_2 = keras.layers.Dense(32, activation='relu')(hidden_1)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='linear')(hidden_2)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "\n",
    "model.compile(loss=keras.losses.MeanSquaredError(),  \n",
    "              optimizer=keras.optimizers.RMSprop(), \n",
    "              metrics=[keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, callbacks=[model_checkpoint_callback], batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can compute the same metrics as in the LR model case\n",
    "* Is the model better?\n",
    "    * Why?\n",
    "* Why do we use .ravel() function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is very good practice to check the loss function values of train/validation data during the training and not only the metrics\n",
    "* Do you see any issue with the val_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_history_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The loss function plot show clear instability of learning\n",
    "* This is a big issue in the regression tasks and it is pretty common one\n",
    "* It is caused by the features magnitude differences\n",
    "* We can solve the matter with feature scaling (normalization)\n",
    "* A https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization layer can be used for solving the matter\n",
    "\n",
    "### Why is magnitude difference an issue?\n",
    "\n",
    "* You can see that the gradient of the slope is orders of magnitude larger than the intercept.\n",
    "\n",
    "![Grad01](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_reg_noscale.png?raw=true)\n",
    "\n",
    "* If we take a look at the one optimization step values change you can see that only the slope changed in value (we see a vertical line in the plot above, with no change in the intercept parameter). \n",
    "    * Thatâ€™s because the slope gradient is way bigger than the intercept gradient.\n",
    "    * Gradient actually points in the direction of steepest ascent.\n",
    "    * Gradient is the vector of all partial derivatives of the loss function with respect to all the model weights.\n",
    "        * **Basically these values will tell you in which direction (+ or - delta) and how much you should change the individual weights values to lower the loss function value**\n",
    "        * The amount we adjust our slope each iteration is controlled by a *learning rate* parameter\n",
    "    \n",
    "![Grad02](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_reg_noscale_grad.png?raw=true)\n",
    "\n",
    "### There are a few ways we can solve our problem above. The most common way is to simply scale your features before gradient descent.\n",
    "\n",
    "![Grad03](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_reg_scale.png?raw=true)\n",
    "\n",
    "* We can see that not the optimization process is not stuck and computed gradients in the individual steps points in the right direction.\n",
    "\n",
    "![Grad04](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_reg_scale_grad.png?raw=true)\n",
    "\n",
    "\n",
    "* **I recommend visiting https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter1_gradient-descent.html for more details about the topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will normalize the data now and try to fit the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can take a look at the mean and variance used in the normalization process for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Mean: ', np.array(norm_layer.variables[0]))\n",
    "print('Variance: ', np.array(norm_layer.variables[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(X_train.shape[1]))\n",
    "norm = norm_layer(inp)                  \n",
    "hidden_1 = keras.layers.Dense(128, activation='relu')(norm)\n",
    "hidden_2 = keras.layers.Dense(32, activation='relu')(hidden_1)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='linear')(hidden_2)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "\n",
    "model.compile(loss=keras.losses.MeanSquaredError(),  \n",
    "              optimizer=keras.optimizers.RMSprop(), \n",
    "              metrics=[keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, callbacks=[model_checkpoint_callback], batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you see any difference?\n",
    "\n",
    "* Normalization can help training of our neural networks as the different features are on a similar scale, which helps to stabilize the gradient descent step, allowing us to use larger learning rates or help models converge faster for a given learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_history_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can transform the output as well\n",
    "* There are multiple scaling options\n",
    "    * MinMax, Std. scale, Log, BoxCox, ...\n",
    "    \n",
    "### We will test *MinMaxScaler* into (-1;1) range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_train_scaled = scaler.fit_transform(np.array(y_train).reshape((-1, 1))).ravel()\n",
    "y_test_scaled = scaler.transform(np.array(y_test).reshape((-1, 1))).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_scaled[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš  Beware the output layer range of the used activation function âš \n",
    "### If there is a mismatch between range of the output activation function and output variable range, the model won't work as ANN may not be able to produce numbers from the correct range\n",
    "\n",
    "### We will try to produce such an issue now\n",
    "* We will set *sigmoid* as an output func. thus we won't be able to output negative numbers\n",
    "\n",
    "# â˜£ ANTI-EXAMPLE â˜£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(X_train.shape[1]))\n",
    "norm = norm_layer(inp)                  \n",
    "hidden_1 = keras.layers.Dense(128, activation='relu')(norm)\n",
    "hidden_2 = keras.layers.Dense(32, activation='relu')(hidden_1)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='sigmoid')(hidden_2)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "\n",
    "model.compile(loss=keras.losses.MeanSquaredError(),  \n",
    "              optimizer=keras.optimizers.RMSprop(), \n",
    "              metrics=[keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_scaled, validation_split=0.2, callbacks=[model_checkpoint_callback], batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "df_pred = pd.DataFrame({'y_true': y_test_scaled, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can transfer the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = scaler.inverse_transform(y_pred.reshape((-1, 1))).ravel()\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the y_test vs. y_pred\n",
    "* Do you see the issue?\n",
    "* How should the ideal plot look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test, y=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_history_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# âœ… Now we will try to fix the issue and replace sigmoid function with the correct one âœ…\n",
    "* What function can we use? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(X_train.shape[1]))\n",
    "norm = norm_layer(inp)                  \n",
    "hidden_1 = keras.layers.Dense(128, activation='relu')(norm)\n",
    "hidden_2 = keras.layers.Dense(32, activation='relu')(hidden_1)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='tanh')(hidden_2)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "\n",
    "model.compile(loss=keras.losses.MeanSquaredError(),  \n",
    "              optimizer=keras.optimizers.RMSprop(), \n",
    "              metrics=[keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_scaled, validation_split=0.2, callbacks=[model_checkpoint_callback], batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "df_pred = pd.DataFrame({'y_true': y_test_scaled, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can transfer the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = scaler.inverse_transform(y_pred.reshape((-1, 1))).ravel()\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred})\n",
    "compute_metrics(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the y_test vs. y_pred\n",
    "* Is it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test, y=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convergence was quite fast\n",
    "* We can see that there is an issue with the val_loss stability as the changes are very low now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_history_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "myIgGem85IvT"
   },
   "source": [
    "## Tasks for the lecture (2p)\n",
    "\n",
    "1) Try to use [PowerTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html) for the output values in a similar manner as the MinMaxScaler - **(1p)**\n",
    "    - When do we use it? Why?\n",
    "    - If you wanted to guess if it helps, what do you think? \n",
    "        * Plot histogram of the output (*mpg*), you can make an educated guess based on it ðŸ™‚\n",
    "    \n",
    "2) Try to design your own network and beat the **benchmark** network used in the lecture - **(1p)**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
